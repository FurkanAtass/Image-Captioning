{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** The architecture of the CNN was given as a pre-trained model and at the end of it, there is an embedding layer that convert our dimensionality and that is a learnable layer to addapte the pre-trained network to our purpose. The RNN architecture is consist of an embedding layer and LSTM layer. The number of LSTM layer is 1 as notebook's default value that was given by you. I used the suggested hyperparameter values in the lesson. I did not increase the number of epochs because the training process taking too much time. \n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** The images were resized 224x224 because of our pretrained model(that we used in the encoder part)'s input dimensions. The normalization values are according to the pretrained model and all pretrained models in torchvision have the same normalization (same mean and std values). \n",
    "\n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** I did not update the pretrained model's parameters because it has already trained on a dataset and I add an embedding layer. The purpose of the embedding layer in the CNN(encoder) part is to addapte the pre-trained network to our purpose. The RNN part is the one that we created and trained completely.\n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** Adam optimizer is the most reccomended optimizer and I start with it. The results in the next part was straightforward and I did not change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=1.85s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 185/414113 [00:00<03:44, 1844.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:32<00:00, 4492.90it/s]\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.torch/models/resnet50-19c8e357.pth\n",
      "100%|██████████| 102502400/102502400 [00:00<00:00, 103431290.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 64          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 800           # dimensionality of image and word embeddings\n",
    "hidden_size = 512         # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 10          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [10/6471], Loss: 5.2365, Perplexity: 188.01801\n",
      "Epoch [1/3], Step [20/6471], Loss: 4.7290, Perplexity: 113.1861\n",
      "Epoch [1/3], Step [30/6471], Loss: 4.3678, Perplexity: 78.87090\n",
      "Epoch [1/3], Step [40/6471], Loss: 4.0916, Perplexity: 59.8384\n",
      "Epoch [1/3], Step [50/6471], Loss: 4.0875, Perplexity: 59.5913\n",
      "Epoch [1/3], Step [60/6471], Loss: 3.9643, Perplexity: 52.6853\n",
      "Epoch [1/3], Step [70/6471], Loss: 3.5704, Perplexity: 35.5317\n",
      "Epoch [1/3], Step [80/6471], Loss: 3.6290, Perplexity: 37.6753\n",
      "Epoch [1/3], Step [90/6471], Loss: 3.6422, Perplexity: 38.1752\n",
      "Epoch [1/3], Step [100/6471], Loss: 3.5920, Perplexity: 36.3069\n",
      "Epoch [1/3], Step [110/6471], Loss: 3.5418, Perplexity: 34.5288\n",
      "Epoch [1/3], Step [120/6471], Loss: 3.5552, Perplexity: 34.9947\n",
      "Epoch [1/3], Step [130/6471], Loss: 3.5275, Perplexity: 34.0372\n",
      "Epoch [1/3], Step [140/6471], Loss: 3.6969, Perplexity: 40.3219\n",
      "Epoch [1/3], Step [150/6471], Loss: 3.5104, Perplexity: 33.4608\n",
      "Epoch [1/3], Step [160/6471], Loss: 3.2769, Perplexity: 26.4940\n",
      "Epoch [1/3], Step [170/6471], Loss: 3.6657, Perplexity: 39.0854\n",
      "Epoch [1/3], Step [180/6471], Loss: 3.4627, Perplexity: 31.9031\n",
      "Epoch [1/3], Step [190/6471], Loss: 3.2595, Perplexity: 26.03780\n",
      "Epoch [1/3], Step [200/6471], Loss: 3.2957, Perplexity: 26.9959\n",
      "Epoch [1/3], Step [210/6471], Loss: 3.4349, Perplexity: 31.0276\n",
      "Epoch [1/3], Step [220/6471], Loss: 3.6399, Perplexity: 38.0876\n",
      "Epoch [1/3], Step [230/6471], Loss: 3.1540, Perplexity: 23.4290\n",
      "Epoch [1/3], Step [240/6471], Loss: 3.3964, Perplexity: 29.8552\n",
      "Epoch [1/3], Step [250/6471], Loss: 3.4911, Perplexity: 32.8234\n",
      "Epoch [1/3], Step [260/6471], Loss: 3.3080, Perplexity: 27.3294\n",
      "Epoch [1/3], Step [270/6471], Loss: 3.2280, Perplexity: 25.2304\n",
      "Epoch [1/3], Step [280/6471], Loss: 3.1187, Perplexity: 22.6164\n",
      "Epoch [1/3], Step [290/6471], Loss: 3.1454, Perplexity: 23.2299\n",
      "Epoch [1/3], Step [300/6471], Loss: 3.4094, Perplexity: 30.2462\n",
      "Epoch [1/3], Step [310/6471], Loss: 3.1022, Perplexity: 22.2466\n",
      "Epoch [1/3], Step [320/6471], Loss: 3.0413, Perplexity: 20.9320\n",
      "Epoch [1/3], Step [330/6471], Loss: 3.0008, Perplexity: 20.1015\n",
      "Epoch [1/3], Step [340/6471], Loss: 3.6359, Perplexity: 37.9350\n",
      "Epoch [1/3], Step [350/6471], Loss: 3.1066, Perplexity: 22.3456\n",
      "Epoch [1/3], Step [360/6471], Loss: 3.0267, Perplexity: 20.6295\n",
      "Epoch [1/3], Step [370/6471], Loss: 3.0630, Perplexity: 21.3918\n",
      "Epoch [1/3], Step [380/6471], Loss: 3.0426, Perplexity: 20.9595\n",
      "Epoch [1/3], Step [390/6471], Loss: 3.0424, Perplexity: 20.9546\n",
      "Epoch [1/3], Step [400/6471], Loss: 3.1051, Perplexity: 22.3105\n",
      "Epoch [1/3], Step [410/6471], Loss: 3.0369, Perplexity: 20.8397\n",
      "Epoch [1/3], Step [420/6471], Loss: 3.0657, Perplexity: 21.4504\n",
      "Epoch [1/3], Step [430/6471], Loss: 3.0419, Perplexity: 20.9440\n",
      "Epoch [1/3], Step [440/6471], Loss: 3.0367, Perplexity: 20.8359\n",
      "Epoch [1/3], Step [450/6471], Loss: 2.9996, Perplexity: 20.0782\n",
      "Epoch [1/3], Step [460/6471], Loss: 3.0418, Perplexity: 20.9436\n",
      "Epoch [1/3], Step [470/6471], Loss: 3.1540, Perplexity: 23.4291\n",
      "Epoch [1/3], Step [480/6471], Loss: 2.9230, Perplexity: 18.5974\n",
      "Epoch [1/3], Step [490/6471], Loss: 2.9676, Perplexity: 19.4452\n",
      "Epoch [1/3], Step [500/6471], Loss: 3.1034, Perplexity: 22.2743\n",
      "Epoch [1/3], Step [510/6471], Loss: 3.1172, Perplexity: 22.5828\n",
      "Epoch [1/3], Step [520/6471], Loss: 2.9882, Perplexity: 19.8501\n",
      "Epoch [1/3], Step [530/6471], Loss: 4.0632, Perplexity: 58.1593\n",
      "Epoch [1/3], Step [540/6471], Loss: 3.1223, Perplexity: 22.6995\n",
      "Epoch [1/3], Step [550/6471], Loss: 3.2472, Perplexity: 25.7193\n",
      "Epoch [1/3], Step [560/6471], Loss: 3.1352, Perplexity: 22.9939\n",
      "Epoch [1/3], Step [570/6471], Loss: 3.1670, Perplexity: 23.7358\n",
      "Epoch [1/3], Step [580/6471], Loss: 3.0591, Perplexity: 21.3075\n",
      "Epoch [1/3], Step [590/6471], Loss: 2.9497, Perplexity: 19.1006\n",
      "Epoch [1/3], Step [600/6471], Loss: 2.9834, Perplexity: 19.7546\n",
      "Epoch [1/3], Step [610/6471], Loss: 3.2350, Perplexity: 25.4065\n",
      "Epoch [1/3], Step [620/6471], Loss: 2.9913, Perplexity: 19.9115\n",
      "Epoch [1/3], Step [630/6471], Loss: 3.3698, Perplexity: 29.0734\n",
      "Epoch [1/3], Step [640/6471], Loss: 3.4585, Perplexity: 31.7707\n",
      "Epoch [1/3], Step [650/6471], Loss: 2.9924, Perplexity: 19.9327\n",
      "Epoch [1/3], Step [660/6471], Loss: 2.6256, Perplexity: 13.8135\n",
      "Epoch [1/3], Step [670/6471], Loss: 2.8459, Perplexity: 17.2177\n",
      "Epoch [1/3], Step [680/6471], Loss: 2.8218, Perplexity: 16.8071\n",
      "Epoch [1/3], Step [690/6471], Loss: 3.1554, Perplexity: 23.4625\n",
      "Epoch [1/3], Step [700/6471], Loss: 2.9849, Perplexity: 19.7838\n",
      "Epoch [1/3], Step [710/6471], Loss: 2.9267, Perplexity: 18.6664\n",
      "Epoch [1/3], Step [720/6471], Loss: 2.7949, Perplexity: 16.3603\n",
      "Epoch [1/3], Step [730/6471], Loss: 2.8981, Perplexity: 18.1390\n",
      "Epoch [1/3], Step [740/6471], Loss: 2.8965, Perplexity: 18.1112\n",
      "Epoch [1/3], Step [750/6471], Loss: 2.7515, Perplexity: 15.6656\n",
      "Epoch [1/3], Step [760/6471], Loss: 2.9620, Perplexity: 19.3370\n",
      "Epoch [1/3], Step [770/6471], Loss: 2.8832, Perplexity: 17.8711\n",
      "Epoch [1/3], Step [780/6471], Loss: 2.7848, Perplexity: 16.1967\n",
      "Epoch [1/3], Step [790/6471], Loss: 2.7277, Perplexity: 15.2982\n",
      "Epoch [1/3], Step [800/6471], Loss: 2.7522, Perplexity: 15.6776\n",
      "Epoch [1/3], Step [810/6471], Loss: 3.2517, Perplexity: 25.8339\n",
      "Epoch [1/3], Step [820/6471], Loss: 2.8553, Perplexity: 17.3794\n",
      "Epoch [1/3], Step [830/6471], Loss: 2.7908, Perplexity: 16.2942\n",
      "Epoch [1/3], Step [840/6471], Loss: 2.7161, Perplexity: 15.1207\n",
      "Epoch [1/3], Step [850/6471], Loss: 2.9983, Perplexity: 20.0516\n",
      "Epoch [1/3], Step [860/6471], Loss: 3.1350, Perplexity: 22.9895\n",
      "Epoch [1/3], Step [870/6471], Loss: 2.7376, Perplexity: 15.4503\n",
      "Epoch [1/3], Step [880/6471], Loss: 2.7089, Perplexity: 15.0132\n",
      "Epoch [1/3], Step [890/6471], Loss: 2.8257, Perplexity: 16.8734\n",
      "Epoch [1/3], Step [900/6471], Loss: 2.6824, Perplexity: 14.6206\n",
      "Epoch [1/3], Step [910/6471], Loss: 2.9267, Perplexity: 18.6661\n",
      "Epoch [1/3], Step [920/6471], Loss: 2.9578, Perplexity: 19.2556\n",
      "Epoch [1/3], Step [930/6471], Loss: 2.8650, Perplexity: 17.5498\n",
      "Epoch [1/3], Step [940/6471], Loss: 2.7268, Perplexity: 15.2842\n",
      "Epoch [1/3], Step [950/6471], Loss: 3.0672, Perplexity: 21.4815\n",
      "Epoch [1/3], Step [960/6471], Loss: 2.6755, Perplexity: 14.5200\n",
      "Epoch [1/3], Step [970/6471], Loss: 2.8368, Perplexity: 17.0606\n",
      "Epoch [1/3], Step [980/6471], Loss: 3.1458, Perplexity: 23.2377\n",
      "Epoch [1/3], Step [990/6471], Loss: 2.8584, Perplexity: 17.4340\n",
      "Epoch [1/3], Step [1000/6471], Loss: 2.4972, Perplexity: 12.1488\n",
      "Epoch [1/3], Step [1010/6471], Loss: 2.9072, Perplexity: 18.3064\n",
      "Epoch [1/3], Step [1020/6471], Loss: 2.7184, Perplexity: 15.1565\n",
      "Epoch [1/3], Step [1030/6471], Loss: 2.5870, Perplexity: 13.2902\n",
      "Epoch [1/3], Step [1040/6471], Loss: 2.6184, Perplexity: 13.7139\n",
      "Epoch [1/3], Step [1050/6471], Loss: 3.2262, Perplexity: 25.1830\n",
      "Epoch [1/3], Step [1060/6471], Loss: 3.0216, Perplexity: 20.5246\n",
      "Epoch [1/3], Step [1070/6471], Loss: 2.5872, Perplexity: 13.2926\n",
      "Epoch [1/3], Step [1080/6471], Loss: 2.5991, Perplexity: 13.4512\n",
      "Epoch [1/3], Step [1090/6471], Loss: 2.6128, Perplexity: 13.6375\n",
      "Epoch [1/3], Step [1100/6471], Loss: 2.5382, Perplexity: 12.6570\n",
      "Epoch [1/3], Step [1110/6471], Loss: 2.6779, Perplexity: 14.5550\n",
      "Epoch [1/3], Step [1120/6471], Loss: 2.6858, Perplexity: 14.6694\n",
      "Epoch [1/3], Step [1130/6471], Loss: 3.1172, Perplexity: 22.5824\n",
      "Epoch [1/3], Step [1140/6471], Loss: 2.7799, Perplexity: 16.1177\n",
      "Epoch [1/3], Step [1150/6471], Loss: 2.6441, Perplexity: 14.0714\n",
      "Epoch [1/3], Step [1160/6471], Loss: 3.2488, Perplexity: 25.7594\n",
      "Epoch [1/3], Step [1170/6471], Loss: 2.6116, Perplexity: 13.6212\n",
      "Epoch [1/3], Step [1180/6471], Loss: 2.7055, Perplexity: 14.9619\n",
      "Epoch [1/3], Step [1190/6471], Loss: 2.7688, Perplexity: 15.9390\n",
      "Epoch [1/3], Step [1200/6471], Loss: 2.7236, Perplexity: 15.2349\n",
      "Epoch [1/3], Step [1210/6471], Loss: 2.7268, Perplexity: 15.2834\n",
      "Epoch [1/3], Step [1220/6471], Loss: 2.5851, Perplexity: 13.2642\n",
      "Epoch [1/3], Step [1230/6471], Loss: 2.9647, Perplexity: 19.3888\n",
      "Epoch [1/3], Step [1240/6471], Loss: 3.0031, Perplexity: 20.1473\n",
      "Epoch [1/3], Step [1250/6471], Loss: 2.6481, Perplexity: 14.1278\n",
      "Epoch [1/3], Step [1260/6471], Loss: 2.5563, Perplexity: 12.8884\n",
      "Epoch [1/3], Step [1270/6471], Loss: 2.6155, Perplexity: 13.6746\n",
      "Epoch [1/3], Step [1280/6471], Loss: 2.8052, Perplexity: 16.5302\n",
      "Epoch [1/3], Step [1290/6471], Loss: 2.6255, Perplexity: 13.8115\n",
      "Epoch [1/3], Step [1300/6471], Loss: 2.7965, Perplexity: 16.3870\n",
      "Epoch [1/3], Step [1310/6471], Loss: 2.8203, Perplexity: 16.7815\n",
      "Epoch [1/3], Step [1320/6471], Loss: 2.9759, Perplexity: 19.6076\n",
      "Epoch [1/3], Step [1330/6471], Loss: 2.6836, Perplexity: 14.6382\n",
      "Epoch [1/3], Step [1340/6471], Loss: 2.5589, Perplexity: 12.9211\n",
      "Epoch [1/3], Step [1350/6471], Loss: 2.6005, Perplexity: 13.4708\n",
      "Epoch [1/3], Step [1360/6471], Loss: 2.5392, Perplexity: 12.6695\n",
      "Epoch [1/3], Step [1370/6471], Loss: 2.4813, Perplexity: 11.9572\n",
      "Epoch [1/3], Step [1380/6471], Loss: 3.0704, Perplexity: 21.5510\n",
      "Epoch [1/3], Step [1390/6471], Loss: 2.6354, Perplexity: 13.9483\n",
      "Epoch [1/3], Step [1400/6471], Loss: 2.6227, Perplexity: 13.7734\n",
      "Epoch [1/3], Step [1410/6471], Loss: 2.7293, Perplexity: 15.3218\n",
      "Epoch [1/3], Step [1420/6471], Loss: 2.4593, Perplexity: 11.6961\n",
      "Epoch [1/3], Step [1430/6471], Loss: 2.6257, Perplexity: 13.8142\n",
      "Epoch [1/3], Step [1440/6471], Loss: 2.5694, Perplexity: 13.0583\n",
      "Epoch [1/3], Step [1450/6471], Loss: 2.7413, Perplexity: 15.5075\n",
      "Epoch [1/3], Step [1460/6471], Loss: 2.6855, Perplexity: 14.6660\n",
      "Epoch [1/3], Step [1470/6471], Loss: 2.4381, Perplexity: 11.4512\n",
      "Epoch [1/3], Step [1480/6471], Loss: 2.5395, Perplexity: 12.6738\n",
      "Epoch [1/3], Step [1490/6471], Loss: 2.5076, Perplexity: 12.2758\n",
      "Epoch [1/3], Step [1500/6471], Loss: 2.6135, Perplexity: 13.6462\n",
      "Epoch [1/3], Step [1510/6471], Loss: 3.2313, Perplexity: 25.3129\n",
      "Epoch [1/3], Step [1520/6471], Loss: 2.8205, Perplexity: 16.7850\n",
      "Epoch [1/3], Step [1530/6471], Loss: 2.7990, Perplexity: 16.4281\n",
      "Epoch [1/3], Step [1540/6471], Loss: 2.5901, Perplexity: 13.3317\n",
      "Epoch [1/3], Step [1550/6471], Loss: 2.4492, Perplexity: 11.5788\n",
      "Epoch [1/3], Step [1560/6471], Loss: 2.5700, Perplexity: 13.0656\n",
      "Epoch [1/3], Step [1570/6471], Loss: 2.4724, Perplexity: 11.8505\n",
      "Epoch [1/3], Step [1580/6471], Loss: 2.4435, Perplexity: 11.5128\n",
      "Epoch [1/3], Step [1590/6471], Loss: 2.6051, Perplexity: 13.5322\n",
      "Epoch [1/3], Step [1600/6471], Loss: 2.5060, Perplexity: 12.2561\n",
      "Epoch [1/3], Step [1610/6471], Loss: 2.3721, Perplexity: 10.7203\n",
      "Epoch [1/3], Step [1620/6471], Loss: 2.7908, Perplexity: 16.2945\n",
      "Epoch [1/3], Step [1630/6471], Loss: 2.7529, Perplexity: 15.6887\n",
      "Epoch [1/3], Step [1640/6471], Loss: 2.4533, Perplexity: 11.6261\n",
      "Epoch [1/3], Step [1650/6471], Loss: 2.5233, Perplexity: 12.4700\n",
      "Epoch [1/3], Step [1660/6471], Loss: 3.3137, Perplexity: 27.4861\n",
      "Epoch [1/3], Step [1670/6471], Loss: 2.5320, Perplexity: 12.5792\n",
      "Epoch [1/3], Step [1680/6471], Loss: 2.7820, Perplexity: 16.1512\n",
      "Epoch [1/3], Step [1690/6471], Loss: 2.4479, Perplexity: 11.5645\n",
      "Epoch [1/3], Step [1700/6471], Loss: 2.8246, Perplexity: 16.8549\n",
      "Epoch [1/3], Step [1710/6471], Loss: 2.7085, Perplexity: 15.0070\n",
      "Epoch [1/3], Step [1720/6471], Loss: 2.1635, Perplexity: 8.70140\n",
      "Epoch [1/3], Step [1730/6471], Loss: 2.5391, Perplexity: 12.6684\n",
      "Epoch [1/3], Step [1740/6471], Loss: 2.6462, Perplexity: 14.1004\n",
      "Epoch [1/3], Step [1750/6471], Loss: 2.5069, Perplexity: 12.2668\n",
      "Epoch [1/3], Step [1760/6471], Loss: 3.3538, Perplexity: 28.6120\n",
      "Epoch [1/3], Step [1770/6471], Loss: 3.9714, Perplexity: 53.0575\n",
      "Epoch [1/3], Step [1780/6471], Loss: 2.4322, Perplexity: 11.3835\n",
      "Epoch [1/3], Step [1790/6471], Loss: 2.3739, Perplexity: 10.7387\n",
      "Epoch [1/3], Step [1800/6471], Loss: 2.5268, Perplexity: 12.5136\n",
      "Epoch [1/3], Step [1810/6471], Loss: 2.6567, Perplexity: 14.2496\n",
      "Epoch [1/3], Step [1820/6471], Loss: 2.8629, Perplexity: 17.5118\n",
      "Epoch [1/3], Step [1830/6471], Loss: 2.5429, Perplexity: 12.7162\n",
      "Epoch [1/3], Step [1840/6471], Loss: 2.9855, Perplexity: 19.7969\n",
      "Epoch [1/3], Step [1850/6471], Loss: 2.4810, Perplexity: 11.9530\n",
      "Epoch [1/3], Step [1860/6471], Loss: 2.6311, Perplexity: 13.8895\n",
      "Epoch [1/3], Step [1870/6471], Loss: 2.6204, Perplexity: 13.7412\n",
      "Epoch [1/3], Step [1880/6471], Loss: 2.5655, Perplexity: 13.0077\n",
      "Epoch [1/3], Step [1890/6471], Loss: 2.8778, Perplexity: 17.7758\n",
      "Epoch [1/3], Step [1900/6471], Loss: 2.5236, Perplexity: 12.4737\n",
      "Epoch [1/3], Step [1910/6471], Loss: 3.4465, Perplexity: 31.3888\n",
      "Epoch [1/3], Step [1920/6471], Loss: 2.2842, Perplexity: 9.81775\n",
      "Epoch [1/3], Step [1930/6471], Loss: 2.4592, Perplexity: 11.6960\n",
      "Epoch [1/3], Step [1940/6471], Loss: 2.4110, Perplexity: 11.1446\n",
      "Epoch [1/3], Step [1950/6471], Loss: 2.5322, Perplexity: 12.5812\n",
      "Epoch [1/3], Step [1960/6471], Loss: 2.4505, Perplexity: 11.5945\n",
      "Epoch [1/3], Step [1970/6471], Loss: 2.8977, Perplexity: 18.1332\n",
      "Epoch [1/3], Step [1980/6471], Loss: 3.0476, Perplexity: 21.0638\n",
      "Epoch [1/3], Step [1990/6471], Loss: 2.4207, Perplexity: 11.2534\n",
      "Epoch [1/3], Step [2000/6471], Loss: 2.4227, Perplexity: 11.2766\n",
      "Epoch [1/3], Step [2010/6471], Loss: 2.3279, Perplexity: 10.2561\n",
      "Epoch [1/3], Step [2020/6471], Loss: 2.6569, Perplexity: 14.2524\n",
      "Epoch [1/3], Step [2030/6471], Loss: 2.3713, Perplexity: 10.7108\n",
      "Epoch [1/3], Step [2040/6471], Loss: 2.5112, Perplexity: 12.3200\n",
      "Epoch [1/3], Step [2050/6471], Loss: 2.8397, Perplexity: 17.1105\n",
      "Epoch [1/3], Step [2060/6471], Loss: 2.8485, Perplexity: 17.2623\n",
      "Epoch [1/3], Step [2070/6471], Loss: 2.5107, Perplexity: 12.3136\n",
      "Epoch [1/3], Step [2080/6471], Loss: 2.3785, Perplexity: 10.7890\n",
      "Epoch [1/3], Step [2090/6471], Loss: 2.2761, Perplexity: 9.73919\n",
      "Epoch [1/3], Step [2100/6471], Loss: 2.4215, Perplexity: 11.2626\n",
      "Epoch [1/3], Step [2110/6471], Loss: 2.7205, Perplexity: 15.1881\n",
      "Epoch [1/3], Step [2120/6471], Loss: 2.3612, Perplexity: 10.6035\n",
      "Epoch [1/3], Step [2130/6471], Loss: 2.5591, Perplexity: 12.9245\n",
      "Epoch [1/3], Step [2140/6471], Loss: 2.6337, Perplexity: 13.9253\n",
      "Epoch [1/3], Step [2150/6471], Loss: 2.6608, Perplexity: 14.3083\n",
      "Epoch [1/3], Step [2160/6471], Loss: 2.8298, Perplexity: 16.9416\n",
      "Epoch [1/3], Step [2170/6471], Loss: 2.5463, Perplexity: 12.7601\n",
      "Epoch [1/3], Step [2180/6471], Loss: 2.6936, Perplexity: 14.7851\n",
      "Epoch [1/3], Step [2190/6471], Loss: 2.5506, Perplexity: 12.8154\n",
      "Epoch [1/3], Step [2200/6471], Loss: 2.4715, Perplexity: 11.8398\n",
      "Epoch [1/3], Step [2210/6471], Loss: 2.2932, Perplexity: 9.90645\n",
      "Epoch [1/3], Step [2220/6471], Loss: 3.1630, Perplexity: 23.6420\n",
      "Epoch [1/3], Step [2230/6471], Loss: 2.2957, Perplexity: 9.93120\n",
      "Epoch [1/3], Step [2240/6471], Loss: 2.2673, Perplexity: 9.65332\n",
      "Epoch [1/3], Step [2250/6471], Loss: 2.7783, Perplexity: 16.0920\n",
      "Epoch [1/3], Step [2260/6471], Loss: 2.2998, Perplexity: 9.97173\n",
      "Epoch [1/3], Step [2270/6471], Loss: 2.5059, Perplexity: 12.2546\n",
      "Epoch [1/3], Step [2280/6471], Loss: 2.5708, Perplexity: 13.0758\n",
      "Epoch [1/3], Step [2290/6471], Loss: 2.6554, Perplexity: 14.2304\n",
      "Epoch [1/3], Step [2300/6471], Loss: 2.2695, Perplexity: 9.67410\n",
      "Epoch [1/3], Step [2310/6471], Loss: 2.8964, Perplexity: 18.1079\n",
      "Epoch [1/3], Step [2320/6471], Loss: 2.5271, Perplexity: 12.5177\n",
      "Epoch [1/3], Step [2330/6471], Loss: 2.4326, Perplexity: 11.3887\n",
      "Epoch [1/3], Step [2340/6471], Loss: 2.4699, Perplexity: 11.8217\n",
      "Epoch [1/3], Step [2350/6471], Loss: 2.3046, Perplexity: 10.0202\n",
      "Epoch [1/3], Step [2360/6471], Loss: 2.4721, Perplexity: 11.8470\n",
      "Epoch [1/3], Step [2370/6471], Loss: 2.4446, Perplexity: 11.5260\n",
      "Epoch [1/3], Step [2380/6471], Loss: 2.3755, Perplexity: 10.7560\n",
      "Epoch [1/3], Step [2390/6471], Loss: 2.1813, Perplexity: 8.85767\n",
      "Epoch [1/3], Step [2400/6471], Loss: 2.4491, Perplexity: 11.5779\n",
      "Epoch [1/3], Step [2410/6471], Loss: 2.5751, Perplexity: 13.1322\n",
      "Epoch [1/3], Step [2420/6471], Loss: 2.4735, Perplexity: 11.8644\n",
      "Epoch [1/3], Step [2430/6471], Loss: 2.4839, Perplexity: 11.9882\n",
      "Epoch [1/3], Step [2440/6471], Loss: 2.5421, Perplexity: 12.7060\n",
      "Epoch [1/3], Step [2450/6471], Loss: 2.6851, Perplexity: 14.6598\n",
      "Epoch [1/3], Step [2460/6471], Loss: 3.1589, Perplexity: 23.5443\n",
      "Epoch [1/3], Step [2470/6471], Loss: 2.7967, Perplexity: 16.3897\n",
      "Epoch [1/3], Step [2480/6471], Loss: 2.5392, Perplexity: 12.6697\n",
      "Epoch [1/3], Step [2490/6471], Loss: 2.7645, Perplexity: 15.8713\n",
      "Epoch [1/3], Step [2500/6471], Loss: 2.5598, Perplexity: 12.9330\n",
      "Epoch [1/3], Step [2510/6471], Loss: 2.4070, Perplexity: 11.1006\n",
      "Epoch [1/3], Step [2520/6471], Loss: 2.2861, Perplexity: 9.83654\n",
      "Epoch [1/3], Step [2530/6471], Loss: 2.6213, Perplexity: 13.7532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [2540/6471], Loss: 2.4657, Perplexity: 11.7716\n",
      "Epoch [1/3], Step [2550/6471], Loss: 2.3291, Perplexity: 10.2687\n",
      "Epoch [1/3], Step [2560/6471], Loss: 2.7943, Perplexity: 16.3520\n",
      "Epoch [1/3], Step [2570/6471], Loss: 2.4877, Perplexity: 12.0338\n",
      "Epoch [1/3], Step [2580/6471], Loss: 2.1846, Perplexity: 8.88679\n",
      "Epoch [1/3], Step [2590/6471], Loss: 2.4802, Perplexity: 11.9435\n",
      "Epoch [1/3], Step [2600/6471], Loss: 2.3746, Perplexity: 10.7471\n",
      "Epoch [1/3], Step [2610/6471], Loss: 2.7177, Perplexity: 15.1460\n",
      "Epoch [1/3], Step [2620/6471], Loss: 2.3412, Perplexity: 10.3942\n",
      "Epoch [1/3], Step [2630/6471], Loss: 2.3012, Perplexity: 9.98633\n",
      "Epoch [1/3], Step [2640/6471], Loss: 2.4246, Perplexity: 11.2976\n",
      "Epoch [1/3], Step [2650/6471], Loss: 2.4668, Perplexity: 11.7847\n",
      "Epoch [1/3], Step [2660/6471], Loss: 2.3483, Perplexity: 10.4679\n",
      "Epoch [1/3], Step [2670/6471], Loss: 2.1943, Perplexity: 8.97369\n",
      "Epoch [1/3], Step [2680/6471], Loss: 2.3152, Perplexity: 10.1273\n",
      "Epoch [1/3], Step [2690/6471], Loss: 2.3524, Perplexity: 10.5113\n",
      "Epoch [1/3], Step [2700/6471], Loss: 2.5110, Perplexity: 12.3177\n",
      "Epoch [1/3], Step [2710/6471], Loss: 2.2490, Perplexity: 9.47845\n",
      "Epoch [1/3], Step [2720/6471], Loss: 2.4323, Perplexity: 11.3851\n",
      "Epoch [1/3], Step [2730/6471], Loss: 2.2187, Perplexity: 9.19538\n",
      "Epoch [1/3], Step [2740/6471], Loss: 2.2080, Perplexity: 9.09723\n",
      "Epoch [1/3], Step [2750/6471], Loss: 2.5988, Perplexity: 13.4476\n",
      "Epoch [1/3], Step [2760/6471], Loss: 2.3890, Perplexity: 10.9026\n",
      "Epoch [1/3], Step [2770/6471], Loss: 2.2523, Perplexity: 9.50992\n",
      "Epoch [1/3], Step [2780/6471], Loss: 2.7821, Perplexity: 16.1533\n",
      "Epoch [1/3], Step [2790/6471], Loss: 2.3779, Perplexity: 10.7818\n",
      "Epoch [1/3], Step [2800/6471], Loss: 2.2830, Perplexity: 9.80648\n",
      "Epoch [1/3], Step [2810/6471], Loss: 2.3892, Perplexity: 10.9052\n",
      "Epoch [1/3], Step [2820/6471], Loss: 2.4159, Perplexity: 11.1998\n",
      "Epoch [1/3], Step [2830/6471], Loss: 2.7683, Perplexity: 15.9313\n",
      "Epoch [1/3], Step [2840/6471], Loss: 2.3788, Perplexity: 10.7917\n",
      "Epoch [1/3], Step [2850/6471], Loss: 2.6406, Perplexity: 14.0220\n",
      "Epoch [1/3], Step [2860/6471], Loss: 2.2529, Perplexity: 9.51494\n",
      "Epoch [1/3], Step [2870/6471], Loss: 2.2412, Perplexity: 9.40433\n",
      "Epoch [1/3], Step [2880/6471], Loss: 2.5222, Perplexity: 12.4558\n",
      "Epoch [1/3], Step [2890/6471], Loss: 2.4938, Perplexity: 12.1074\n",
      "Epoch [1/3], Step [2900/6471], Loss: 2.2615, Perplexity: 9.59754\n",
      "Epoch [1/3], Step [2910/6471], Loss: 2.3924, Perplexity: 10.9396\n",
      "Epoch [1/3], Step [2920/6471], Loss: 2.3403, Perplexity: 10.3849\n",
      "Epoch [1/3], Step [2930/6471], Loss: 2.5266, Perplexity: 12.5104\n",
      "Epoch [1/3], Step [2940/6471], Loss: 2.2673, Perplexity: 9.65344\n",
      "Epoch [1/3], Step [2950/6471], Loss: 2.7241, Perplexity: 15.2433\n",
      "Epoch [1/3], Step [2960/6471], Loss: 2.2698, Perplexity: 9.67773\n",
      "Epoch [1/3], Step [2970/6471], Loss: 2.5336, Perplexity: 12.5985\n",
      "Epoch [1/3], Step [2980/6471], Loss: 2.3599, Perplexity: 10.5900\n",
      "Epoch [1/3], Step [2990/6471], Loss: 2.3208, Perplexity: 10.1843\n",
      "Epoch [1/3], Step [3000/6471], Loss: 2.5891, Perplexity: 13.3183\n",
      "Epoch [1/3], Step [3010/6471], Loss: 2.1377, Perplexity: 8.48012\n",
      "Epoch [1/3], Step [3020/6471], Loss: 2.2287, Perplexity: 9.28824\n",
      "Epoch [1/3], Step [3030/6471], Loss: 2.6672, Perplexity: 14.4000\n",
      "Epoch [1/3], Step [3040/6471], Loss: 2.4587, Perplexity: 11.6893\n",
      "Epoch [1/3], Step [3050/6471], Loss: 2.4985, Perplexity: 12.1638\n",
      "Epoch [1/3], Step [3060/6471], Loss: 2.3831, Perplexity: 10.8385\n",
      "Epoch [1/3], Step [3070/6471], Loss: 2.7975, Perplexity: 16.4028\n",
      "Epoch [1/3], Step [3080/6471], Loss: 2.3248, Perplexity: 10.2243\n",
      "Epoch [1/3], Step [3090/6471], Loss: 2.2617, Perplexity: 9.59926\n",
      "Epoch [1/3], Step [3100/6471], Loss: 2.3397, Perplexity: 10.3781\n",
      "Epoch [1/3], Step [3110/6471], Loss: 2.2829, Perplexity: 9.80487\n",
      "Epoch [1/3], Step [3120/6471], Loss: 2.7623, Perplexity: 15.8356\n",
      "Epoch [1/3], Step [3130/6471], Loss: 2.2506, Perplexity: 9.49342\n",
      "Epoch [1/3], Step [3140/6471], Loss: 2.5659, Perplexity: 13.0125\n",
      "Epoch [1/3], Step [3150/6471], Loss: 2.7883, Perplexity: 16.2537\n",
      "Epoch [1/3], Step [3160/6471], Loss: 2.3608, Perplexity: 10.5993\n",
      "Epoch [1/3], Step [3170/6471], Loss: 2.3610, Perplexity: 10.6017\n",
      "Epoch [1/3], Step [3180/6471], Loss: 2.1867, Perplexity: 8.90609\n",
      "Epoch [1/3], Step [3190/6471], Loss: 2.3458, Perplexity: 10.4421\n",
      "Epoch [1/3], Step [3200/6471], Loss: 2.3431, Perplexity: 10.4134\n",
      "Epoch [1/3], Step [3210/6471], Loss: 2.3798, Perplexity: 10.8032\n",
      "Epoch [1/3], Step [3220/6471], Loss: 2.2664, Perplexity: 9.64488\n",
      "Epoch [1/3], Step [3230/6471], Loss: 2.4284, Perplexity: 11.3412\n",
      "Epoch [1/3], Step [3240/6471], Loss: 2.3280, Perplexity: 10.2575\n",
      "Epoch [1/3], Step [3250/6471], Loss: 2.3433, Perplexity: 10.4156\n",
      "Epoch [1/3], Step [3260/6471], Loss: 2.5460, Perplexity: 12.7557\n",
      "Epoch [1/3], Step [3270/6471], Loss: 2.4791, Perplexity: 11.9306\n",
      "Epoch [1/3], Step [3280/6471], Loss: 2.3098, Perplexity: 10.0728\n",
      "Epoch [1/3], Step [3290/6471], Loss: 2.3373, Perplexity: 10.3528\n",
      "Epoch [1/3], Step [3300/6471], Loss: 2.6024, Perplexity: 13.4960\n",
      "Epoch [1/3], Step [3310/6471], Loss: 2.0849, Perplexity: 8.04414\n",
      "Epoch [1/3], Step [3320/6471], Loss: 2.1941, Perplexity: 8.97188\n",
      "Epoch [1/3], Step [3330/6471], Loss: 2.7928, Perplexity: 16.3261\n",
      "Epoch [1/3], Step [3340/6471], Loss: 2.3771, Perplexity: 10.7736\n",
      "Epoch [1/3], Step [3350/6471], Loss: 2.2420, Perplexity: 9.41253\n",
      "Epoch [1/3], Step [3360/6471], Loss: 2.3443, Perplexity: 10.4256\n",
      "Epoch [1/3], Step [3370/6471], Loss: 2.3393, Perplexity: 10.3743\n",
      "Epoch [1/3], Step [3380/6471], Loss: 2.5159, Perplexity: 12.3772\n",
      "Epoch [1/3], Step [3390/6471], Loss: 2.3551, Perplexity: 10.5396\n",
      "Epoch [1/3], Step [3400/6471], Loss: 2.2777, Perplexity: 9.75371\n",
      "Epoch [1/3], Step [3410/6471], Loss: 2.7053, Perplexity: 14.9595\n",
      "Epoch [1/3], Step [3420/6471], Loss: 2.4628, Perplexity: 11.7378\n",
      "Epoch [1/3], Step [3430/6471], Loss: 2.3658, Perplexity: 10.6524\n",
      "Epoch [1/3], Step [3440/6471], Loss: 2.1503, Perplexity: 8.58760\n",
      "Epoch [1/3], Step [3450/6471], Loss: 2.3065, Perplexity: 10.0394\n",
      "Epoch [1/3], Step [3460/6471], Loss: 2.2674, Perplexity: 9.65424\n",
      "Epoch [1/3], Step [3470/6471], Loss: 2.2864, Perplexity: 9.83984\n",
      "Epoch [1/3], Step [3480/6471], Loss: 2.1362, Perplexity: 8.46681\n",
      "Epoch [1/3], Step [3490/6471], Loss: 2.2762, Perplexity: 9.73958\n",
      "Epoch [1/3], Step [3500/6471], Loss: 2.5647, Perplexity: 12.9962\n",
      "Epoch [1/3], Step [3510/6471], Loss: 2.1106, Perplexity: 8.25296\n",
      "Epoch [1/3], Step [3520/6471], Loss: 2.2876, Perplexity: 9.85133\n",
      "Epoch [1/3], Step [3530/6471], Loss: 2.3686, Perplexity: 10.6828\n",
      "Epoch [1/3], Step [3540/6471], Loss: 2.3508, Perplexity: 10.4935\n",
      "Epoch [1/3], Step [3550/6471], Loss: 2.2393, Perplexity: 9.38702\n",
      "Epoch [1/3], Step [3560/6471], Loss: 2.2694, Perplexity: 9.67364\n",
      "Epoch [1/3], Step [3570/6471], Loss: 2.4266, Perplexity: 11.3201\n",
      "Epoch [1/3], Step [3580/6471], Loss: 2.4038, Perplexity: 11.06466\n",
      "Epoch [1/3], Step [3590/6471], Loss: 2.1531, Perplexity: 8.61158\n",
      "Epoch [1/3], Step [3600/6471], Loss: 2.2238, Perplexity: 9.24230\n",
      "Epoch [1/3], Step [3610/6471], Loss: 2.9434, Perplexity: 18.9803\n",
      "Epoch [1/3], Step [3620/6471], Loss: 2.7507, Perplexity: 15.6540\n",
      "Epoch [1/3], Step [3630/6471], Loss: 2.2334, Perplexity: 9.33174\n",
      "Epoch [1/3], Step [3640/6471], Loss: 2.3052, Perplexity: 10.0262\n",
      "Epoch [1/3], Step [3650/6471], Loss: 2.5412, Perplexity: 12.6951\n",
      "Epoch [1/3], Step [3660/6471], Loss: 2.3571, Perplexity: 10.5598\n",
      "Epoch [1/3], Step [3670/6471], Loss: 2.3950, Perplexity: 10.9687\n",
      "Epoch [1/3], Step [3680/6471], Loss: 2.3182, Perplexity: 10.1577\n",
      "Epoch [1/3], Step [3690/6471], Loss: 2.3185, Perplexity: 10.1605\n",
      "Epoch [1/3], Step [3700/6471], Loss: 2.4797, Perplexity: 11.9375\n",
      "Epoch [1/3], Step [3710/6471], Loss: 2.1997, Perplexity: 9.02204\n",
      "Epoch [1/3], Step [3720/6471], Loss: 2.1820, Perplexity: 8.86415\n",
      "Epoch [1/3], Step [3730/6471], Loss: 3.0203, Perplexity: 20.4966\n",
      "Epoch [1/3], Step [3740/6471], Loss: 2.3982, Perplexity: 11.0029\n",
      "Epoch [1/3], Step [3750/6471], Loss: 2.3081, Perplexity: 10.0550\n",
      "Epoch [1/3], Step [3760/6471], Loss: 2.6675, Perplexity: 14.4035\n",
      "Epoch [1/3], Step [3770/6471], Loss: 2.5634, Perplexity: 12.9800\n",
      "Epoch [1/3], Step [3780/6471], Loss: 2.2724, Perplexity: 9.70260\n",
      "Epoch [1/3], Step [3790/6471], Loss: 2.4353, Perplexity: 11.4197\n",
      "Epoch [1/3], Step [3800/6471], Loss: 2.3623, Perplexity: 10.6154\n",
      "Epoch [1/3], Step [3810/6471], Loss: 2.4957, Perplexity: 12.1299\n",
      "Epoch [1/3], Step [3820/6471], Loss: 2.2489, Perplexity: 9.47697\n",
      "Epoch [1/3], Step [3830/6471], Loss: 2.3873, Perplexity: 10.8843\n",
      "Epoch [1/3], Step [3840/6471], Loss: 2.4437, Perplexity: 11.5150\n",
      "Epoch [1/3], Step [3850/6471], Loss: 2.5745, Perplexity: 13.1245\n",
      "Epoch [1/3], Step [3860/6471], Loss: 2.6448, Perplexity: 14.0813\n",
      "Epoch [1/3], Step [3870/6471], Loss: 2.1605, Perplexity: 8.67563\n",
      "Epoch [1/3], Step [3880/6471], Loss: 2.3917, Perplexity: 10.9318\n",
      "Epoch [1/3], Step [3890/6471], Loss: 2.4044, Perplexity: 11.0718\n",
      "Epoch [1/3], Step [3900/6471], Loss: 2.1443, Perplexity: 8.53622\n",
      "Epoch [1/3], Step [3910/6471], Loss: 2.1978, Perplexity: 9.00525\n",
      "Epoch [1/3], Step [3920/6471], Loss: 2.3452, Perplexity: 10.4359\n",
      "Epoch [1/3], Step [3930/6471], Loss: 2.4341, Perplexity: 11.4058\n",
      "Epoch [1/3], Step [3940/6471], Loss: 2.2741, Perplexity: 9.71925\n",
      "Epoch [1/3], Step [3950/6471], Loss: 2.5984, Perplexity: 13.4421\n",
      "Epoch [1/3], Step [3960/6471], Loss: 2.1722, Perplexity: 8.77763\n",
      "Epoch [1/3], Step [3970/6471], Loss: 2.1496, Perplexity: 8.58150\n",
      "Epoch [1/3], Step [3980/6471], Loss: 2.6629, Perplexity: 14.3384\n",
      "Epoch [1/3], Step [3990/6471], Loss: 2.2648, Perplexity: 9.62964\n",
      "Epoch [1/3], Step [4000/6471], Loss: 2.1990, Perplexity: 9.01585\n",
      "Epoch [1/3], Step [4010/6471], Loss: 2.3529, Perplexity: 10.5163\n",
      "Epoch [1/3], Step [4020/6471], Loss: 2.1373, Perplexity: 8.47627\n",
      "Epoch [1/3], Step [4030/6471], Loss: 2.2379, Perplexity: 9.37396\n",
      "Epoch [1/3], Step [4040/6471], Loss: 2.4876, Perplexity: 12.0319\n",
      "Epoch [1/3], Step [4050/6471], Loss: 2.2100, Perplexity: 9.11572\n",
      "Epoch [1/3], Step [4060/6471], Loss: 2.3104, Perplexity: 10.0783\n",
      "Epoch [1/3], Step [4070/6471], Loss: 3.1206, Perplexity: 22.6608\n",
      "Epoch [1/3], Step [4080/6471], Loss: 2.3129, Perplexity: 10.1036\n",
      "Epoch [1/3], Step [4090/6471], Loss: 2.5666, Perplexity: 13.0213\n",
      "Epoch [1/3], Step [4100/6471], Loss: 1.9910, Perplexity: 7.32298\n",
      "Epoch [1/3], Step [4110/6471], Loss: 2.1545, Perplexity: 8.62351\n",
      "Epoch [1/3], Step [4120/6471], Loss: 2.3308, Perplexity: 10.2860\n",
      "Epoch [1/3], Step [4130/6471], Loss: 2.1319, Perplexity: 8.43121\n",
      "Epoch [1/3], Step [4140/6471], Loss: 2.4300, Perplexity: 11.3591\n",
      "Epoch [1/3], Step [4150/6471], Loss: 2.3904, Perplexity: 10.9184\n",
      "Epoch [1/3], Step [4160/6471], Loss: 2.4148, Perplexity: 11.1875\n",
      "Epoch [1/3], Step [4170/6471], Loss: 2.1296, Perplexity: 8.41150\n",
      "Epoch [1/3], Step [4180/6471], Loss: 2.4852, Perplexity: 12.0039\n",
      "Epoch [1/3], Step [4190/6471], Loss: 2.2379, Perplexity: 9.37392\n",
      "Epoch [1/3], Step [4200/6471], Loss: 2.3680, Perplexity: 10.6759\n",
      "Epoch [1/3], Step [4210/6471], Loss: 2.2172, Perplexity: 9.18139\n",
      "Epoch [1/3], Step [4220/6471], Loss: 2.4009, Perplexity: 11.0333\n",
      "Epoch [1/3], Step [4230/6471], Loss: 2.1019, Perplexity: 8.18157\n",
      "Epoch [1/3], Step [4240/6471], Loss: 2.0712, Perplexity: 7.93458\n",
      "Epoch [1/3], Step [4250/6471], Loss: 2.1891, Perplexity: 8.92726\n",
      "Epoch [1/3], Step [4260/6471], Loss: 2.2388, Perplexity: 9.38243\n",
      "Epoch [1/3], Step [4270/6471], Loss: 2.1651, Perplexity: 8.71563\n",
      "Epoch [1/3], Step [4280/6471], Loss: 2.2152, Perplexity: 9.16359\n",
      "Epoch [1/3], Step [4290/6471], Loss: 2.5887, Perplexity: 13.3127\n",
      "Epoch [1/3], Step [4300/6471], Loss: 2.4385, Perplexity: 11.4554\n",
      "Epoch [1/3], Step [4310/6471], Loss: 2.2717, Perplexity: 9.69551\n",
      "Epoch [1/3], Step [4320/6471], Loss: 3.0657, Perplexity: 21.4497\n",
      "Epoch [1/3], Step [4330/6471], Loss: 2.0290, Perplexity: 7.60628\n",
      "Epoch [1/3], Step [4340/6471], Loss: 2.1526, Perplexity: 8.60759\n",
      "Epoch [1/3], Step [4350/6471], Loss: 2.3614, Perplexity: 10.6056\n",
      "Epoch [1/3], Step [4360/6471], Loss: 2.2311, Perplexity: 9.30996\n",
      "Epoch [1/3], Step [4370/6471], Loss: 2.2127, Perplexity: 9.14034\n",
      "Epoch [1/3], Step [4380/6471], Loss: 2.1466, Perplexity: 8.55608\n",
      "Epoch [1/3], Step [4390/6471], Loss: 2.5299, Perplexity: 12.5519\n",
      "Epoch [1/3], Step [4400/6471], Loss: 2.1917, Perplexity: 8.95013\n",
      "Epoch [1/3], Step [4410/6471], Loss: 2.3747, Perplexity: 10.7482\n",
      "Epoch [1/3], Step [4420/6471], Loss: 2.3861, Perplexity: 10.8711\n",
      "Epoch [1/3], Step [4430/6471], Loss: 2.1648, Perplexity: 8.71312\n",
      "Epoch [1/3], Step [4440/6471], Loss: 2.3117, Perplexity: 10.0916\n",
      "Epoch [1/3], Step [4450/6471], Loss: 2.4155, Perplexity: 11.1950\n",
      "Epoch [1/3], Step [4460/6471], Loss: 2.1333, Perplexity: 8.44259\n",
      "Epoch [1/3], Step [4470/6471], Loss: 2.0078, Perplexity: 7.44708\n",
      "Epoch [1/3], Step [4480/6471], Loss: 2.0986, Perplexity: 8.15468\n",
      "Epoch [1/3], Step [4490/6471], Loss: 2.0797, Perplexity: 8.00173\n",
      "Epoch [1/3], Step [4500/6471], Loss: 4.7596, Perplexity: 116.6991\n",
      "Epoch [1/3], Step [4510/6471], Loss: 3.4459, Perplexity: 31.3701\n",
      "Epoch [1/3], Step [4520/6471], Loss: 2.3105, Perplexity: 10.0797\n",
      "Epoch [1/3], Step [4530/6471], Loss: 2.3456, Perplexity: 10.4395\n",
      "Epoch [1/3], Step [4540/6471], Loss: 2.3790, Perplexity: 10.7946\n",
      "Epoch [1/3], Step [4550/6471], Loss: 2.1100, Perplexity: 8.24829\n",
      "Epoch [1/3], Step [4560/6471], Loss: 2.8031, Perplexity: 16.4956\n",
      "Epoch [1/3], Step [4570/6471], Loss: 2.2776, Perplexity: 9.75322\n",
      "Epoch [1/3], Step [4580/6471], Loss: 2.3176, Perplexity: 10.15096\n",
      "Epoch [1/3], Step [4590/6471], Loss: 2.3981, Perplexity: 11.0021\n",
      "Epoch [1/3], Step [4600/6471], Loss: 2.2386, Perplexity: 9.37982\n",
      "Epoch [1/3], Step [4610/6471], Loss: 2.3078, Perplexity: 10.0523\n",
      "Epoch [1/3], Step [4620/6471], Loss: 2.3751, Perplexity: 10.7523\n",
      "Epoch [1/3], Step [4630/6471], Loss: 2.1593, Perplexity: 8.66532\n",
      "Epoch [1/3], Step [4640/6471], Loss: 2.3547, Perplexity: 10.5346\n",
      "Epoch [1/3], Step [4650/6471], Loss: 2.9235, Perplexity: 18.6063\n",
      "Epoch [1/3], Step [4660/6471], Loss: 2.4169, Perplexity: 11.2111\n",
      "Epoch [1/3], Step [4670/6471], Loss: 2.0238, Perplexity: 7.56729\n",
      "Epoch [1/3], Step [4680/6471], Loss: 2.1065, Perplexity: 8.21971\n",
      "Epoch [1/3], Step [4690/6471], Loss: 2.2257, Perplexity: 9.25986\n",
      "Epoch [1/3], Step [4700/6471], Loss: 2.1935, Perplexity: 8.96682\n",
      "Epoch [1/3], Step [4710/6471], Loss: 2.2228, Perplexity: 9.23304\n",
      "Epoch [1/3], Step [4720/6471], Loss: 2.3286, Perplexity: 10.2634\n",
      "Epoch [1/3], Step [4730/6471], Loss: 2.1799, Perplexity: 8.84548\n",
      "Epoch [1/3], Step [4740/6471], Loss: 1.9490, Perplexity: 7.02143\n",
      "Epoch [1/3], Step [4750/6471], Loss: 2.0769, Perplexity: 7.97998\n",
      "Epoch [1/3], Step [4760/6471], Loss: 2.5001, Perplexity: 12.1831\n",
      "Epoch [1/3], Step [4770/6471], Loss: 2.1912, Perplexity: 8.94552\n",
      "Epoch [1/3], Step [4780/6471], Loss: 2.4179, Perplexity: 11.2224\n",
      "Epoch [1/3], Step [4790/6471], Loss: 2.1416, Perplexity: 8.51272\n",
      "Epoch [1/3], Step [4820/6471], Loss: 2.1359, Perplexity: 8.46515\n",
      "Epoch [1/3], Step [4830/6471], Loss: 2.1446, Perplexity: 8.53850\n",
      "Epoch [1/3], Step [4840/6471], Loss: 2.3546, Perplexity: 10.5341\n",
      "Epoch [1/3], Step [4850/6471], Loss: 2.2360, Perplexity: 9.35558\n",
      "Epoch [1/3], Step [4860/6471], Loss: 2.1319, Perplexity: 8.43112\n",
      "Epoch [1/3], Step [4870/6471], Loss: 2.2375, Perplexity: 9.37033\n",
      "Epoch [1/3], Step [4880/6471], Loss: 2.0759, Perplexity: 7.97188\n",
      "Epoch [1/3], Step [4890/6471], Loss: 2.3370, Perplexity: 10.3503\n",
      "Epoch [1/3], Step [4900/6471], Loss: 2.6902, Perplexity: 14.7353\n",
      "Epoch [1/3], Step [4910/6471], Loss: 2.2487, Perplexity: 9.47554\n",
      "Epoch [1/3], Step [4920/6471], Loss: 2.1326, Perplexity: 8.43645\n",
      "Epoch [1/3], Step [4930/6471], Loss: 2.2657, Perplexity: 9.63838\n",
      "Epoch [1/3], Step [4940/6471], Loss: 2.2894, Perplexity: 9.86917\n",
      "Epoch [1/3], Step [4950/6471], Loss: 2.1685, Perplexity: 8.74501\n",
      "Epoch [1/3], Step [4960/6471], Loss: 2.2287, Perplexity: 9.28817\n",
      "Epoch [1/3], Step [4970/6471], Loss: 2.2845, Perplexity: 9.82073\n",
      "Epoch [1/3], Step [4980/6471], Loss: 2.2009, Perplexity: 9.03303\n",
      "Epoch [1/3], Step [4990/6471], Loss: 2.4378, Perplexity: 11.4480\n",
      "Epoch [1/3], Step [5000/6471], Loss: 2.3367, Perplexity: 10.3468\n",
      "Epoch [1/3], Step [5010/6471], Loss: 2.3222, Perplexity: 10.1979\n",
      "Epoch [1/3], Step [5020/6471], Loss: 2.3485, Perplexity: 10.4703\n",
      "Epoch [1/3], Step [5030/6471], Loss: 2.1994, Perplexity: 9.01954\n",
      "Epoch [1/3], Step [5040/6471], Loss: 2.3261, Perplexity: 10.2380\n",
      "Epoch [1/3], Step [5050/6471], Loss: 2.2266, Perplexity: 9.26819\n",
      "Epoch [1/3], Step [5060/6471], Loss: 2.3628, Perplexity: 10.6202\n",
      "Epoch [1/3], Step [5070/6471], Loss: 2.2094, Perplexity: 9.11067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [5080/6471], Loss: 2.0906, Perplexity: 8.08979\n",
      "Epoch [1/3], Step [5090/6471], Loss: 2.2758, Perplexity: 9.73610\n",
      "Epoch [1/3], Step [5100/6471], Loss: 2.3978, Perplexity: 10.9989\n",
      "Epoch [1/3], Step [5120/6471], Loss: 2.0787, Perplexity: 7.99444\n",
      "Epoch [1/3], Step [5130/6471], Loss: 3.1460, Perplexity: 23.2436\n",
      "Epoch [1/3], Step [5140/6471], Loss: 2.1303, Perplexity: 8.41777\n",
      "Epoch [1/3], Step [5150/6471], Loss: 2.2297, Perplexity: 9.29694\n",
      "Epoch [1/3], Step [5160/6471], Loss: 2.1246, Perplexity: 8.36993\n",
      "Epoch [1/3], Step [5170/6471], Loss: 2.5225, Perplexity: 12.4601\n",
      "Epoch [1/3], Step [5180/6471], Loss: 2.0552, Perplexity: 7.80870\n",
      "Epoch [1/3], Step [5190/6471], Loss: 2.0132, Perplexity: 7.48752\n",
      "Epoch [1/3], Step [5200/6471], Loss: 2.1077, Perplexity: 8.22907\n",
      "Epoch [1/3], Step [5210/6471], Loss: 2.4201, Perplexity: 11.2473\n",
      "Epoch [1/3], Step [5220/6471], Loss: 2.4272, Perplexity: 11.3266\n",
      "Epoch [1/3], Step [5230/6471], Loss: 2.5384, Perplexity: 12.6591\n",
      "Epoch [1/3], Step [5240/6471], Loss: 2.1502, Perplexity: 8.58672\n",
      "Epoch [1/3], Step [5250/6471], Loss: 2.1325, Perplexity: 8.43553\n",
      "Epoch [1/3], Step [5260/6471], Loss: 1.9972, Perplexity: 7.36859\n",
      "Epoch [1/3], Step [5270/6471], Loss: 2.3649, Perplexity: 10.6426\n",
      "Epoch [1/3], Step [5280/6471], Loss: 2.2179, Perplexity: 9.18776\n",
      "Epoch [1/3], Step [5290/6471], Loss: 2.4521, Perplexity: 11.6123\n",
      "Epoch [1/3], Step [5300/6471], Loss: 2.2049, Perplexity: 9.06926\n",
      "Epoch [1/3], Step [5310/6471], Loss: 2.0744, Perplexity: 7.96004\n",
      "Epoch [1/3], Step [5320/6471], Loss: 2.2913, Perplexity: 9.88811\n",
      "Epoch [1/3], Step [5330/6471], Loss: 2.1791, Perplexity: 8.83874\n",
      "Epoch [1/3], Step [5340/6471], Loss: 2.2348, Perplexity: 9.34435\n",
      "Epoch [1/3], Step [5350/6471], Loss: 2.1553, Perplexity: 8.63075\n",
      "Epoch [1/3], Step [5360/6471], Loss: 2.1480, Perplexity: 8.56753\n",
      "Epoch [1/3], Step [5370/6471], Loss: 2.2283, Perplexity: 9.28407\n",
      "Epoch [1/3], Step [5380/6471], Loss: 2.3081, Perplexity: 10.0556\n",
      "Epoch [1/3], Step [5390/6471], Loss: 2.4560, Perplexity: 11.6582\n",
      "Epoch [1/3], Step [5400/6471], Loss: 2.4812, Perplexity: 11.9553\n",
      "Epoch [1/3], Step [5410/6471], Loss: 2.6376, Perplexity: 13.9792\n",
      "Epoch [1/3], Step [5420/6471], Loss: 2.1169, Perplexity: 8.3050\n",
      "Epoch [1/3], Step [5430/6471], Loss: 2.3014, Perplexity: 9.98836\n",
      "Epoch [1/3], Step [5440/6471], Loss: 2.1580, Perplexity: 8.65408\n",
      "Epoch [1/3], Step [5450/6471], Loss: 2.5326, Perplexity: 12.5866\n",
      "Epoch [1/3], Step [5460/6471], Loss: 2.2665, Perplexity: 9.64579\n",
      "Epoch [1/3], Step [5470/6471], Loss: 2.1304, Perplexity: 8.41855\n",
      "Epoch [1/3], Step [5480/6471], Loss: 2.2852, Perplexity: 9.82764\n",
      "Epoch [1/3], Step [5490/6471], Loss: 2.0263, Perplexity: 7.58566\n",
      "Epoch [1/3], Step [5500/6471], Loss: 2.6855, Perplexity: 14.6661\n",
      "Epoch [1/3], Step [5510/6471], Loss: 2.1601, Perplexity: 8.67234\n",
      "Epoch [1/3], Step [5520/6471], Loss: 2.2883, Perplexity: 9.85786\n",
      "Epoch [1/3], Step [5530/6471], Loss: 2.1286, Perplexity: 8.40351\n",
      "Epoch [1/3], Step [5540/6471], Loss: 2.3573, Perplexity: 10.5619\n",
      "Epoch [1/3], Step [5550/6471], Loss: 2.1095, Perplexity: 8.24375\n",
      "Epoch [1/3], Step [5560/6471], Loss: 1.9625, Perplexity: 7.11731\n",
      "Epoch [1/3], Step [5570/6471], Loss: 2.3113, Perplexity: 10.0878\n",
      "Epoch [1/3], Step [5580/6471], Loss: 2.0098, Perplexity: 7.46168\n",
      "Epoch [1/3], Step [5590/6471], Loss: 2.1951, Perplexity: 8.98111\n",
      "Epoch [1/3], Step [5600/6471], Loss: 2.4805, Perplexity: 11.9473\n",
      "Epoch [1/3], Step [5610/6471], Loss: 2.2841, Perplexity: 9.8172\n",
      "Epoch [1/3], Step [5620/6471], Loss: 2.3031, Perplexity: 10.0052\n",
      "Epoch [1/3], Step [5630/6471], Loss: 2.1498, Perplexity: 8.58301\n",
      "Epoch [1/3], Step [5640/6471], Loss: 1.9910, Perplexity: 7.32251\n",
      "Epoch [1/3], Step [5650/6471], Loss: 2.0894, Perplexity: 8.07997\n",
      "Epoch [1/3], Step [5660/6471], Loss: 2.1778, Perplexity: 8.82691\n",
      "Epoch [1/3], Step [5670/6471], Loss: 2.2610, Perplexity: 9.59290\n",
      "Epoch [1/3], Step [5680/6471], Loss: 2.2739, Perplexity: 9.71738\n",
      "Epoch [1/3], Step [5690/6471], Loss: 2.1349, Perplexity: 8.45640\n",
      "Epoch [1/3], Step [5700/6471], Loss: 2.1025, Perplexity: 8.18668\n",
      "Epoch [1/3], Step [5710/6471], Loss: 2.0174, Perplexity: 7.51890\n",
      "Epoch [1/3], Step [5720/6471], Loss: 2.9804, Perplexity: 19.6950\n",
      "Epoch [1/3], Step [5730/6471], Loss: 1.9749, Perplexity: 7.20563\n",
      "Epoch [1/3], Step [5740/6471], Loss: 2.0578, Perplexity: 7.82843\n",
      "Epoch [1/3], Step [5750/6471], Loss: 1.9090, Perplexity: 6.74669\n",
      "Epoch [1/3], Step [5760/6471], Loss: 2.1008, Perplexity: 8.17240\n",
      "Epoch [1/3], Step [5770/6471], Loss: 2.3073, Perplexity: 10.0471\n",
      "Epoch [1/3], Step [5780/6471], Loss: 2.1122, Perplexity: 8.26678\n",
      "Epoch [1/3], Step [5790/6471], Loss: 2.1269, Perplexity: 8.38872\n",
      "Epoch [1/3], Step [5800/6471], Loss: 2.2811, Perplexity: 9.78781\n",
      "Epoch [1/3], Step [5810/6471], Loss: 2.2036, Perplexity: 9.05806\n",
      "Epoch [1/3], Step [5820/6471], Loss: 2.1385, Perplexity: 8.48683\n",
      "Epoch [1/3], Step [5830/6471], Loss: 2.2619, Perplexity: 9.60142\n",
      "Epoch [1/3], Step [5840/6471], Loss: 2.0353, Perplexity: 7.65437\n",
      "Epoch [1/3], Step [5850/6471], Loss: 2.1545, Perplexity: 8.62372\n",
      "Epoch [1/3], Step [5860/6471], Loss: 2.0820, Perplexity: 8.02052\n",
      "Epoch [1/3], Step [5870/6471], Loss: 2.0045, Perplexity: 7.42262\n",
      "Epoch [1/3], Step [5880/6471], Loss: 2.1445, Perplexity: 8.53754\n",
      "Epoch [1/3], Step [5890/6471], Loss: 2.1656, Perplexity: 8.72009\n",
      "Epoch [1/3], Step [5900/6471], Loss: 2.0340, Perplexity: 7.64497\n",
      "Epoch [1/3], Step [5910/6471], Loss: 2.3653, Perplexity: 10.6470\n",
      "Epoch [1/3], Step [5920/6471], Loss: 2.1636, Perplexity: 8.70226\n",
      "Epoch [1/3], Step [5930/6471], Loss: 2.4039, Perplexity: 11.0657\n",
      "Epoch [1/3], Step [5940/6471], Loss: 2.4212, Perplexity: 11.2593\n",
      "Epoch [1/3], Step [5950/6471], Loss: 2.1422, Perplexity: 8.51850\n",
      "Epoch [1/3], Step [5960/6471], Loss: 2.5608, Perplexity: 12.9466\n",
      "Epoch [1/3], Step [5970/6471], Loss: 2.2076, Perplexity: 9.09370\n",
      "Epoch [1/3], Step [5980/6471], Loss: 2.2448, Perplexity: 9.43880\n",
      "Epoch [1/3], Step [5990/6471], Loss: 2.2846, Perplexity: 9.8213\n",
      "Epoch [1/3], Step [6000/6471], Loss: 2.2687, Perplexity: 9.66679\n",
      "Epoch [1/3], Step [6010/6471], Loss: 2.3958, Perplexity: 10.9772\n",
      "Epoch [1/3], Step [6020/6471], Loss: 2.1368, Perplexity: 8.47270\n",
      "Epoch [1/3], Step [6030/6471], Loss: 1.9924, Perplexity: 7.33348\n",
      "Epoch [1/3], Step [6040/6471], Loss: 2.1420, Perplexity: 8.51634\n",
      "Epoch [1/3], Step [6050/6471], Loss: 1.9496, Perplexity: 7.02576\n",
      "Epoch [1/3], Step [6060/6471], Loss: 2.2667, Perplexity: 9.64790\n",
      "Epoch [1/3], Step [6070/6471], Loss: 2.1093, Perplexity: 8.24234\n",
      "Epoch [1/3], Step [6080/6471], Loss: 2.1762, Perplexity: 8.81256\n",
      "Epoch [1/3], Step [6090/6471], Loss: 2.0805, Perplexity: 8.0084\n",
      "Epoch [1/3], Step [6100/6471], Loss: 2.1187, Perplexity: 8.32041\n",
      "Epoch [1/3], Step [6110/6471], Loss: 2.1654, Perplexity: 8.71858\n",
      "Epoch [1/3], Step [6120/6471], Loss: 2.1260, Perplexity: 8.38097\n",
      "Epoch [1/3], Step [6130/6471], Loss: 2.1837, Perplexity: 8.8792\n",
      "Epoch [1/3], Step [6140/6471], Loss: 2.1158, Perplexity: 8.29646\n",
      "Epoch [1/3], Step [6150/6471], Loss: 2.4855, Perplexity: 12.0069\n",
      "Epoch [1/3], Step [6160/6471], Loss: 1.9701, Perplexity: 7.17140\n",
      "Epoch [1/3], Step [6170/6471], Loss: 2.2979, Perplexity: 9.95318\n",
      "Epoch [1/3], Step [6180/6471], Loss: 2.0347, Perplexity: 7.64968\n",
      "Epoch [1/3], Step [6190/6471], Loss: 2.0563, Perplexity: 7.81670\n",
      "Epoch [1/3], Step [6200/6471], Loss: 2.3655, Perplexity: 10.6493\n",
      "Epoch [1/3], Step [6210/6471], Loss: 2.1916, Perplexity: 8.94953\n",
      "Epoch [1/3], Step [6220/6471], Loss: 2.1302, Perplexity: 8.41629\n",
      "Epoch [1/3], Step [6230/6471], Loss: 2.3579, Perplexity: 10.5691\n",
      "Epoch [1/3], Step [6240/6471], Loss: 2.7195, Perplexity: 15.1722\n",
      "Epoch [1/3], Step [6250/6471], Loss: 1.8813, Perplexity: 6.56234\n",
      "Epoch [1/3], Step [6260/6471], Loss: 2.1611, Perplexity: 8.68112\n",
      "Epoch [1/3], Step [6270/6471], Loss: 2.0611, Perplexity: 7.85482\n",
      "Epoch [1/3], Step [6280/6471], Loss: 2.1464, Perplexity: 8.55410\n",
      "Epoch [1/3], Step [6290/6471], Loss: 2.7221, Perplexity: 15.2117\n",
      "Epoch [1/3], Step [6300/6471], Loss: 2.0133, Perplexity: 7.48835\n",
      "Epoch [1/3], Step [6310/6471], Loss: 2.4252, Perplexity: 11.3042\n",
      "Epoch [1/3], Step [6320/6471], Loss: 2.2368, Perplexity: 9.36338\n",
      "Epoch [1/3], Step [6330/6471], Loss: 2.0777, Perplexity: 7.9863\n",
      "Epoch [1/3], Step [6340/6471], Loss: 2.1414, Perplexity: 8.51104\n",
      "Epoch [1/3], Step [6350/6471], Loss: 1.9124, Perplexity: 6.76966\n",
      "Epoch [1/3], Step [6360/6471], Loss: 2.1061, Perplexity: 8.21633\n",
      "Epoch [1/3], Step [6370/6471], Loss: 1.9120, Perplexity: 6.76657\n",
      "Epoch [1/3], Step [6380/6471], Loss: 2.6142, Perplexity: 13.6564\n",
      "Epoch [1/3], Step [6390/6471], Loss: 2.1748, Perplexity: 8.80086\n",
      "Epoch [1/3], Step [6400/6471], Loss: 2.1470, Perplexity: 8.55896\n",
      "Epoch [1/3], Step [6410/6471], Loss: 2.1453, Perplexity: 8.54434\n",
      "Epoch [1/3], Step [6420/6471], Loss: 2.6259, Perplexity: 13.8167\n",
      "Epoch [1/3], Step [6430/6471], Loss: 2.2799, Perplexity: 9.77532\n",
      "Epoch [1/3], Step [6440/6471], Loss: 2.0491, Perplexity: 7.76064\n",
      "Epoch [1/3], Step [6450/6471], Loss: 1.9810, Perplexity: 7.24994\n",
      "Epoch [1/3], Step [6460/6471], Loss: 2.5743, Perplexity: 13.1218\n",
      "Epoch [1/3], Step [6470/6471], Loss: 2.5350, Perplexity: 12.6163\n",
      "Epoch [2/3], Step [10/6471], Loss: 2.2022, Perplexity: 9.0451158\n",
      "Epoch [2/3], Step [20/6471], Loss: 1.9458, Perplexity: 6.99919\n",
      "Epoch [2/3], Step [30/6471], Loss: 2.1763, Perplexity: 8.81404\n",
      "Epoch [2/3], Step [40/6471], Loss: 2.1065, Perplexity: 8.21956\n",
      "Epoch [2/3], Step [50/6471], Loss: 2.3039, Perplexity: 10.0128\n",
      "Epoch [2/3], Step [60/6471], Loss: 2.3438, Perplexity: 10.4209\n",
      "Epoch [2/3], Step [70/6471], Loss: 2.1367, Perplexity: 8.47163\n",
      "Epoch [2/3], Step [80/6471], Loss: 2.2739, Perplexity: 9.71685\n",
      "Epoch [2/3], Step [90/6471], Loss: 2.6317, Perplexity: 13.8980\n",
      "Epoch [2/3], Step [100/6471], Loss: 2.0859, Perplexity: 8.0520\n",
      "Epoch [2/3], Step [110/6471], Loss: 2.2543, Perplexity: 9.52869\n",
      "Epoch [2/3], Step [120/6471], Loss: 2.1571, Perplexity: 8.6460\n",
      "Epoch [2/3], Step [130/6471], Loss: 2.1980, Perplexity: 9.00664\n",
      "Epoch [2/3], Step [140/6471], Loss: 2.1179, Perplexity: 8.31341\n",
      "Epoch [2/3], Step [150/6471], Loss: 2.1783, Perplexity: 8.83103\n",
      "Epoch [2/3], Step [160/6471], Loss: 2.5871, Perplexity: 13.2917\n",
      "Epoch [2/3], Step [170/6471], Loss: 1.9976, Perplexity: 7.37171\n",
      "Epoch [2/3], Step [180/6471], Loss: 1.9304, Perplexity: 6.89212\n",
      "Epoch [2/3], Step [190/6471], Loss: 2.2022, Perplexity: 9.04514\n",
      "Epoch [2/3], Step [200/6471], Loss: 2.2470, Perplexity: 9.45949\n",
      "Epoch [2/3], Step [210/6471], Loss: 2.1061, Perplexity: 8.2161\n",
      "Epoch [2/3], Step [220/6471], Loss: 2.0590, Perplexity: 7.83808\n",
      "Epoch [2/3], Step [230/6471], Loss: 2.2778, Perplexity: 9.75522\n",
      "Epoch [2/3], Step [240/6471], Loss: 2.2291, Perplexity: 9.29184\n",
      "Epoch [2/3], Step [250/6471], Loss: 2.2229, Perplexity: 9.23384\n",
      "Epoch [2/3], Step [260/6471], Loss: 2.0451, Perplexity: 7.72992\n",
      "Epoch [2/3], Step [270/6471], Loss: 2.2702, Perplexity: 9.68166\n",
      "Epoch [2/3], Step [280/6471], Loss: 2.2344, Perplexity: 9.34134\n",
      "Epoch [2/3], Step [290/6471], Loss: 2.3604, Perplexity: 10.5948\n",
      "Epoch [2/3], Step [300/6471], Loss: 2.1925, Perplexity: 8.95794\n",
      "Epoch [2/3], Step [310/6471], Loss: 2.1792, Perplexity: 8.83925\n",
      "Epoch [2/3], Step [320/6471], Loss: 1.9733, Perplexity: 7.19457\n",
      "Epoch [2/3], Step [330/6471], Loss: 2.0926, Perplexity: 8.10610\n",
      "Epoch [2/3], Step [340/6471], Loss: 2.3082, Perplexity: 10.0567\n",
      "Epoch [2/3], Step [350/6471], Loss: 2.1213, Perplexity: 8.34210\n",
      "Epoch [2/3], Step [360/6471], Loss: 2.2202, Perplexity: 9.2092\n",
      "Epoch [2/3], Step [370/6471], Loss: 2.2407, Perplexity: 9.39987\n",
      "Epoch [2/3], Step [380/6471], Loss: 2.0506, Perplexity: 7.77287\n",
      "Epoch [2/3], Step [390/6471], Loss: 2.0793, Perplexity: 7.99913\n",
      "Epoch [2/3], Step [400/6471], Loss: 2.1930, Perplexity: 8.96190\n",
      "Epoch [2/3], Step [410/6471], Loss: 2.0214, Perplexity: 7.54858\n",
      "Epoch [2/3], Step [420/6471], Loss: 2.5493, Perplexity: 12.7980\n",
      "Epoch [2/3], Step [430/6471], Loss: 2.3511, Perplexity: 10.4975\n",
      "Epoch [2/3], Step [440/6471], Loss: 2.2086, Perplexity: 9.10308\n",
      "Epoch [2/3], Step [450/6471], Loss: 2.5708, Perplexity: 13.0760\n",
      "Epoch [2/3], Step [460/6471], Loss: 2.0784, Perplexity: 7.99163\n",
      "Epoch [2/3], Step [470/6471], Loss: 2.2103, Perplexity: 9.11848\n",
      "Epoch [2/3], Step [480/6471], Loss: 2.0138, Perplexity: 7.49190\n",
      "Epoch [2/3], Step [490/6471], Loss: 2.2649, Perplexity: 9.63064\n",
      "Epoch [2/3], Step [500/6471], Loss: 2.1002, Perplexity: 8.16758\n",
      "Epoch [2/3], Step [510/6471], Loss: 2.0592, Perplexity: 7.83959\n",
      "Epoch [2/3], Step [520/6471], Loss: 2.1107, Perplexity: 8.25362\n",
      "Epoch [2/3], Step [530/6471], Loss: 2.3295, Perplexity: 10.2727\n",
      "Epoch [2/3], Step [540/6471], Loss: 1.9990, Perplexity: 7.38163\n",
      "Epoch [2/3], Step [550/6471], Loss: 2.2038, Perplexity: 9.05903\n",
      "Epoch [2/3], Step [560/6471], Loss: 2.4095, Perplexity: 11.1287\n",
      "Epoch [2/3], Step [570/6471], Loss: 2.0909, Perplexity: 8.09263\n",
      "Epoch [2/3], Step [580/6471], Loss: 2.1259, Perplexity: 8.38010\n",
      "Epoch [2/3], Step [590/6471], Loss: 2.2271, Perplexity: 9.27270\n",
      "Epoch [2/3], Step [600/6471], Loss: 2.0761, Perplexity: 7.97335\n",
      "Epoch [2/3], Step [610/6471], Loss: 2.8509, Perplexity: 17.3029\n",
      "Epoch [2/3], Step [620/6471], Loss: 2.5098, Perplexity: 12.3026\n",
      "Epoch [2/3], Step [630/6471], Loss: 2.5093, Perplexity: 12.2960\n",
      "Epoch [2/3], Step [640/6471], Loss: 2.2261, Perplexity: 9.26362\n",
      "Epoch [2/3], Step [650/6471], Loss: 2.0863, Perplexity: 8.05544\n",
      "Epoch [2/3], Step [660/6471], Loss: 2.0827, Perplexity: 8.02630\n",
      "Epoch [2/3], Step [670/6471], Loss: 2.2485, Perplexity: 9.47362\n",
      "Epoch [2/3], Step [680/6471], Loss: 2.2117, Perplexity: 9.13106\n",
      "Epoch [2/3], Step [690/6471], Loss: 2.0178, Perplexity: 7.52195\n",
      "Epoch [2/3], Step [700/6471], Loss: 2.3871, Perplexity: 10.8819\n",
      "Epoch [2/3], Step [710/6471], Loss: 2.2575, Perplexity: 9.55883\n",
      "Epoch [2/3], Step [720/6471], Loss: 2.8565, Perplexity: 17.4003\n",
      "Epoch [2/3], Step [730/6471], Loss: 2.2296, Perplexity: 9.29598\n",
      "Epoch [2/3], Step [740/6471], Loss: 2.4187, Perplexity: 11.2308\n",
      "Epoch [2/3], Step [750/6471], Loss: 2.2654, Perplexity: 9.63459\n",
      "Epoch [2/3], Step [760/6471], Loss: 2.3461, Perplexity: 10.4442\n",
      "Epoch [2/3], Step [770/6471], Loss: 2.0988, Perplexity: 8.1561\n",
      "Epoch [2/3], Step [780/6471], Loss: 2.0249, Perplexity: 7.57544\n",
      "Epoch [2/3], Step [790/6471], Loss: 2.2835, Perplexity: 9.81066\n",
      "Epoch [2/3], Step [800/6471], Loss: 2.4213, Perplexity: 11.2599\n",
      "Epoch [2/3], Step [810/6471], Loss: 2.3164, Perplexity: 10.1391\n",
      "Epoch [2/3], Step [820/6471], Loss: 2.2545, Perplexity: 9.53030\n",
      "Epoch [2/3], Step [830/6471], Loss: 2.2783, Perplexity: 9.7606\n",
      "Epoch [2/3], Step [840/6471], Loss: 3.5125, Perplexity: 33.5337\n",
      "Epoch [2/3], Step [850/6471], Loss: 2.1689, Perplexity: 8.74896\n",
      "Epoch [2/3], Step [860/6471], Loss: 2.4533, Perplexity: 11.6270\n",
      "Epoch [2/3], Step [870/6471], Loss: 2.2137, Perplexity: 9.14918\n",
      "Epoch [2/3], Step [880/6471], Loss: 2.2373, Perplexity: 9.36797\n",
      "Epoch [2/3], Step [890/6471], Loss: 2.2518, Perplexity: 9.50456\n",
      "Epoch [2/3], Step [900/6471], Loss: 2.1751, Perplexity: 8.80320\n",
      "Epoch [2/3], Step [910/6471], Loss: 2.0895, Perplexity: 8.08052\n",
      "Epoch [2/3], Step [920/6471], Loss: 1.9488, Perplexity: 7.02053\n",
      "Epoch [2/3], Step [930/6471], Loss: 2.3489, Perplexity: 10.4742\n",
      "Epoch [2/3], Step [940/6471], Loss: 2.0281, Perplexity: 7.59951\n",
      "Epoch [2/3], Step [950/6471], Loss: 2.0760, Perplexity: 7.97237\n",
      "Epoch [2/3], Step [960/6471], Loss: 2.2445, Perplexity: 9.4358\n",
      "Epoch [2/3], Step [970/6471], Loss: 2.2205, Perplexity: 9.21237\n",
      "Epoch [2/3], Step [980/6471], Loss: 2.3370, Perplexity: 10.3503\n",
      "Epoch [2/3], Step [990/6471], Loss: 2.1619, Perplexity: 8.68727\n",
      "Epoch [2/3], Step [1000/6471], Loss: 1.9033, Perplexity: 6.7083\n",
      "Epoch [2/3], Step [1010/6471], Loss: 2.1372, Perplexity: 8.47543\n",
      "Epoch [2/3], Step [1020/6471], Loss: 2.0976, Perplexity: 8.14676\n",
      "Epoch [2/3], Step [1030/6471], Loss: 2.0295, Perplexity: 7.61007\n",
      "Epoch [2/3], Step [1040/6471], Loss: 1.8917, Perplexity: 6.63077\n",
      "Epoch [2/3], Step [1050/6471], Loss: 2.2709, Perplexity: 9.68805\n",
      "Epoch [2/3], Step [1060/6471], Loss: 2.9582, Perplexity: 19.2626\n",
      "Epoch [2/3], Step [1070/6471], Loss: 2.1814, Perplexity: 8.85870\n",
      "Epoch [2/3], Step [1080/6471], Loss: 2.1854, Perplexity: 8.89458\n",
      "Epoch [2/3], Step [1090/6471], Loss: 2.1656, Perplexity: 8.72026\n",
      "Epoch [2/3], Step [1100/6471], Loss: 2.1822, Perplexity: 8.86609\n",
      "Epoch [2/3], Step [1110/6471], Loss: 2.2039, Perplexity: 9.06078\n",
      "Epoch [2/3], Step [1120/6471], Loss: 2.0705, Perplexity: 7.9290\n",
      "Epoch [2/3], Step [1130/6471], Loss: 2.0077, Perplexity: 7.44593\n",
      "Epoch [2/3], Step [1140/6471], Loss: 2.4400, Perplexity: 11.4729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [1150/6471], Loss: 2.4608, Perplexity: 11.7137\n",
      "Epoch [2/3], Step [1160/6471], Loss: 2.0901, Perplexity: 8.08613\n",
      "Epoch [2/3], Step [1170/6471], Loss: 2.0822, Perplexity: 8.02253\n",
      "Epoch [2/3], Step [1180/6471], Loss: 2.3325, Perplexity: 10.3032\n",
      "Epoch [2/3], Step [1190/6471], Loss: 2.4536, Perplexity: 11.6306\n",
      "Epoch [2/3], Step [1200/6471], Loss: 1.9695, Perplexity: 7.16673\n",
      "Epoch [2/3], Step [1210/6471], Loss: 2.2360, Perplexity: 9.35591\n",
      "Epoch [2/3], Step [1220/6471], Loss: 2.0586, Perplexity: 7.83545\n",
      "Epoch [2/3], Step [1230/6471], Loss: 2.2384, Perplexity: 9.37806\n",
      "Epoch [2/3], Step [1240/6471], Loss: 2.1266, Perplexity: 8.38667\n",
      "Epoch [2/3], Step [1250/6471], Loss: 2.0715, Perplexity: 7.93691\n",
      "Epoch [2/3], Step [1260/6471], Loss: 2.2723, Perplexity: 9.70139\n",
      "Epoch [2/3], Step [1270/6471], Loss: 2.0800, Perplexity: 8.00434\n",
      "Epoch [2/3], Step [1280/6471], Loss: 2.0976, Perplexity: 8.14664\n",
      "Epoch [2/3], Step [1290/6471], Loss: 2.2599, Perplexity: 9.5823\n",
      "Epoch [2/3], Step [1300/6471], Loss: 2.0162, Perplexity: 7.51011\n",
      "Epoch [2/3], Step [1310/6471], Loss: 2.1283, Perplexity: 8.40077\n",
      "Epoch [2/3], Step [1320/6471], Loss: 2.0143, Perplexity: 7.49580\n",
      "Epoch [2/3], Step [1330/6471], Loss: 2.2520, Perplexity: 9.5067\n",
      "Epoch [2/3], Step [1340/6471], Loss: 2.2163, Perplexity: 9.17308\n",
      "Epoch [2/3], Step [1350/6471], Loss: 2.1584, Perplexity: 8.65778\n",
      "Epoch [2/3], Step [1360/6471], Loss: 1.9501, Perplexity: 7.02938\n",
      "Epoch [2/3], Step [1370/6471], Loss: 2.0774, Perplexity: 7.98331\n",
      "Epoch [2/3], Step [1380/6471], Loss: 2.1058, Perplexity: 8.21412\n",
      "Epoch [2/3], Step [1390/6471], Loss: 1.9502, Perplexity: 7.0298\n",
      "Epoch [2/3], Step [1400/6471], Loss: 2.1711, Perplexity: 8.76762\n",
      "Epoch [2/3], Step [1410/6471], Loss: 2.0890, Perplexity: 8.07702\n",
      "Epoch [2/3], Step [1420/6471], Loss: 2.1097, Perplexity: 8.24603\n",
      "Epoch [2/3], Step [1430/6471], Loss: 2.0597, Perplexity: 7.8440\n",
      "Epoch [2/3], Step [1440/6471], Loss: 2.2860, Perplexity: 9.83546\n",
      "Epoch [2/3], Step [1450/6471], Loss: 2.3208, Perplexity: 10.1836\n",
      "Epoch [2/3], Step [1460/6471], Loss: 2.3465, Perplexity: 10.4488\n",
      "Epoch [2/3], Step [1470/6471], Loss: 1.9458, Perplexity: 6.9995\n",
      "Epoch [2/3], Step [1480/6471], Loss: 2.2426, Perplexity: 9.41768\n",
      "Epoch [2/3], Step [1490/6471], Loss: 2.1368, Perplexity: 8.47208\n",
      "Epoch [2/3], Step [1500/6471], Loss: 2.0093, Perplexity: 7.45826\n",
      "Epoch [2/3], Step [1510/6471], Loss: 2.5184, Perplexity: 12.4089\n",
      "Epoch [2/3], Step [1520/6471], Loss: 2.4210, Perplexity: 11.2575\n",
      "Epoch [2/3], Step [1530/6471], Loss: 2.0711, Perplexity: 7.9334\n",
      "Epoch [2/3], Step [1540/6471], Loss: 2.2842, Perplexity: 9.8183\n",
      "Epoch [2/3], Step [1550/6471], Loss: 2.1450, Perplexity: 8.54209\n",
      "Epoch [2/3], Step [1560/6471], Loss: 2.0349, Perplexity: 7.65170\n",
      "Epoch [2/3], Step [1570/6471], Loss: 2.2286, Perplexity: 9.28659\n",
      "Epoch [2/3], Step [1580/6471], Loss: 2.2612, Perplexity: 9.59450\n",
      "Epoch [2/3], Step [1590/6471], Loss: 2.6990, Perplexity: 14.8647\n",
      "Epoch [2/3], Step [1600/6471], Loss: 1.9513, Perplexity: 7.03793\n",
      "Epoch [2/3], Step [1610/6471], Loss: 2.1529, Perplexity: 8.60963\n",
      "Epoch [2/3], Step [1620/6471], Loss: 2.0459, Perplexity: 7.73648\n",
      "Epoch [2/3], Step [1630/6471], Loss: 2.2642, Perplexity: 9.62330\n",
      "Epoch [2/3], Step [1640/6471], Loss: 2.7407, Perplexity: 15.4982\n",
      "Epoch [2/3], Step [1650/6471], Loss: 2.1375, Perplexity: 8.4784\n",
      "Epoch [2/3], Step [1660/6471], Loss: 2.4249, Perplexity: 11.3011\n",
      "Epoch [2/3], Step [1670/6471], Loss: 2.1476, Perplexity: 8.56395\n",
      "Epoch [2/3], Step [1680/6471], Loss: 2.5620, Perplexity: 12.9614\n",
      "Epoch [2/3], Step [1690/6471], Loss: 2.0950, Perplexity: 8.12533\n",
      "Epoch [2/3], Step [1700/6471], Loss: 2.2368, Perplexity: 9.36370\n",
      "Epoch [2/3], Step [1710/6471], Loss: 2.0449, Perplexity: 7.7283\n",
      "Epoch [2/3], Step [1720/6471], Loss: 2.1108, Perplexity: 8.25513\n",
      "Epoch [2/3], Step [1730/6471], Loss: 2.2909, Perplexity: 9.8840\n",
      "Epoch [2/3], Step [1740/6471], Loss: 2.3255, Perplexity: 10.2317\n",
      "Epoch [2/3], Step [1750/6471], Loss: 2.2704, Perplexity: 9.68338\n",
      "Epoch [2/3], Step [1760/6471], Loss: 2.0641, Perplexity: 7.8786\n",
      "Epoch [2/3], Step [1770/6471], Loss: 2.2252, Perplexity: 9.25540\n",
      "Epoch [2/3], Step [1780/6471], Loss: 2.0751, Perplexity: 7.96520\n",
      "Epoch [2/3], Step [1790/6471], Loss: 2.3100, Perplexity: 10.0747\n",
      "Epoch [2/3], Step [1800/6471], Loss: 2.0415, Perplexity: 7.70248\n",
      "Epoch [2/3], Step [1810/6471], Loss: 2.0882, Perplexity: 8.0704\n",
      "Epoch [2/3], Step [1820/6471], Loss: 2.1768, Perplexity: 8.8179\n",
      "Epoch [2/3], Step [1830/6471], Loss: 1.8648, Perplexity: 6.45458\n",
      "Epoch [2/3], Step [1840/6471], Loss: 2.3517, Perplexity: 10.5033\n",
      "Epoch [2/3], Step [1850/6471], Loss: 2.1037, Perplexity: 8.19626\n",
      "Epoch [2/3], Step [1860/6471], Loss: 2.0749, Perplexity: 7.96384\n",
      "Epoch [2/3], Step [1870/6471], Loss: 1.8307, Perplexity: 6.23820\n",
      "Epoch [2/3], Step [1880/6471], Loss: 2.2308, Perplexity: 9.30712\n",
      "Epoch [2/3], Step [1890/6471], Loss: 2.1926, Perplexity: 8.95868\n",
      "Epoch [2/3], Step [1900/6471], Loss: 1.9119, Perplexity: 6.76608\n",
      "Epoch [2/3], Step [1910/6471], Loss: 2.1397, Perplexity: 8.49655\n",
      "Epoch [2/3], Step [1920/6471], Loss: 2.0740, Perplexity: 7.95635\n",
      "Epoch [2/3], Step [1930/6471], Loss: 2.2635, Perplexity: 9.61701\n",
      "Epoch [2/3], Step [1940/6471], Loss: 2.5945, Perplexity: 13.3904\n",
      "Epoch [2/3], Step [1950/6471], Loss: 1.9977, Perplexity: 7.3723\n",
      "Epoch [2/3], Step [1960/6471], Loss: 2.0517, Perplexity: 7.78136\n",
      "Epoch [2/3], Step [1970/6471], Loss: 2.2669, Perplexity: 9.64909\n",
      "Epoch [2/3], Step [1980/6471], Loss: 2.3453, Perplexity: 10.4361\n",
      "Epoch [2/3], Step [1990/6471], Loss: 2.2362, Perplexity: 9.35790\n",
      "Epoch [2/3], Step [2000/6471], Loss: 2.2861, Perplexity: 9.83638\n",
      "Epoch [2/3], Step [2010/6471], Loss: 2.1716, Perplexity: 8.77266\n",
      "Epoch [2/3], Step [2020/6471], Loss: 2.2008, Perplexity: 9.03254\n",
      "Epoch [2/3], Step [2030/6471], Loss: 2.1846, Perplexity: 8.88673\n",
      "Epoch [2/3], Step [2040/6471], Loss: 2.1784, Perplexity: 8.83221\n",
      "Epoch [2/3], Step [2050/6471], Loss: 2.1120, Perplexity: 8.26500\n",
      "Epoch [2/3], Step [2060/6471], Loss: 2.0068, Perplexity: 7.43970\n",
      "Epoch [2/3], Step [2070/6471], Loss: 2.0572, Perplexity: 7.82431\n",
      "Epoch [2/3], Step [2080/6471], Loss: 2.0638, Perplexity: 7.87584\n",
      "Epoch [2/3], Step [2090/6471], Loss: 2.2937, Perplexity: 9.91129\n",
      "Epoch [2/3], Step [2100/6471], Loss: 2.5483, Perplexity: 12.7855\n",
      "Epoch [2/3], Step [2110/6471], Loss: 1.9412, Perplexity: 6.96739\n",
      "Epoch [2/3], Step [2120/6471], Loss: 1.9798, Perplexity: 7.24134\n",
      "Epoch [2/3], Step [2130/6471], Loss: 2.4945, Perplexity: 12.1157\n",
      "Epoch [2/3], Step [2140/6471], Loss: 2.3421, Perplexity: 10.4027\n",
      "Epoch [2/3], Step [2150/6471], Loss: 2.5552, Perplexity: 12.8745\n",
      "Epoch [2/3], Step [2160/6471], Loss: 2.2452, Perplexity: 9.44185\n",
      "Epoch [2/3], Step [2170/6471], Loss: 1.8169, Perplexity: 6.1527\n",
      "Epoch [2/3], Step [2180/6471], Loss: 1.9696, Perplexity: 7.16806\n",
      "Epoch [2/3], Step [2190/6471], Loss: 2.6263, Perplexity: 13.8231\n",
      "Epoch [2/3], Step [2200/6471], Loss: 2.2119, Perplexity: 9.13306\n",
      "Epoch [2/3], Step [2210/6471], Loss: 1.9360, Perplexity: 6.93072\n",
      "Epoch [2/3], Step [2220/6471], Loss: 2.2939, Perplexity: 9.91313\n",
      "Epoch [2/3], Step [2230/6471], Loss: 2.1963, Perplexity: 8.99143\n",
      "Epoch [2/3], Step [2240/6471], Loss: 2.1826, Perplexity: 8.86975\n",
      "Epoch [2/3], Step [2250/6471], Loss: 1.9603, Perplexity: 7.10125\n",
      "Epoch [2/3], Step [2260/6471], Loss: 2.2514, Perplexity: 9.50120\n",
      "Epoch [2/3], Step [2270/6471], Loss: 2.1584, Perplexity: 8.6568\n",
      "Epoch [2/3], Step [2280/6471], Loss: 1.9482, Perplexity: 7.01576\n",
      "Epoch [2/3], Step [2290/6471], Loss: 2.2479, Perplexity: 9.46803\n",
      "Epoch [2/3], Step [2300/6471], Loss: 1.9889, Perplexity: 7.30751\n",
      "Epoch [2/3], Step [2310/6471], Loss: 2.1323, Perplexity: 8.43401\n",
      "Epoch [2/3], Step [2320/6471], Loss: 2.1432, Perplexity: 8.52675\n",
      "Epoch [2/3], Step [2330/6471], Loss: 1.9507, Perplexity: 7.03392\n",
      "Epoch [2/3], Step [2340/6471], Loss: 2.2163, Perplexity: 9.17307\n",
      "Epoch [2/3], Step [2350/6471], Loss: 2.4630, Perplexity: 11.7397\n",
      "Epoch [2/3], Step [2360/6471], Loss: 2.6271, Perplexity: 13.8339\n",
      "Epoch [2/3], Step [2370/6471], Loss: 2.0200, Perplexity: 7.53810\n",
      "Epoch [2/3], Step [2380/6471], Loss: 2.2421, Perplexity: 9.41302\n",
      "Epoch [2/3], Step [2390/6471], Loss: 2.0308, Perplexity: 7.62049\n",
      "Epoch [2/3], Step [2400/6471], Loss: 2.0804, Perplexity: 8.00777\n",
      "Epoch [2/3], Step [2410/6471], Loss: 2.0940, Perplexity: 8.11730\n",
      "Epoch [2/3], Step [2420/6471], Loss: 2.5285, Perplexity: 12.5346\n",
      "Epoch [2/3], Step [2430/6471], Loss: 2.0765, Perplexity: 7.97676\n",
      "Epoch [2/3], Step [2440/6471], Loss: 1.9094, Perplexity: 6.74887\n",
      "Epoch [2/3], Step [2450/6471], Loss: 2.4306, Perplexity: 11.3661\n",
      "Epoch [2/3], Step [2460/6471], Loss: 2.3006, Perplexity: 9.98035\n",
      "Epoch [2/3], Step [2470/6471], Loss: 1.9871, Perplexity: 7.29445\n",
      "Epoch [2/3], Step [2480/6471], Loss: 2.4223, Perplexity: 11.2713\n",
      "Epoch [2/3], Step [2490/6471], Loss: 2.0872, Perplexity: 8.0622\n",
      "Epoch [2/3], Step [2500/6471], Loss: 1.9299, Perplexity: 6.88857\n",
      "Epoch [2/3], Step [2510/6471], Loss: 1.9883, Perplexity: 7.30333\n",
      "Epoch [2/3], Step [2520/6471], Loss: 2.1564, Perplexity: 8.63970\n",
      "Epoch [2/3], Step [2530/6471], Loss: 1.9759, Perplexity: 7.21290\n",
      "Epoch [2/3], Step [2540/6471], Loss: 2.1412, Perplexity: 8.50959\n",
      "Epoch [2/3], Step [2550/6471], Loss: 2.1353, Perplexity: 8.45941\n",
      "Epoch [2/3], Step [2560/6471], Loss: 2.6655, Perplexity: 14.3756\n",
      "Epoch [2/3], Step [2570/6471], Loss: 2.1754, Perplexity: 8.8054\n",
      "Epoch [2/3], Step [2580/6471], Loss: 2.1453, Perplexity: 8.54480\n",
      "Epoch [2/3], Step [2590/6471], Loss: 1.9712, Perplexity: 7.1796\n",
      "Epoch [2/3], Step [2600/6471], Loss: 2.1928, Perplexity: 8.9603\n",
      "Epoch [2/3], Step [2610/6471], Loss: 2.0779, Perplexity: 7.98807\n",
      "Epoch [2/3], Step [2620/6471], Loss: 2.0130, Perplexity: 7.48587\n",
      "Epoch [2/3], Step [2630/6471], Loss: 1.9721, Perplexity: 7.1860\n",
      "Epoch [2/3], Step [2640/6471], Loss: 1.9597, Perplexity: 7.09746\n",
      "Epoch [2/3], Step [2650/6471], Loss: 1.9684, Perplexity: 7.15892\n",
      "Epoch [2/3], Step [2660/6471], Loss: 2.4475, Perplexity: 11.5595\n",
      "Epoch [2/3], Step [2670/6471], Loss: 2.0587, Perplexity: 7.8354\n",
      "Epoch [2/3], Step [2680/6471], Loss: 2.2547, Perplexity: 9.5321\n",
      "Epoch [2/3], Step [2690/6471], Loss: 2.4644, Perplexity: 11.7564\n",
      "Epoch [2/3], Step [2700/6471], Loss: 2.1167, Perplexity: 8.30351\n",
      "Epoch [2/3], Step [2710/6471], Loss: 2.2640, Perplexity: 9.6216\n",
      "Epoch [2/3], Step [2720/6471], Loss: 1.9025, Perplexity: 6.70266\n",
      "Epoch [2/3], Step [2730/6471], Loss: 2.0115, Perplexity: 7.47475\n",
      "Epoch [2/3], Step [2740/6471], Loss: 2.1458, Perplexity: 8.5485\n",
      "Epoch [2/3], Step [2750/6471], Loss: 2.0708, Perplexity: 7.93095\n",
      "Epoch [2/3], Step [2760/6471], Loss: 2.1499, Perplexity: 8.58394\n",
      "Epoch [2/3], Step [2770/6471], Loss: 2.0109, Perplexity: 7.47038\n",
      "Epoch [2/3], Step [2780/6471], Loss: 2.0314, Perplexity: 7.6245\n",
      "Epoch [2/3], Step [2790/6471], Loss: 2.0543, Perplexity: 7.8017\n",
      "Epoch [2/3], Step [2800/6471], Loss: 1.7745, Perplexity: 5.89723\n",
      "Epoch [2/3], Step [2810/6471], Loss: 2.0666, Perplexity: 7.89787\n",
      "Epoch [2/3], Step [2820/6471], Loss: 2.1035, Perplexity: 8.1945\n",
      "Epoch [2/3], Step [2830/6471], Loss: 2.1367, Perplexity: 8.4717\n",
      "Epoch [2/3], Step [2840/6471], Loss: 2.1916, Perplexity: 8.94973\n",
      "Epoch [2/3], Step [2850/6471], Loss: 2.1388, Perplexity: 8.48966\n",
      "Epoch [2/3], Step [2860/6471], Loss: 2.0312, Perplexity: 7.6230\n",
      "Epoch [2/3], Step [2870/6471], Loss: 2.1054, Perplexity: 8.21053\n",
      "Epoch [2/3], Step [2880/6471], Loss: 2.0213, Perplexity: 7.54783\n",
      "Epoch [2/3], Step [2890/6471], Loss: 2.0872, Perplexity: 8.0622\n",
      "Epoch [2/3], Step [2900/6471], Loss: 2.0349, Perplexity: 7.6513\n",
      "Epoch [2/3], Step [2910/6471], Loss: 2.1744, Perplexity: 8.7966\n",
      "Epoch [2/3], Step [2920/6471], Loss: 2.1362, Perplexity: 8.46743\n",
      "Epoch [2/3], Step [2930/6471], Loss: 2.1993, Perplexity: 9.01896\n",
      "Epoch [2/3], Step [2940/6471], Loss: 2.3640, Perplexity: 10.6333\n",
      "Epoch [2/3], Step [2950/6471], Loss: 2.1271, Perplexity: 8.39052\n",
      "Epoch [2/3], Step [2960/6471], Loss: 2.0208, Perplexity: 7.54437\n",
      "Epoch [2/3], Step [2970/6471], Loss: 2.1969, Perplexity: 8.9971\n",
      "Epoch [2/3], Step [2980/6471], Loss: 2.4133, Perplexity: 11.1706\n",
      "Epoch [2/3], Step [2990/6471], Loss: 1.7746, Perplexity: 5.8978\n",
      "Epoch [2/3], Step [3000/6471], Loss: 2.0986, Perplexity: 8.1544\n",
      "Epoch [2/3], Step [3010/6471], Loss: 1.9527, Perplexity: 7.04788\n",
      "Epoch [2/3], Step [3020/6471], Loss: 2.2781, Perplexity: 9.75813\n",
      "Epoch [2/3], Step [3030/6471], Loss: 2.2399, Perplexity: 9.3920\n",
      "Epoch [2/3], Step [3040/6471], Loss: 2.0098, Perplexity: 7.46192\n",
      "Epoch [2/3], Step [3050/6471], Loss: 2.1001, Perplexity: 8.16701\n",
      "Epoch [2/3], Step [3060/6471], Loss: 2.2063, Perplexity: 9.08181\n",
      "Epoch [2/3], Step [3070/6471], Loss: 1.9418, Perplexity: 6.97116\n",
      "Epoch [2/3], Step [3080/6471], Loss: 2.1919, Perplexity: 8.95204\n",
      "Epoch [2/3], Step [3090/6471], Loss: 2.1604, Perplexity: 8.6745\n",
      "Epoch [2/3], Step [3100/6471], Loss: 1.9347, Perplexity: 6.92176\n",
      "Epoch [2/3], Step [3110/6471], Loss: 1.9886, Perplexity: 7.3055\n",
      "Epoch [2/3], Step [3120/6471], Loss: 2.0775, Perplexity: 7.98464\n",
      "Epoch [2/3], Step [3130/6471], Loss: 2.1521, Perplexity: 8.60294\n",
      "Epoch [2/3], Step [3140/6471], Loss: 1.9476, Perplexity: 7.01164\n",
      "Epoch [2/3], Step [3150/6471], Loss: 2.0222, Perplexity: 7.5553\n",
      "Epoch [2/3], Step [3160/6471], Loss: 1.9939, Perplexity: 7.34449\n",
      "Epoch [2/3], Step [3170/6471], Loss: 2.4519, Perplexity: 11.6108\n",
      "Epoch [2/3], Step [3180/6471], Loss: 1.9590, Perplexity: 7.09199\n",
      "Epoch [2/3], Step [3190/6471], Loss: 1.9901, Perplexity: 7.31644\n",
      "Epoch [2/3], Step [3200/6471], Loss: 2.0830, Perplexity: 8.02885\n",
      "Epoch [2/3], Step [3210/6471], Loss: 2.0098, Perplexity: 7.46183\n",
      "Epoch [2/3], Step [3220/6471], Loss: 2.0754, Perplexity: 7.96743\n",
      "Epoch [2/3], Step [3230/6471], Loss: 2.2038, Perplexity: 9.0593\n",
      "Epoch [2/3], Step [3240/6471], Loss: 1.9897, Perplexity: 7.31326\n",
      "Epoch [2/3], Step [3250/6471], Loss: 2.3474, Perplexity: 10.4579\n",
      "Epoch [2/3], Step [3260/6471], Loss: 2.0000, Perplexity: 7.38948\n",
      "Epoch [2/3], Step [3270/6471], Loss: 2.3499, Perplexity: 10.4848\n",
      "Epoch [2/3], Step [3280/6471], Loss: 1.9648, Perplexity: 7.13331\n",
      "Epoch [2/3], Step [3290/6471], Loss: 2.1587, Perplexity: 8.65973\n",
      "Epoch [2/3], Step [3300/6471], Loss: 2.3753, Perplexity: 10.7538\n",
      "Epoch [2/3], Step [3310/6471], Loss: 2.0641, Perplexity: 7.87811\n",
      "Epoch [2/3], Step [3320/6471], Loss: 2.2126, Perplexity: 9.13927\n",
      "Epoch [2/3], Step [3330/6471], Loss: 1.9904, Perplexity: 7.31820\n",
      "Epoch [2/3], Step [3340/6471], Loss: 2.1247, Perplexity: 8.3705\n",
      "Epoch [2/3], Step [3350/6471], Loss: 2.0960, Perplexity: 8.1338\n",
      "Epoch [2/3], Step [3360/6471], Loss: 2.1514, Perplexity: 8.59685\n",
      "Epoch [2/3], Step [3370/6471], Loss: 1.9556, Perplexity: 7.06825\n",
      "Epoch [2/3], Step [3380/6471], Loss: 2.0193, Perplexity: 7.53310\n",
      "Epoch [2/3], Step [3390/6471], Loss: 1.9002, Perplexity: 6.68712\n",
      "Epoch [2/3], Step [3400/6471], Loss: 1.9486, Perplexity: 7.01914\n",
      "Epoch [2/3], Step [3410/6471], Loss: 2.0161, Perplexity: 7.50895\n",
      "Epoch [2/3], Step [3420/6471], Loss: 1.9783, Perplexity: 7.2307\n",
      "Epoch [2/3], Step [3430/6471], Loss: 1.9412, Perplexity: 6.9670\n",
      "Epoch [2/3], Step [3440/6471], Loss: 2.1564, Perplexity: 8.64031\n",
      "Epoch [2/3], Step [3450/6471], Loss: 2.2598, Perplexity: 9.58147\n",
      "Epoch [2/3], Step [3460/6471], Loss: 2.0929, Perplexity: 8.10811\n",
      "Epoch [2/3], Step [3470/6471], Loss: 1.9938, Perplexity: 7.34351\n",
      "Epoch [2/3], Step [3480/6471], Loss: 2.0784, Perplexity: 7.99180\n",
      "Epoch [2/3], Step [3490/6471], Loss: 1.9231, Perplexity: 6.84196\n",
      "Epoch [2/3], Step [3500/6471], Loss: 1.9461, Perplexity: 7.0012\n",
      "Epoch [2/3], Step [3510/6471], Loss: 2.1081, Perplexity: 8.2327\n",
      "Epoch [2/3], Step [3520/6471], Loss: 1.9410, Perplexity: 6.9657\n",
      "Epoch [2/3], Step [3530/6471], Loss: 2.1013, Perplexity: 8.17645\n",
      "Epoch [2/3], Step [3540/6471], Loss: 1.8688, Perplexity: 6.48050\n",
      "Epoch [2/3], Step [3550/6471], Loss: 1.9828, Perplexity: 7.26331\n",
      "Epoch [2/3], Step [3560/6471], Loss: 2.0266, Perplexity: 7.58799\n",
      "Epoch [2/3], Step [3570/6471], Loss: 2.1105, Perplexity: 8.25246\n",
      "Epoch [2/3], Step [3580/6471], Loss: 1.9482, Perplexity: 7.01599\n",
      "Epoch [2/3], Step [3590/6471], Loss: 2.3047, Perplexity: 10.0214\n",
      "Epoch [2/3], Step [3600/6471], Loss: 2.0763, Perplexity: 7.9747\n",
      "Epoch [2/3], Step [3610/6471], Loss: 2.0442, Perplexity: 7.72334\n",
      "Epoch [2/3], Step [3620/6471], Loss: 1.8176, Perplexity: 6.15721\n",
      "Epoch [2/3], Step [3630/6471], Loss: 2.1156, Perplexity: 8.29420\n",
      "Epoch [2/3], Step [3640/6471], Loss: 2.1334, Perplexity: 8.44330\n",
      "Epoch [2/3], Step [3650/6471], Loss: 2.0898, Perplexity: 8.08326\n",
      "Epoch [2/3], Step [3660/6471], Loss: 2.0814, Perplexity: 8.0157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [3670/6471], Loss: 2.1286, Perplexity: 8.40349\n",
      "Epoch [2/3], Step [3680/6471], Loss: 2.1623, Perplexity: 8.6914\n",
      "Epoch [2/3], Step [3690/6471], Loss: 1.9643, Perplexity: 7.12960\n",
      "Epoch [2/3], Step [3700/6471], Loss: 2.2409, Perplexity: 9.40226\n",
      "Epoch [2/3], Step [3710/6471], Loss: 1.9936, Perplexity: 7.34206\n",
      "Epoch [2/3], Step [3720/6471], Loss: 2.5099, Perplexity: 12.3032\n",
      "Epoch [2/3], Step [3730/6471], Loss: 2.2513, Perplexity: 9.49998\n",
      "Epoch [2/3], Step [3740/6471], Loss: 1.9960, Perplexity: 7.35935\n",
      "Epoch [2/3], Step [3750/6471], Loss: 2.1635, Perplexity: 8.7019\n",
      "Epoch [2/3], Step [3760/6471], Loss: 1.9814, Perplexity: 7.25305\n",
      "Epoch [2/3], Step [3770/6471], Loss: 2.5575, Perplexity: 12.9032\n",
      "Epoch [2/3], Step [3780/6471], Loss: 1.9252, Perplexity: 6.85651\n",
      "Epoch [2/3], Step [3790/6471], Loss: 1.9968, Perplexity: 7.36516\n",
      "Epoch [2/3], Step [3800/6471], Loss: 2.6904, Perplexity: 14.7370\n",
      "Epoch [2/3], Step [3810/6471], Loss: 2.0385, Perplexity: 7.67905\n",
      "Epoch [2/3], Step [3820/6471], Loss: 2.2235, Perplexity: 9.23946\n",
      "Epoch [2/3], Step [3830/6471], Loss: 2.0251, Perplexity: 7.57680\n",
      "Epoch [2/3], Step [3840/6471], Loss: 2.2290, Perplexity: 9.29052\n",
      "Epoch [2/3], Step [3850/6471], Loss: 1.9595, Perplexity: 7.09555\n",
      "Epoch [2/3], Step [3860/6471], Loss: 2.0842, Perplexity: 8.03848\n",
      "Epoch [2/3], Step [3870/6471], Loss: 2.1959, Perplexity: 8.98807\n",
      "Epoch [2/3], Step [3880/6471], Loss: 2.0528, Perplexity: 7.78993\n",
      "Epoch [2/3], Step [3890/6471], Loss: 2.1936, Perplexity: 8.9671\n",
      "Epoch [2/3], Step [3900/6471], Loss: 1.8921, Perplexity: 6.63306\n",
      "Epoch [2/3], Step [3910/6471], Loss: 2.4459, Perplexity: 11.5413\n",
      "Epoch [2/3], Step [3920/6471], Loss: 2.0522, Perplexity: 7.78499\n",
      "Epoch [2/3], Step [3930/6471], Loss: 2.0901, Perplexity: 8.08576\n",
      "Epoch [2/3], Step [3940/6471], Loss: 1.9914, Perplexity: 7.32552\n",
      "Epoch [2/3], Step [3950/6471], Loss: 2.0355, Perplexity: 7.65599\n",
      "Epoch [2/3], Step [3960/6471], Loss: 2.4159, Perplexity: 11.1995\n",
      "Epoch [2/3], Step [3970/6471], Loss: 2.0166, Perplexity: 7.51268\n",
      "Epoch [2/3], Step [3980/6471], Loss: 2.3822, Perplexity: 10.8292\n",
      "Epoch [2/3], Step [3990/6471], Loss: 2.0664, Perplexity: 7.89603\n",
      "Epoch [2/3], Step [4000/6471], Loss: 2.1556, Perplexity: 8.63355\n",
      "Epoch [2/3], Step [4010/6471], Loss: 2.0678, Perplexity: 7.9073\n",
      "Epoch [2/3], Step [4020/6471], Loss: 1.9019, Perplexity: 6.6987\n",
      "Epoch [2/3], Step [4030/6471], Loss: 1.9462, Perplexity: 7.00247\n",
      "Epoch [2/3], Step [4040/6471], Loss: 1.9161, Perplexity: 6.79424\n",
      "Epoch [2/3], Step [4050/6471], Loss: 2.0323, Perplexity: 7.63178\n",
      "Epoch [2/3], Step [4060/6471], Loss: 2.6903, Perplexity: 14.7355\n",
      "Epoch [2/3], Step [4070/6471], Loss: 2.0587, Perplexity: 7.83581\n",
      "Epoch [2/3], Step [4080/6471], Loss: 2.1444, Perplexity: 8.53654\n",
      "Epoch [2/3], Step [4090/6471], Loss: 1.9665, Perplexity: 7.1457\n",
      "Epoch [2/3], Step [4100/6471], Loss: 1.8420, Perplexity: 6.30944\n",
      "Epoch [2/3], Step [4110/6471], Loss: 2.1864, Perplexity: 8.90339\n",
      "Epoch [2/3], Step [4120/6471], Loss: 2.1402, Perplexity: 8.50124\n",
      "Epoch [2/3], Step [4130/6471], Loss: 2.0811, Perplexity: 8.01291\n",
      "Epoch [2/3], Step [4140/6471], Loss: 2.2245, Perplexity: 9.2492\n",
      "Epoch [2/3], Step [4150/6471], Loss: 1.9994, Perplexity: 7.3845\n",
      "Epoch [2/3], Step [4160/6471], Loss: 1.9587, Perplexity: 7.0904\n",
      "Epoch [2/3], Step [4170/6471], Loss: 2.0027, Perplexity: 7.4087\n",
      "Epoch [2/3], Step [4180/6471], Loss: 2.2126, Perplexity: 9.13934\n",
      "Epoch [2/3], Step [4190/6471], Loss: 2.0303, Perplexity: 7.61615\n",
      "Epoch [2/3], Step [4200/6471], Loss: 2.0439, Perplexity: 7.72073\n",
      "Epoch [2/3], Step [4210/6471], Loss: 1.9038, Perplexity: 6.71126\n",
      "Epoch [2/3], Step [4220/6471], Loss: 2.3008, Perplexity: 9.98256\n",
      "Epoch [2/3], Step [4230/6471], Loss: 1.9535, Perplexity: 7.05352\n",
      "Epoch [2/3], Step [4240/6471], Loss: 1.8801, Perplexity: 6.55417\n",
      "Epoch [2/3], Step [4250/6471], Loss: 1.9619, Perplexity: 7.11310\n",
      "Epoch [2/3], Step [4260/6471], Loss: 2.2878, Perplexity: 9.85317\n",
      "Epoch [2/3], Step [4270/6471], Loss: 2.2618, Perplexity: 9.60051\n",
      "Epoch [2/3], Step [4280/6471], Loss: 1.8339, Perplexity: 6.2580\n",
      "Epoch [2/3], Step [4290/6471], Loss: 2.1265, Perplexity: 8.3853\n",
      "Epoch [2/3], Step [4300/6471], Loss: 1.8873, Perplexity: 6.60169\n",
      "Epoch [2/3], Step [4310/6471], Loss: 1.9358, Perplexity: 6.92933\n",
      "Epoch [2/3], Step [4320/6471], Loss: 2.1908, Perplexity: 8.94262\n",
      "Epoch [2/3], Step [4330/6471], Loss: 1.9433, Perplexity: 6.98163\n",
      "Epoch [2/3], Step [4340/6471], Loss: 1.8987, Perplexity: 6.67745\n",
      "Epoch [2/3], Step [4350/6471], Loss: 2.4197, Perplexity: 11.2425\n",
      "Epoch [2/3], Step [4360/6471], Loss: 1.9896, Perplexity: 7.31234\n",
      "Epoch [2/3], Step [4370/6471], Loss: 1.9629, Perplexity: 7.12016\n",
      "Epoch [2/3], Step [4380/6471], Loss: 2.0657, Perplexity: 7.89112\n",
      "Epoch [2/3], Step [4390/6471], Loss: 2.1436, Perplexity: 8.52998\n",
      "Epoch [2/3], Step [4400/6471], Loss: 1.9749, Perplexity: 7.2057\n",
      "Epoch [2/3], Step [4410/6471], Loss: 2.0661, Perplexity: 7.89439\n",
      "Epoch [2/3], Step [4420/6471], Loss: 1.9876, Perplexity: 7.2983\n",
      "Epoch [2/3], Step [4430/6471], Loss: 2.0687, Perplexity: 7.91449\n",
      "Epoch [2/3], Step [4440/6471], Loss: 2.0148, Perplexity: 7.49927\n",
      "Epoch [2/3], Step [4450/6471], Loss: 1.9557, Perplexity: 7.06894\n",
      "Epoch [2/3], Step [4460/6471], Loss: 2.3397, Perplexity: 10.3780\n",
      "Epoch [2/3], Step [4470/6471], Loss: 2.1003, Perplexity: 8.1687\n",
      "Epoch [2/3], Step [4480/6471], Loss: 1.9972, Perplexity: 7.3684\n",
      "Epoch [2/3], Step [4490/6471], Loss: 2.7613, Perplexity: 15.8209\n",
      "Epoch [2/3], Step [4500/6471], Loss: 2.0192, Perplexity: 7.5324\n",
      "Epoch [2/3], Step [4510/6471], Loss: 1.9473, Perplexity: 7.0096\n",
      "Epoch [2/3], Step [4520/6471], Loss: 2.1588, Perplexity: 8.6605\n",
      "Epoch [2/3], Step [4530/6471], Loss: 2.0050, Perplexity: 7.42572\n",
      "Epoch [2/3], Step [4540/6471], Loss: 2.0273, Perplexity: 7.5933\n",
      "Epoch [2/3], Step [4550/6471], Loss: 2.1346, Perplexity: 8.45340\n",
      "Epoch [2/3], Step [4560/6471], Loss: 2.1280, Perplexity: 8.39789\n",
      "Epoch [2/3], Step [4570/6471], Loss: 2.1625, Perplexity: 8.69288\n",
      "Epoch [2/3], Step [4580/6471], Loss: 2.2212, Perplexity: 9.21816\n",
      "Epoch [2/3], Step [4590/6471], Loss: 2.0518, Perplexity: 7.78181\n",
      "Epoch [2/3], Step [4600/6471], Loss: 2.0366, Perplexity: 7.6645\n",
      "Epoch [2/3], Step [4610/6471], Loss: 2.0083, Perplexity: 7.45065\n",
      "Epoch [2/3], Step [4620/6471], Loss: 1.9782, Perplexity: 7.23010\n",
      "Epoch [2/3], Step [4630/6471], Loss: 2.1241, Perplexity: 8.3651\n",
      "Epoch [2/3], Step [4640/6471], Loss: 2.0239, Perplexity: 7.56753\n",
      "Epoch [2/3], Step [4650/6471], Loss: 2.5050, Perplexity: 12.2431\n",
      "Epoch [2/3], Step [4660/6471], Loss: 2.1914, Perplexity: 8.9473\n",
      "Epoch [2/3], Step [4670/6471], Loss: 1.9690, Perplexity: 7.16380\n",
      "Epoch [2/3], Step [4680/6471], Loss: 2.0946, Perplexity: 8.1225\n",
      "Epoch [2/3], Step [4690/6471], Loss: 2.1085, Perplexity: 8.23594\n",
      "Epoch [2/3], Step [4700/6471], Loss: 2.1027, Perplexity: 8.1886\n",
      "Epoch [2/3], Step [4710/6471], Loss: 2.1369, Perplexity: 8.47317\n",
      "Epoch [2/3], Step [4720/6471], Loss: 1.8611, Perplexity: 6.43056\n",
      "Epoch [2/3], Step [4730/6471], Loss: 2.0499, Perplexity: 7.76732\n",
      "Epoch [2/3], Step [4740/6471], Loss: 2.0697, Perplexity: 7.92250\n",
      "Epoch [2/3], Step [4750/6471], Loss: 2.0153, Perplexity: 7.5029\n",
      "Epoch [2/3], Step [4760/6471], Loss: 1.9271, Perplexity: 6.86922\n",
      "Epoch [2/3], Step [4770/6471], Loss: 2.2676, Perplexity: 9.6566\n",
      "Epoch [2/3], Step [4780/6471], Loss: 2.0907, Perplexity: 8.0905\n",
      "Epoch [2/3], Step [4790/6471], Loss: 1.8756, Perplexity: 6.52506\n",
      "Epoch [2/3], Step [4800/6471], Loss: 2.1372, Perplexity: 8.47584\n",
      "Epoch [2/3], Step [4810/6471], Loss: 1.7738, Perplexity: 5.89313\n",
      "Epoch [2/3], Step [4820/6471], Loss: 2.6128, Perplexity: 13.6379\n",
      "Epoch [2/3], Step [4830/6471], Loss: 1.9723, Perplexity: 7.18749\n",
      "Epoch [2/3], Step [4840/6471], Loss: 1.8976, Perplexity: 6.66968\n",
      "Epoch [2/3], Step [4850/6471], Loss: 2.1185, Perplexity: 8.31873\n",
      "Epoch [2/3], Step [4860/6471], Loss: 2.0443, Perplexity: 7.72359\n",
      "Epoch [2/3], Step [4870/6471], Loss: 1.9407, Perplexity: 6.96372\n",
      "Epoch [2/3], Step [4880/6471], Loss: 2.0258, Perplexity: 7.58207\n",
      "Epoch [2/3], Step [4890/6471], Loss: 1.9945, Perplexity: 7.34820\n",
      "Epoch [2/3], Step [4900/6471], Loss: 1.8679, Perplexity: 6.47491\n",
      "Epoch [2/3], Step [4910/6471], Loss: 2.0803, Perplexity: 8.00662\n",
      "Epoch [2/3], Step [4920/6471], Loss: 1.8963, Perplexity: 6.6610\n",
      "Epoch [2/3], Step [4930/6471], Loss: 2.0769, Perplexity: 7.97970\n",
      "Epoch [2/3], Step [4940/6471], Loss: 2.1696, Perplexity: 8.7545\n",
      "Epoch [2/3], Step [4950/6471], Loss: 2.0685, Perplexity: 7.9131\n",
      "Epoch [2/3], Step [4960/6471], Loss: 2.3983, Perplexity: 11.0043\n",
      "Epoch [2/3], Step [4970/6471], Loss: 2.8538, Perplexity: 17.3539\n",
      "Epoch [2/3], Step [4980/6471], Loss: 2.0057, Perplexity: 7.43094\n",
      "Epoch [2/3], Step [4990/6471], Loss: 2.1209, Perplexity: 8.33847\n",
      "Epoch [2/3], Step [5000/6471], Loss: 2.2582, Perplexity: 9.5660\n",
      "Epoch [2/3], Step [5010/6471], Loss: 2.2509, Perplexity: 9.4963\n",
      "Epoch [2/3], Step [5020/6471], Loss: 2.1028, Perplexity: 8.18949\n",
      "Epoch [2/3], Step [5030/6471], Loss: 2.1469, Perplexity: 8.55829\n",
      "Epoch [2/3], Step [5040/6471], Loss: 2.0758, Perplexity: 7.97137\n",
      "Epoch [2/3], Step [5050/6471], Loss: 1.9361, Perplexity: 6.9320\n",
      "Epoch [2/3], Step [5060/6471], Loss: 1.9362, Perplexity: 6.93215\n",
      "Epoch [2/3], Step [5070/6471], Loss: 2.0529, Perplexity: 7.79080\n",
      "Epoch [2/3], Step [5080/6471], Loss: 2.0475, Perplexity: 7.74827\n",
      "Epoch [2/3], Step [5090/6471], Loss: 2.1148, Perplexity: 8.2878\n",
      "Epoch [2/3], Step [5100/6471], Loss: 1.8753, Perplexity: 6.52303\n",
      "Epoch [2/3], Step [5110/6471], Loss: 1.9119, Perplexity: 6.7659\n",
      "Epoch [2/3], Step [5120/6471], Loss: 2.0940, Perplexity: 8.11753\n",
      "Epoch [2/3], Step [5130/6471], Loss: 2.6771, Perplexity: 14.5429\n",
      "Epoch [2/3], Step [5140/6471], Loss: 2.0409, Perplexity: 7.69747\n",
      "Epoch [2/3], Step [5150/6471], Loss: 2.3147, Perplexity: 10.1223\n",
      "Epoch [2/3], Step [5160/6471], Loss: 2.1017, Perplexity: 8.1800\n",
      "Epoch [2/3], Step [5170/6471], Loss: 2.0740, Perplexity: 7.95668\n",
      "Epoch [2/3], Step [5180/6471], Loss: 1.8015, Perplexity: 6.05897\n",
      "Epoch [2/3], Step [5190/6471], Loss: 2.0582, Perplexity: 7.83194\n",
      "Epoch [2/3], Step [5200/6471], Loss: 2.1053, Perplexity: 8.20927\n",
      "Epoch [2/3], Step [5210/6471], Loss: 2.0841, Perplexity: 8.03760\n",
      "Epoch [2/3], Step [5220/6471], Loss: 2.3144, Perplexity: 10.1190\n",
      "Epoch [2/3], Step [5230/6471], Loss: 2.0954, Perplexity: 8.12879\n",
      "Epoch [2/3], Step [5240/6471], Loss: 2.8720, Perplexity: 17.6727\n",
      "Epoch [2/3], Step [5250/6471], Loss: 2.5214, Perplexity: 12.4456\n",
      "Epoch [2/3], Step [5260/6471], Loss: 2.0215, Perplexity: 7.5494\n",
      "Epoch [2/3], Step [5270/6471], Loss: 1.9755, Perplexity: 7.21053\n",
      "Epoch [2/3], Step [5280/6471], Loss: 2.3702, Perplexity: 10.6992\n",
      "Epoch [2/3], Step [5290/6471], Loss: 1.9470, Perplexity: 7.00787\n",
      "Epoch [2/3], Step [5300/6471], Loss: 2.4008, Perplexity: 11.0323\n",
      "Epoch [2/3], Step [5310/6471], Loss: 2.0933, Perplexity: 8.11193\n",
      "Epoch [2/3], Step [5320/6471], Loss: 2.3877, Perplexity: 10.8879\n",
      "Epoch [2/3], Step [5330/6471], Loss: 2.2447, Perplexity: 9.4377\n",
      "Epoch [2/3], Step [5340/6471], Loss: 1.9727, Perplexity: 7.19009\n",
      "Epoch [2/3], Step [5350/6471], Loss: 2.0639, Perplexity: 7.87691\n",
      "Epoch [2/3], Step [5360/6471], Loss: 2.2002, Perplexity: 9.0272\n",
      "Epoch [2/3], Step [5370/6471], Loss: 2.0699, Perplexity: 7.92413\n",
      "Epoch [2/3], Step [5380/6471], Loss: 2.3633, Perplexity: 10.6260\n",
      "Epoch [2/3], Step [5390/6471], Loss: 2.0492, Perplexity: 7.76205\n",
      "Epoch [2/3], Step [5400/6471], Loss: 2.0579, Perplexity: 7.8295\n",
      "Epoch [2/3], Step [5410/6471], Loss: 2.2050, Perplexity: 9.07035\n",
      "Epoch [2/3], Step [5420/6471], Loss: 2.1729, Perplexity: 8.7838\n",
      "Epoch [2/3], Step [5430/6471], Loss: 2.1167, Perplexity: 8.30373\n",
      "Epoch [2/3], Step [5440/6471], Loss: 2.3272, Perplexity: 10.2494\n",
      "Epoch [2/3], Step [5450/6471], Loss: 2.1747, Perplexity: 8.7997\n",
      "Epoch [2/3], Step [5460/6471], Loss: 1.9399, Perplexity: 6.95780\n",
      "Epoch [2/3], Step [5470/6471], Loss: 2.1792, Perplexity: 8.8392\n",
      "Epoch [2/3], Step [5480/6471], Loss: 2.1725, Perplexity: 8.7801\n",
      "Epoch [2/3], Step [5490/6471], Loss: 2.0147, Perplexity: 7.49855\n",
      "Epoch [2/3], Step [5500/6471], Loss: 1.9566, Perplexity: 7.0753\n",
      "Epoch [2/3], Step [5510/6471], Loss: 2.0875, Perplexity: 8.06469\n",
      "Epoch [2/3], Step [5520/6471], Loss: 1.9410, Perplexity: 6.9658\n",
      "Epoch [2/3], Step [5530/6471], Loss: 2.0130, Perplexity: 7.48560\n",
      "Epoch [2/3], Step [5540/6471], Loss: 2.1450, Perplexity: 8.54239\n",
      "Epoch [2/3], Step [5550/6471], Loss: 1.8999, Perplexity: 6.6850\n",
      "Epoch [2/3], Step [5560/6471], Loss: 2.1212, Perplexity: 8.34143\n",
      "Epoch [2/3], Step [5570/6471], Loss: 2.0076, Perplexity: 7.44516\n",
      "Epoch [2/3], Step [5580/6471], Loss: 2.7420, Perplexity: 15.5182\n",
      "Epoch [2/3], Step [5590/6471], Loss: 1.9801, Perplexity: 7.24378\n",
      "Epoch [2/3], Step [5600/6471], Loss: 1.9248, Perplexity: 6.8539\n",
      "Epoch [2/3], Step [5610/6471], Loss: 1.9049, Perplexity: 6.7189\n",
      "Epoch [2/3], Step [5620/6471], Loss: 2.0781, Perplexity: 7.98928\n",
      "Epoch [2/3], Step [5630/6471], Loss: 1.9460, Perplexity: 7.0006\n",
      "Epoch [2/3], Step [5640/6471], Loss: 1.9323, Perplexity: 6.9050\n",
      "Epoch [2/3], Step [5650/6471], Loss: 2.0384, Perplexity: 7.67857\n",
      "Epoch [2/3], Step [5660/6471], Loss: 2.0891, Perplexity: 8.07800\n",
      "Epoch [2/3], Step [5670/6471], Loss: 1.9497, Perplexity: 7.02689\n",
      "Epoch [2/3], Step [5680/6471], Loss: 2.1243, Perplexity: 8.36721\n",
      "Epoch [2/3], Step [5690/6471], Loss: 2.1394, Perplexity: 8.49408\n",
      "Epoch [2/3], Step [5700/6471], Loss: 2.0681, Perplexity: 7.9095\n",
      "Epoch [2/3], Step [5710/6471], Loss: 2.1169, Perplexity: 8.30552\n",
      "Epoch [2/3], Step [5720/6471], Loss: 1.7929, Perplexity: 6.00671\n",
      "Epoch [2/3], Step [5730/6471], Loss: 2.1459, Perplexity: 8.55001\n",
      "Epoch [2/3], Step [5740/6471], Loss: 1.8751, Perplexity: 6.5217\n",
      "Epoch [2/3], Step [5750/6471], Loss: 2.1110, Perplexity: 8.25645\n",
      "Epoch [2/3], Step [5760/6471], Loss: 1.9557, Perplexity: 7.06867\n",
      "Epoch [2/3], Step [5770/6471], Loss: 1.9896, Perplexity: 7.31268\n",
      "Epoch [2/3], Step [5780/6471], Loss: 2.2214, Perplexity: 9.2200\n",
      "Epoch [2/3], Step [5790/6471], Loss: 1.8376, Perplexity: 6.28174\n",
      "Epoch [2/3], Step [5800/6471], Loss: 2.0683, Perplexity: 7.91109\n",
      "Epoch [2/3], Step [5810/6471], Loss: 2.2924, Perplexity: 9.8990\n",
      "Epoch [2/3], Step [5820/6471], Loss: 1.8960, Perplexity: 6.65915\n",
      "Epoch [2/3], Step [5830/6471], Loss: 2.0621, Perplexity: 7.86263\n",
      "Epoch [2/3], Step [5840/6471], Loss: 1.9319, Perplexity: 6.90289\n",
      "Epoch [2/3], Step [5850/6471], Loss: 2.2585, Perplexity: 9.5683\n",
      "Epoch [2/3], Step [5860/6471], Loss: 2.0081, Perplexity: 7.44928\n",
      "Epoch [2/3], Step [5870/6471], Loss: 2.3382, Perplexity: 10.3624\n",
      "Epoch [2/3], Step [5880/6471], Loss: 2.0347, Perplexity: 7.6501\n",
      "Epoch [2/3], Step [5890/6471], Loss: 2.0396, Perplexity: 7.6876\n",
      "Epoch [2/3], Step [5900/6471], Loss: 2.3561, Perplexity: 10.5496\n",
      "Epoch [2/3], Step [5910/6471], Loss: 1.9238, Perplexity: 6.84711\n",
      "Epoch [2/3], Step [5920/6471], Loss: 1.9656, Perplexity: 7.13924\n",
      "Epoch [2/3], Step [5930/6471], Loss: 2.1430, Perplexity: 8.52486\n",
      "Epoch [2/3], Step [5940/6471], Loss: 2.1078, Perplexity: 8.2303\n",
      "Epoch [2/3], Step [5950/6471], Loss: 1.8034, Perplexity: 6.0704\n",
      "Epoch [2/3], Step [5960/6471], Loss: 2.1425, Perplexity: 8.52068\n",
      "Epoch [2/3], Step [5970/6471], Loss: 2.1185, Perplexity: 8.31861\n",
      "Epoch [2/3], Step [5980/6471], Loss: 1.8866, Perplexity: 6.59689\n",
      "Epoch [2/3], Step [5990/6471], Loss: 1.9946, Perplexity: 7.34940\n",
      "Epoch [2/3], Step [6000/6471], Loss: 1.9417, Perplexity: 6.97030\n",
      "Epoch [2/3], Step [6010/6471], Loss: 2.2009, Perplexity: 9.0327\n",
      "Epoch [2/3], Step [6020/6471], Loss: 1.8310, Perplexity: 6.2400\n",
      "Epoch [2/3], Step [6030/6471], Loss: 2.0477, Perplexity: 7.75015\n",
      "Epoch [2/3], Step [6040/6471], Loss: 2.0133, Perplexity: 7.48837\n",
      "Epoch [2/3], Step [6050/6471], Loss: 2.0338, Perplexity: 7.64345\n",
      "Epoch [2/3], Step [6060/6471], Loss: 2.0264, Perplexity: 7.5868\n",
      "Epoch [2/3], Step [6070/6471], Loss: 2.0938, Perplexity: 8.11564\n",
      "Epoch [2/3], Step [6080/6471], Loss: 2.0375, Perplexity: 7.67177\n",
      "Epoch [2/3], Step [6090/6471], Loss: 2.0530, Perplexity: 7.79119\n",
      "Epoch [2/3], Step [6100/6471], Loss: 2.0744, Perplexity: 7.95952\n",
      "Epoch [2/3], Step [6110/6471], Loss: 2.1913, Perplexity: 8.94693\n",
      "Epoch [2/3], Step [6120/6471], Loss: 1.9706, Perplexity: 7.17533\n",
      "Epoch [2/3], Step [6130/6471], Loss: 2.1171, Perplexity: 8.30721\n",
      "Epoch [2/3], Step [6140/6471], Loss: 2.1148, Perplexity: 8.2875\n",
      "Epoch [2/3], Step [6150/6471], Loss: 1.9771, Perplexity: 7.22155\n",
      "Epoch [2/3], Step [6160/6471], Loss: 1.9441, Perplexity: 6.9870\n",
      "Epoch [2/3], Step [6170/6471], Loss: 2.0602, Perplexity: 7.84741\n",
      "Epoch [2/3], Step [6180/6471], Loss: 2.0877, Perplexity: 8.06650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [6190/6471], Loss: 1.9999, Perplexity: 7.38810\n",
      "Epoch [2/3], Step [6200/6471], Loss: 1.9628, Perplexity: 7.11908\n",
      "Epoch [2/3], Step [6210/6471], Loss: 2.6422, Perplexity: 14.0447\n",
      "Epoch [2/3], Step [6220/6471], Loss: 2.2769, Perplexity: 9.74665\n",
      "Epoch [2/3], Step [6230/6471], Loss: 2.1137, Perplexity: 8.27919\n",
      "Epoch [2/3], Step [6240/6471], Loss: 1.9205, Perplexity: 6.8245\n",
      "Epoch [2/3], Step [6250/6471], Loss: 2.2510, Perplexity: 9.4970\n",
      "Epoch [2/3], Step [6260/6471], Loss: 1.9313, Perplexity: 6.8987\n",
      "Epoch [2/3], Step [6270/6471], Loss: 2.0717, Perplexity: 7.93831\n",
      "Epoch [2/3], Step [6280/6471], Loss: 3.0142, Perplexity: 20.3734\n",
      "Epoch [2/3], Step [6290/6471], Loss: 2.2255, Perplexity: 9.2586\n",
      "Epoch [2/3], Step [6300/6471], Loss: 2.8951, Perplexity: 18.0849\n",
      "Epoch [2/3], Step [6310/6471], Loss: 1.9692, Perplexity: 7.16528\n",
      "Epoch [2/3], Step [6320/6471], Loss: 1.8983, Perplexity: 6.67455\n",
      "Epoch [2/3], Step [6330/6471], Loss: 2.0690, Perplexity: 7.91734\n",
      "Epoch [2/3], Step [6340/6471], Loss: 1.8301, Perplexity: 6.2346\n",
      "Epoch [2/3], Step [6350/6471], Loss: 2.0490, Perplexity: 7.76001\n",
      "Epoch [2/3], Step [6360/6471], Loss: 2.0057, Perplexity: 7.43121\n",
      "Epoch [2/3], Step [6370/6471], Loss: 2.0951, Perplexity: 8.1261\n",
      "Epoch [2/3], Step [6380/6471], Loss: 2.0504, Perplexity: 7.7709\n",
      "Epoch [2/3], Step [6390/6471], Loss: 2.3325, Perplexity: 10.3036\n",
      "Epoch [2/3], Step [6400/6471], Loss: 2.1386, Perplexity: 8.48774\n",
      "Epoch [2/3], Step [6410/6471], Loss: 1.9380, Perplexity: 6.94480\n",
      "Epoch [2/3], Step [6420/6471], Loss: 2.1365, Perplexity: 8.47016\n",
      "Epoch [2/3], Step [6430/6471], Loss: 2.1523, Perplexity: 8.60469\n",
      "Epoch [2/3], Step [6440/6471], Loss: 1.9756, Perplexity: 7.21071\n",
      "Epoch [2/3], Step [6450/6471], Loss: 2.0770, Perplexity: 7.98058\n",
      "Epoch [2/3], Step [6460/6471], Loss: 2.1211, Perplexity: 8.34008\n",
      "Epoch [2/3], Step [6470/6471], Loss: 2.1278, Perplexity: 8.39620\n",
      "Epoch [3/3], Step [10/6471], Loss: 2.1895, Perplexity: 8.930488\n",
      "Epoch [3/3], Step [20/6471], Loss: 1.9491, Perplexity: 7.02261\n",
      "Epoch [3/3], Step [30/6471], Loss: 2.2931, Perplexity: 9.9057\n",
      "Epoch [3/3], Step [40/6471], Loss: 2.1168, Perplexity: 8.30494\n",
      "Epoch [3/3], Step [50/6471], Loss: 2.0625, Perplexity: 7.86600\n",
      "Epoch [3/3], Step [60/6471], Loss: 2.0382, Perplexity: 7.67683\n",
      "Epoch [3/3], Step [70/6471], Loss: 1.8020, Perplexity: 6.0615\n",
      "Epoch [3/3], Step [80/6471], Loss: 1.9016, Perplexity: 6.6963\n",
      "Epoch [3/3], Step [90/6471], Loss: 2.1546, Perplexity: 8.62418\n",
      "Epoch [3/3], Step [100/6471], Loss: 1.8827, Perplexity: 6.5710\n",
      "Epoch [3/3], Step [110/6471], Loss: 2.0282, Perplexity: 7.6003\n",
      "Epoch [3/3], Step [120/6471], Loss: 2.1300, Perplexity: 8.41494\n",
      "Epoch [3/3], Step [130/6471], Loss: 2.1647, Perplexity: 8.71240\n",
      "Epoch [3/3], Step [140/6471], Loss: 1.9701, Perplexity: 7.17126\n",
      "Epoch [3/3], Step [150/6471], Loss: 1.8644, Perplexity: 6.4518\n",
      "Epoch [3/3], Step [160/6471], Loss: 2.1492, Perplexity: 8.5776\n",
      "Epoch [3/3], Step [170/6471], Loss: 1.8162, Perplexity: 6.1487\n",
      "Epoch [3/3], Step [180/6471], Loss: 1.9610, Perplexity: 7.1067\n",
      "Epoch [3/3], Step [190/6471], Loss: 3.3031, Perplexity: 27.1969\n",
      "Epoch [3/3], Step [200/6471], Loss: 2.1670, Perplexity: 8.7319\n",
      "Epoch [3/3], Step [210/6471], Loss: 1.9359, Perplexity: 6.92993\n",
      "Epoch [3/3], Step [220/6471], Loss: 1.9106, Perplexity: 6.7572\n",
      "Epoch [3/3], Step [230/6471], Loss: 1.9787, Perplexity: 7.2336\n",
      "Epoch [3/3], Step [240/6471], Loss: 1.9425, Perplexity: 6.9763\n",
      "Epoch [3/3], Step [250/6471], Loss: 1.9750, Perplexity: 7.20697\n",
      "Epoch [3/3], Step [260/6471], Loss: 1.9571, Perplexity: 7.07905\n",
      "Epoch [3/3], Step [270/6471], Loss: 1.8366, Perplexity: 6.27542\n",
      "Epoch [3/3], Step [280/6471], Loss: 2.2964, Perplexity: 9.9384\n",
      "Epoch [3/3], Step [290/6471], Loss: 1.9458, Perplexity: 6.99959\n",
      "Epoch [3/3], Step [300/6471], Loss: 2.2397, Perplexity: 9.39047\n",
      "Epoch [3/3], Step [310/6471], Loss: 2.2790, Perplexity: 9.7674\n",
      "Epoch [3/3], Step [320/6471], Loss: 1.8542, Perplexity: 6.38671\n",
      "Epoch [3/3], Step [330/6471], Loss: 2.0303, Perplexity: 7.61656\n",
      "Epoch [3/3], Step [340/6471], Loss: 1.8463, Perplexity: 6.33602\n",
      "Epoch [3/3], Step [350/6471], Loss: 1.8620, Perplexity: 6.43660\n",
      "Epoch [3/3], Step [360/6471], Loss: 1.9288, Perplexity: 6.8813\n",
      "Epoch [3/3], Step [370/6471], Loss: 2.1162, Perplexity: 8.29974\n",
      "Epoch [3/3], Step [380/6471], Loss: 2.0408, Perplexity: 7.6970\n",
      "Epoch [3/3], Step [390/6471], Loss: 2.0441, Perplexity: 7.7226\n",
      "Epoch [3/3], Step [400/6471], Loss: 1.9840, Perplexity: 7.27209\n",
      "Epoch [3/3], Step [410/6471], Loss: 2.1406, Perplexity: 8.50473\n",
      "Epoch [3/3], Step [420/6471], Loss: 2.5391, Perplexity: 12.6684\n",
      "Epoch [3/3], Step [430/6471], Loss: 2.6238, Perplexity: 13.7880\n",
      "Epoch [3/3], Step [440/6471], Loss: 2.0166, Perplexity: 7.51255\n",
      "Epoch [3/3], Step [450/6471], Loss: 2.2535, Perplexity: 9.52102\n",
      "Epoch [3/3], Step [460/6471], Loss: 2.0639, Perplexity: 7.87681\n",
      "Epoch [3/3], Step [470/6471], Loss: 2.0475, Perplexity: 7.74870\n",
      "Epoch [3/3], Step [480/6471], Loss: 2.0207, Perplexity: 7.54361\n",
      "Epoch [3/3], Step [490/6471], Loss: 2.3624, Perplexity: 10.6163\n",
      "Epoch [3/3], Step [500/6471], Loss: 2.6064, Perplexity: 13.5496\n",
      "Epoch [3/3], Step [510/6471], Loss: 1.9259, Perplexity: 6.86163\n",
      "Epoch [3/3], Step [520/6471], Loss: 2.1601, Perplexity: 8.6720\n",
      "Epoch [3/3], Step [530/6471], Loss: 1.9597, Perplexity: 7.0972\n",
      "Epoch [3/3], Step [540/6471], Loss: 2.1704, Perplexity: 8.76149\n",
      "Epoch [3/3], Step [550/6471], Loss: 1.7851, Perplexity: 5.96009\n",
      "Epoch [3/3], Step [560/6471], Loss: 2.0395, Perplexity: 7.6870\n",
      "Epoch [3/3], Step [570/6471], Loss: 1.9205, Perplexity: 6.82433\n",
      "Epoch [3/3], Step [580/6471], Loss: 2.1266, Perplexity: 8.3862\n",
      "Epoch [3/3], Step [590/6471], Loss: 2.2242, Perplexity: 9.24623\n",
      "Epoch [3/3], Step [600/6471], Loss: 2.0269, Perplexity: 7.59078\n",
      "Epoch [3/3], Step [610/6471], Loss: 1.8173, Perplexity: 6.1552\n",
      "Epoch [3/3], Step [620/6471], Loss: 2.0490, Perplexity: 7.76028\n",
      "Epoch [3/3], Step [630/6471], Loss: 1.9139, Perplexity: 6.77935\n",
      "Epoch [3/3], Step [640/6471], Loss: 1.9382, Perplexity: 6.94625\n",
      "Epoch [3/3], Step [650/6471], Loss: 1.7892, Perplexity: 5.98449\n",
      "Epoch [3/3], Step [660/6471], Loss: 1.8020, Perplexity: 6.0615\n",
      "Epoch [3/3], Step [670/6471], Loss: 1.9192, Perplexity: 6.8155\n",
      "Epoch [3/3], Step [680/6471], Loss: 1.9543, Perplexity: 7.0587\n",
      "Epoch [3/3], Step [690/6471], Loss: 2.2020, Perplexity: 9.04358\n",
      "Epoch [3/3], Step [700/6471], Loss: 2.0418, Perplexity: 7.70475\n",
      "Epoch [3/3], Step [710/6471], Loss: 1.9160, Perplexity: 6.7938\n",
      "Epoch [3/3], Step [720/6471], Loss: 2.2205, Perplexity: 9.21163\n",
      "Epoch [3/3], Step [730/6471], Loss: 2.1083, Perplexity: 8.2345\n",
      "Epoch [3/3], Step [740/6471], Loss: 1.9074, Perplexity: 6.73533\n",
      "Epoch [3/3], Step [750/6471], Loss: 1.8605, Perplexity: 6.4271\n",
      "Epoch [3/3], Step [760/6471], Loss: 2.3911, Perplexity: 10.9252\n",
      "Epoch [3/3], Step [770/6471], Loss: 2.0445, Perplexity: 7.7255\n",
      "Epoch [3/3], Step [780/6471], Loss: 1.8403, Perplexity: 6.2982\n",
      "Epoch [3/3], Step [790/6471], Loss: 2.0942, Perplexity: 8.1188\n",
      "Epoch [3/3], Step [800/6471], Loss: 1.9035, Perplexity: 6.70941\n",
      "Epoch [3/3], Step [810/6471], Loss: 2.2086, Perplexity: 9.10342\n",
      "Epoch [3/3], Step [820/6471], Loss: 2.0657, Perplexity: 7.8908\n",
      "Epoch [3/3], Step [830/6471], Loss: 1.9834, Perplexity: 7.2672\n",
      "Epoch [3/3], Step [840/6471], Loss: 1.9661, Perplexity: 7.14278\n",
      "Epoch [3/3], Step [850/6471], Loss: 1.8785, Perplexity: 6.5437\n",
      "Epoch [3/3], Step [860/6471], Loss: 1.9548, Perplexity: 7.0623\n",
      "Epoch [3/3], Step [870/6471], Loss: 2.2590, Perplexity: 9.57339\n",
      "Epoch [3/3], Step [880/6471], Loss: 2.0071, Perplexity: 7.4420\n",
      "Epoch [3/3], Step [890/6471], Loss: 2.0135, Perplexity: 7.48949\n",
      "Epoch [3/3], Step [900/6471], Loss: 2.2259, Perplexity: 9.2615\n",
      "Epoch [3/3], Step [910/6471], Loss: 2.0333, Perplexity: 7.63919\n",
      "Epoch [3/3], Step [920/6471], Loss: 2.0967, Perplexity: 8.1391\n",
      "Epoch [3/3], Step [930/6471], Loss: 2.0014, Perplexity: 7.3994\n",
      "Epoch [3/3], Step [940/6471], Loss: 1.9858, Perplexity: 7.28528\n",
      "Epoch [3/3], Step [950/6471], Loss: 2.4618, Perplexity: 11.7254\n",
      "Epoch [3/3], Step [960/6471], Loss: 1.8078, Perplexity: 6.09687\n",
      "Epoch [3/3], Step [970/6471], Loss: 2.0648, Perplexity: 7.88400\n",
      "Epoch [3/3], Step [980/6471], Loss: 2.3670, Perplexity: 10.6652\n",
      "Epoch [3/3], Step [990/6471], Loss: 2.0162, Perplexity: 7.5098\n",
      "Epoch [3/3], Step [1000/6471], Loss: 1.9072, Perplexity: 6.7345\n",
      "Epoch [3/3], Step [1010/6471], Loss: 2.0635, Perplexity: 7.87378\n",
      "Epoch [3/3], Step [1020/6471], Loss: 1.9435, Perplexity: 6.98305\n",
      "Epoch [3/3], Step [1030/6471], Loss: 1.9847, Perplexity: 7.27711\n",
      "Epoch [3/3], Step [1040/6471], Loss: 1.9579, Perplexity: 7.08433\n",
      "Epoch [3/3], Step [1050/6471], Loss: 1.9865, Perplexity: 7.28974\n",
      "Epoch [3/3], Step [1060/6471], Loss: 1.8922, Perplexity: 6.63368\n",
      "Epoch [3/3], Step [1070/6471], Loss: 2.0212, Perplexity: 7.54759\n",
      "Epoch [3/3], Step [1080/6471], Loss: 1.9009, Perplexity: 6.69178\n",
      "Epoch [3/3], Step [1090/6471], Loss: 2.1767, Perplexity: 8.81747\n",
      "Epoch [3/3], Step [1100/6471], Loss: 2.0474, Perplexity: 7.74800\n",
      "Epoch [3/3], Step [1110/6471], Loss: 2.1785, Perplexity: 8.83309\n",
      "Epoch [3/3], Step [1120/6471], Loss: 1.9067, Perplexity: 6.73096\n",
      "Epoch [3/3], Step [1130/6471], Loss: 1.8687, Perplexity: 6.47971\n",
      "Epoch [3/3], Step [1140/6471], Loss: 2.1101, Perplexity: 8.2490\n",
      "Epoch [3/3], Step [1150/6471], Loss: 1.8501, Perplexity: 6.36062\n",
      "Epoch [3/3], Step [1160/6471], Loss: 2.5743, Perplexity: 13.1219\n",
      "Epoch [3/3], Step [1170/6471], Loss: 2.0506, Perplexity: 7.77298\n",
      "Epoch [3/3], Step [1180/6471], Loss: 1.8106, Perplexity: 6.11409\n",
      "Epoch [3/3], Step [1190/6471], Loss: 2.1529, Perplexity: 8.6101\n",
      "Epoch [3/3], Step [1200/6471], Loss: 1.9476, Perplexity: 7.01220\n",
      "Epoch [3/3], Step [1210/6471], Loss: 2.1172, Perplexity: 8.30781\n",
      "Epoch [3/3], Step [1220/6471], Loss: 2.2761, Perplexity: 9.73874\n",
      "Epoch [3/3], Step [1230/6471], Loss: 1.8373, Perplexity: 6.27990\n",
      "Epoch [3/3], Step [1240/6471], Loss: 1.9799, Perplexity: 7.24186\n",
      "Epoch [3/3], Step [1250/6471], Loss: 1.9129, Perplexity: 6.77302\n",
      "Epoch [3/3], Step [1260/6471], Loss: 2.0385, Perplexity: 7.67913\n",
      "Epoch [3/3], Step [1270/6471], Loss: 1.9880, Perplexity: 7.30101\n",
      "Epoch [3/3], Step [1280/6471], Loss: 2.0013, Perplexity: 7.39883\n",
      "Epoch [3/3], Step [1290/6471], Loss: 1.8340, Perplexity: 6.2587\n",
      "Epoch [3/3], Step [1300/6471], Loss: 2.0815, Perplexity: 8.0161\n",
      "Epoch [3/3], Step [1310/6471], Loss: 2.4065, Perplexity: 11.0956\n",
      "Epoch [3/3], Step [1320/6471], Loss: 2.6765, Perplexity: 14.5342\n",
      "Epoch [3/3], Step [1330/6471], Loss: 1.9437, Perplexity: 6.98489\n",
      "Epoch [3/3], Step [1340/6471], Loss: 2.3175, Perplexity: 10.1506\n",
      "Epoch [3/3], Step [1350/6471], Loss: 2.1597, Perplexity: 8.66898\n",
      "Epoch [3/3], Step [1360/6471], Loss: 2.1026, Perplexity: 8.18780\n",
      "Epoch [3/3], Step [1370/6471], Loss: 2.0434, Perplexity: 7.71727\n",
      "Epoch [3/3], Step [1380/6471], Loss: 1.9931, Perplexity: 7.33858\n",
      "Epoch [3/3], Step [1390/6471], Loss: 1.8712, Perplexity: 6.49631\n",
      "Epoch [3/3], Step [1400/6471], Loss: 2.1917, Perplexity: 8.95028\n",
      "Epoch [3/3], Step [1410/6471], Loss: 2.3184, Perplexity: 10.1592\n",
      "Epoch [3/3], Step [1420/6471], Loss: 1.9379, Perplexity: 6.9441\n",
      "Epoch [3/3], Step [1430/6471], Loss: 1.8716, Perplexity: 6.49887\n",
      "Epoch [3/3], Step [1440/6471], Loss: 2.0447, Perplexity: 7.7270\n",
      "Epoch [3/3], Step [1450/6471], Loss: 2.1760, Perplexity: 8.8107\n",
      "Epoch [3/3], Step [1460/6471], Loss: 2.9441, Perplexity: 18.9927\n",
      "Epoch [3/3], Step [1470/6471], Loss: 2.0088, Perplexity: 7.45423\n",
      "Epoch [3/3], Step [1480/6471], Loss: 1.9386, Perplexity: 6.9492\n",
      "Epoch [3/3], Step [1490/6471], Loss: 2.1426, Perplexity: 8.52140\n",
      "Epoch [3/3], Step [1500/6471], Loss: 1.9106, Perplexity: 6.75695\n",
      "Epoch [3/3], Step [1510/6471], Loss: 2.1104, Perplexity: 8.2518\n",
      "Epoch [3/3], Step [1520/6471], Loss: 2.1395, Perplexity: 8.4953\n",
      "Epoch [3/3], Step [1530/6471], Loss: 2.2329, Perplexity: 9.32700\n",
      "Epoch [3/3], Step [1540/6471], Loss: 2.0068, Perplexity: 7.43974\n",
      "Epoch [3/3], Step [1550/6471], Loss: 1.9504, Perplexity: 7.0315\n",
      "Epoch [3/3], Step [1560/6471], Loss: 2.0404, Perplexity: 7.69363\n",
      "Epoch [3/3], Step [1570/6471], Loss: 1.9960, Perplexity: 7.3592\n",
      "Epoch [3/3], Step [1580/6471], Loss: 2.2528, Perplexity: 9.51412\n",
      "Epoch [3/3], Step [1590/6471], Loss: 2.0639, Perplexity: 7.87635\n",
      "Epoch [3/3], Step [1600/6471], Loss: 2.0650, Perplexity: 7.88543\n",
      "Epoch [3/3], Step [1610/6471], Loss: 2.0583, Perplexity: 7.8330\n",
      "Epoch [3/3], Step [1620/6471], Loss: 2.0058, Perplexity: 7.43224\n",
      "Epoch [3/3], Step [1630/6471], Loss: 2.0752, Perplexity: 7.96635\n",
      "Epoch [3/3], Step [1640/6471], Loss: 2.0372, Perplexity: 7.66886\n",
      "Epoch [3/3], Step [1650/6471], Loss: 2.0565, Perplexity: 7.8185\n",
      "Epoch [3/3], Step [1660/6471], Loss: 2.3232, Perplexity: 10.2084\n",
      "Epoch [3/3], Step [1670/6471], Loss: 2.0088, Perplexity: 7.4545\n",
      "Epoch [3/3], Step [1680/6471], Loss: 2.1796, Perplexity: 8.8424\n",
      "Epoch [3/3], Step [1690/6471], Loss: 1.9905, Perplexity: 7.31905\n",
      "Epoch [3/3], Step [1700/6471], Loss: 1.8143, Perplexity: 6.1367\n",
      "Epoch [3/3], Step [1710/6471], Loss: 2.0566, Perplexity: 7.81936\n",
      "Epoch [3/3], Step [1720/6471], Loss: 2.0624, Perplexity: 7.86500\n",
      "Epoch [3/3], Step [1730/6471], Loss: 1.7673, Perplexity: 5.8553\n",
      "Epoch [3/3], Step [1740/6471], Loss: 1.8521, Perplexity: 6.37328\n",
      "Epoch [3/3], Step [1750/6471], Loss: 2.0974, Perplexity: 8.1449\n",
      "Epoch [3/3], Step [1760/6471], Loss: 2.0601, Perplexity: 7.8466\n",
      "Epoch [3/3], Step [1770/6471], Loss: 1.9536, Perplexity: 7.05414\n",
      "Epoch [3/3], Step [1780/6471], Loss: 1.8839, Perplexity: 6.5790\n",
      "Epoch [3/3], Step [1790/6471], Loss: 2.4149, Perplexity: 11.1892\n",
      "Epoch [3/3], Step [1800/6471], Loss: 2.2752, Perplexity: 9.7299\n",
      "Epoch [3/3], Step [1810/6471], Loss: 2.2192, Perplexity: 9.20041\n",
      "Epoch [3/3], Step [1820/6471], Loss: 1.8974, Perplexity: 6.66870\n",
      "Epoch [3/3], Step [1830/6471], Loss: 2.1314, Perplexity: 8.42674\n",
      "Epoch [3/3], Step [1840/6471], Loss: 2.2405, Perplexity: 9.39776\n",
      "Epoch [3/3], Step [1850/6471], Loss: 2.0100, Perplexity: 7.46327\n",
      "Epoch [3/3], Step [1860/6471], Loss: 2.1672, Perplexity: 8.73414\n",
      "Epoch [3/3], Step [1870/6471], Loss: 1.9729, Perplexity: 7.1914\n",
      "Epoch [3/3], Step [1880/6471], Loss: 2.0566, Perplexity: 7.81927\n",
      "Epoch [3/3], Step [1890/6471], Loss: 1.7687, Perplexity: 5.8635\n",
      "Epoch [3/3], Step [1900/6471], Loss: 1.9281, Perplexity: 6.8765\n",
      "Epoch [3/3], Step [1910/6471], Loss: 1.8691, Perplexity: 6.4822\n",
      "Epoch [3/3], Step [1920/6471], Loss: 2.1545, Perplexity: 8.62342\n",
      "Epoch [3/3], Step [1930/6471], Loss: 1.9754, Perplexity: 7.20924\n",
      "Epoch [3/3], Step [1940/6471], Loss: 2.1450, Perplexity: 8.54180\n",
      "Epoch [3/3], Step [1950/6471], Loss: 1.9650, Perplexity: 7.13500\n",
      "Epoch [3/3], Step [1960/6471], Loss: 1.9320, Perplexity: 6.9030\n",
      "Epoch [3/3], Step [1970/6471], Loss: 2.0076, Perplexity: 7.44530\n",
      "Epoch [3/3], Step [1980/6471], Loss: 1.9045, Perplexity: 6.7159\n",
      "Epoch [3/3], Step [1990/6471], Loss: 1.9591, Perplexity: 7.09293\n",
      "Epoch [3/3], Step [2000/6471], Loss: 1.8947, Perplexity: 6.6507\n",
      "Epoch [3/3], Step [2010/6471], Loss: 1.9956, Perplexity: 7.3565\n",
      "Epoch [3/3], Step [2020/6471], Loss: 1.9698, Perplexity: 7.1695\n",
      "Epoch [3/3], Step [2030/6471], Loss: 1.9784, Perplexity: 7.23126\n",
      "Epoch [3/3], Step [2040/6471], Loss: 2.2276, Perplexity: 9.2775\n",
      "Epoch [3/3], Step [2050/6471], Loss: 1.8768, Perplexity: 6.53232\n",
      "Epoch [3/3], Step [2060/6471], Loss: 1.9115, Perplexity: 6.76317\n",
      "Epoch [3/3], Step [2070/6471], Loss: 2.0565, Perplexity: 7.81888\n",
      "Epoch [3/3], Step [2080/6471], Loss: 1.9426, Perplexity: 6.9770\n",
      "Epoch [3/3], Step [2090/6471], Loss: 2.1113, Perplexity: 8.2589\n",
      "Epoch [3/3], Step [2100/6471], Loss: 1.8399, Perplexity: 6.29615\n",
      "Epoch [3/3], Step [2110/6471], Loss: 2.2755, Perplexity: 9.73255\n",
      "Epoch [3/3], Step [2120/6471], Loss: 2.0294, Perplexity: 7.6099\n",
      "Epoch [3/3], Step [2130/6471], Loss: 1.9643, Perplexity: 7.1297\n",
      "Epoch [3/3], Step [2140/6471], Loss: 1.9354, Perplexity: 6.9268\n",
      "Epoch [3/3], Step [2150/6471], Loss: 2.1699, Perplexity: 8.75728\n",
      "Epoch [3/3], Step [2160/6471], Loss: 2.0478, Perplexity: 7.75052\n",
      "Epoch [3/3], Step [2170/6471], Loss: 1.8564, Perplexity: 6.40075\n",
      "Epoch [3/3], Step [2180/6471], Loss: 1.9222, Perplexity: 6.83628\n",
      "Epoch [3/3], Step [2190/6471], Loss: 2.2893, Perplexity: 9.8684\n",
      "Epoch [3/3], Step [2200/6471], Loss: 1.8795, Perplexity: 6.5499\n",
      "Epoch [3/3], Step [2210/6471], Loss: 2.0635, Perplexity: 7.8737\n",
      "Epoch [3/3], Step [2220/6471], Loss: 2.1127, Perplexity: 8.2708\n",
      "Epoch [3/3], Step [2230/6471], Loss: 2.0241, Perplexity: 7.56971\n",
      "Epoch [3/3], Step [2240/6471], Loss: 2.1117, Perplexity: 8.26229\n",
      "Epoch [3/3], Step [2250/6471], Loss: 2.0535, Perplexity: 7.7951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [2260/6471], Loss: 1.9838, Perplexity: 7.2705\n",
      "Epoch [3/3], Step [2270/6471], Loss: 1.9591, Perplexity: 7.09285\n",
      "Epoch [3/3], Step [2280/6471], Loss: 2.2550, Perplexity: 9.53500\n",
      "Epoch [3/3], Step [2290/6471], Loss: 2.0316, Perplexity: 7.62658\n",
      "Epoch [3/3], Step [2300/6471], Loss: 2.9876, Perplexity: 19.8382\n",
      "Epoch [3/3], Step [2310/6471], Loss: 2.1518, Perplexity: 8.60079\n",
      "Epoch [3/3], Step [2320/6471], Loss: 1.9810, Perplexity: 7.24973\n",
      "Epoch [3/3], Step [2330/6471], Loss: 2.1288, Perplexity: 8.40489\n",
      "Epoch [3/3], Step [2340/6471], Loss: 1.9533, Perplexity: 7.0517\n",
      "Epoch [3/3], Step [2350/6471], Loss: 2.0219, Perplexity: 7.55296\n",
      "Epoch [3/3], Step [2360/6471], Loss: 2.1188, Perplexity: 8.3209\n",
      "Epoch [3/3], Step [2370/6471], Loss: 2.0040, Perplexity: 7.4186\n",
      "Epoch [3/3], Step [2380/6471], Loss: 2.1421, Perplexity: 8.51710\n",
      "Epoch [3/3], Step [2390/6471], Loss: 2.5461, Perplexity: 12.7573\n",
      "Epoch [3/3], Step [2400/6471], Loss: 2.1275, Perplexity: 8.39424\n",
      "Epoch [3/3], Step [2410/6471], Loss: 2.2328, Perplexity: 9.32585\n",
      "Epoch [3/3], Step [2420/6471], Loss: 2.2339, Perplexity: 9.33614\n",
      "Epoch [3/3], Step [2430/6471], Loss: 1.9329, Perplexity: 6.90980\n",
      "Epoch [3/3], Step [2440/6471], Loss: 2.1100, Perplexity: 8.24834\n",
      "Epoch [3/3], Step [2450/6471], Loss: 1.8876, Perplexity: 6.60358\n",
      "Epoch [3/3], Step [2460/6471], Loss: 1.9882, Perplexity: 7.3022\n",
      "Epoch [3/3], Step [2470/6471], Loss: 1.8734, Perplexity: 6.51063\n",
      "Epoch [3/3], Step [2480/6471], Loss: 2.2433, Perplexity: 9.42450\n",
      "Epoch [3/3], Step [2490/6471], Loss: 1.9797, Perplexity: 7.2404\n",
      "Epoch [3/3], Step [2500/6471], Loss: 2.0816, Perplexity: 8.01724\n",
      "Epoch [3/3], Step [2510/6471], Loss: 2.0575, Perplexity: 7.8265\n",
      "Epoch [3/3], Step [2520/6471], Loss: 1.8704, Perplexity: 6.49091\n",
      "Epoch [3/3], Step [2530/6471], Loss: 1.9042, Perplexity: 6.71416\n",
      "Epoch [3/3], Step [2540/6471], Loss: 2.2011, Perplexity: 9.03527\n",
      "Epoch [3/3], Step [2550/6471], Loss: 1.9188, Perplexity: 6.81254\n",
      "Epoch [3/3], Step [2560/6471], Loss: 2.0450, Perplexity: 7.72892\n",
      "Epoch [3/3], Step [2570/6471], Loss: 1.9289, Perplexity: 6.8819\n",
      "Epoch [3/3], Step [2580/6471], Loss: 2.3759, Perplexity: 10.7608\n",
      "Epoch [3/3], Step [2590/6471], Loss: 2.5973, Perplexity: 13.4269\n",
      "Epoch [3/3], Step [2600/6471], Loss: 2.0973, Perplexity: 8.1438\n",
      "Epoch [3/3], Step [2610/6471], Loss: 1.9877, Perplexity: 7.29906\n",
      "Epoch [3/3], Step [2620/6471], Loss: 2.0010, Perplexity: 7.3966\n",
      "Epoch [3/3], Step [2630/6471], Loss: 1.9392, Perplexity: 6.9530\n",
      "Epoch [3/3], Step [2640/6471], Loss: 2.8763, Perplexity: 17.7484\n",
      "Epoch [3/3], Step [2650/6471], Loss: 1.8646, Perplexity: 6.45365\n",
      "Epoch [3/3], Step [2660/6471], Loss: 1.8701, Perplexity: 6.48937\n",
      "Epoch [3/3], Step [2670/6471], Loss: 1.9870, Perplexity: 7.29374\n",
      "Epoch [3/3], Step [2680/6471], Loss: 1.8797, Perplexity: 6.5517\n",
      "Epoch [3/3], Step [2690/6471], Loss: 2.0439, Perplexity: 7.7209\n",
      "Epoch [3/3], Step [2700/6471], Loss: 2.5376, Perplexity: 12.6487\n",
      "Epoch [3/3], Step [2710/6471], Loss: 1.9103, Perplexity: 6.75507\n",
      "Epoch [3/3], Step [2720/6471], Loss: 1.9038, Perplexity: 6.71170\n",
      "Epoch [3/3], Step [2730/6471], Loss: 2.3055, Perplexity: 10.0291\n",
      "Epoch [3/3], Step [2740/6471], Loss: 1.8697, Perplexity: 6.4865\n",
      "Epoch [3/3], Step [2750/6471], Loss: 3.0292, Perplexity: 20.6802\n",
      "Epoch [3/3], Step [2760/6471], Loss: 1.9987, Perplexity: 7.37938\n",
      "Epoch [3/3], Step [2770/6471], Loss: 2.0325, Perplexity: 7.63295\n",
      "Epoch [3/3], Step [2780/6471], Loss: 1.9448, Perplexity: 6.9926\n",
      "Epoch [3/3], Step [2790/6471], Loss: 2.0810, Perplexity: 8.0124\n",
      "Epoch [3/3], Step [2800/6471], Loss: 2.3193, Perplexity: 10.1687\n",
      "Epoch [3/3], Step [2810/6471], Loss: 2.1004, Perplexity: 8.1696\n",
      "Epoch [3/3], Step [2820/6471], Loss: 2.8541, Perplexity: 17.3584\n",
      "Epoch [3/3], Step [2830/6471], Loss: 2.2669, Perplexity: 9.64947\n",
      "Epoch [3/3], Step [2840/6471], Loss: 1.9839, Perplexity: 7.2711\n",
      "Epoch [3/3], Step [2850/6471], Loss: 2.0721, Perplexity: 7.9417\n",
      "Epoch [3/3], Step [2860/6471], Loss: 1.9941, Perplexity: 7.3454\n",
      "Epoch [3/3], Step [2870/6471], Loss: 2.2670, Perplexity: 9.65068\n",
      "Epoch [3/3], Step [2880/6471], Loss: 1.8885, Perplexity: 6.6096\n",
      "Epoch [3/3], Step [2890/6471], Loss: 2.3452, Perplexity: 10.4355\n",
      "Epoch [3/3], Step [2900/6471], Loss: 2.0534, Perplexity: 7.79471\n",
      "Epoch [3/3], Step [2910/6471], Loss: 1.7257, Perplexity: 5.6163\n",
      "Epoch [3/3], Step [2920/6471], Loss: 1.8576, Perplexity: 6.40824\n",
      "Epoch [3/3], Step [2930/6471], Loss: 2.0559, Perplexity: 7.81408\n",
      "Epoch [3/3], Step [2940/6471], Loss: 1.9481, Perplexity: 7.01569\n",
      "Epoch [3/3], Step [2950/6471], Loss: 1.9170, Perplexity: 6.8005\n",
      "Epoch [3/3], Step [2960/6471], Loss: 2.4377, Perplexity: 11.4472\n",
      "Epoch [3/3], Step [2970/6471], Loss: 1.8174, Perplexity: 6.15606\n",
      "Epoch [3/3], Step [2980/6471], Loss: 2.0424, Perplexity: 7.7094\n",
      "Epoch [3/3], Step [2990/6471], Loss: 1.8900, Perplexity: 6.6194\n",
      "Epoch [3/3], Step [3000/6471], Loss: 2.1635, Perplexity: 8.70141\n",
      "Epoch [3/3], Step [3010/6471], Loss: 1.9334, Perplexity: 6.9128\n",
      "Epoch [3/3], Step [3020/6471], Loss: 1.8919, Perplexity: 6.63193\n",
      "Epoch [3/3], Step [3030/6471], Loss: 2.4236, Perplexity: 11.2869\n",
      "Epoch [3/3], Step [3040/6471], Loss: 1.7839, Perplexity: 5.9532\n",
      "Epoch [3/3], Step [3050/6471], Loss: 2.0925, Perplexity: 8.1050\n",
      "Epoch [3/3], Step [3060/6471], Loss: 2.2017, Perplexity: 9.0408\n",
      "Epoch [3/3], Step [3070/6471], Loss: 2.1815, Perplexity: 8.85920\n",
      "Epoch [3/3], Step [3080/6471], Loss: 1.9807, Perplexity: 7.24768\n",
      "Epoch [3/3], Step [3090/6471], Loss: 1.9421, Perplexity: 6.97350\n",
      "Epoch [3/3], Step [3100/6471], Loss: 2.3121, Perplexity: 10.0959\n",
      "Epoch [3/3], Step [3110/6471], Loss: 2.0300, Perplexity: 7.61434\n",
      "Epoch [3/3], Step [3120/6471], Loss: 2.1396, Perplexity: 8.4963\n",
      "Epoch [3/3], Step [3130/6471], Loss: 2.0447, Perplexity: 7.72721\n",
      "Epoch [3/3], Step [3140/6471], Loss: 1.9811, Perplexity: 7.2504\n",
      "Epoch [3/3], Step [3150/6471], Loss: 2.0562, Perplexity: 7.8159\n",
      "Epoch [3/3], Step [3160/6471], Loss: 2.7275, Perplexity: 15.2944\n",
      "Epoch [3/3], Step [3170/6471], Loss: 1.8900, Perplexity: 6.6193\n",
      "Epoch [3/3], Step [3180/6471], Loss: 1.9809, Perplexity: 7.24907\n",
      "Epoch [3/3], Step [3190/6471], Loss: 1.9371, Perplexity: 6.93851\n",
      "Epoch [3/3], Step [3200/6471], Loss: 1.8795, Perplexity: 6.5500\n",
      "Epoch [3/3], Step [3210/6471], Loss: 1.9635, Perplexity: 7.12435\n",
      "Epoch [3/3], Step [3220/6471], Loss: 1.9044, Perplexity: 6.7155\n",
      "Epoch [3/3], Step [3230/6471], Loss: 2.2455, Perplexity: 9.44510\n",
      "Epoch [3/3], Step [3240/6471], Loss: 1.8507, Perplexity: 6.3644\n",
      "Epoch [3/3], Step [3250/6471], Loss: 2.0303, Perplexity: 7.61610\n",
      "Epoch [3/3], Step [3260/6471], Loss: 1.9780, Perplexity: 7.22847\n",
      "Epoch [3/3], Step [3270/6471], Loss: 1.9228, Perplexity: 6.83997\n",
      "Epoch [3/3], Step [3280/6471], Loss: 2.1298, Perplexity: 8.4132\n",
      "Epoch [3/3], Step [3290/6471], Loss: 2.0660, Perplexity: 7.89347\n",
      "Epoch [3/3], Step [3300/6471], Loss: 2.0042, Perplexity: 7.4201\n",
      "Epoch [3/3], Step [3310/6471], Loss: 2.1672, Perplexity: 8.7335\n",
      "Epoch [3/3], Step [3320/6471], Loss: 1.9672, Perplexity: 7.15080\n",
      "Epoch [3/3], Step [3330/6471], Loss: 2.2266, Perplexity: 9.26819\n",
      "Epoch [3/3], Step [3340/6471], Loss: 1.9385, Perplexity: 6.94836\n",
      "Epoch [3/3], Step [3350/6471], Loss: 2.1625, Perplexity: 8.6930\n",
      "Epoch [3/3], Step [3360/6471], Loss: 2.0901, Perplexity: 8.08531\n",
      "Epoch [3/3], Step [3370/6471], Loss: 2.0913, Perplexity: 8.09545\n",
      "Epoch [3/3], Step [3380/6471], Loss: 2.4377, Perplexity: 11.4468\n",
      "Epoch [3/3], Step [3390/6471], Loss: 1.8634, Perplexity: 6.4455\n",
      "Epoch [3/3], Step [3400/6471], Loss: 1.9967, Perplexity: 7.3644\n",
      "Epoch [3/3], Step [3410/6471], Loss: 2.1551, Perplexity: 8.6290\n",
      "Epoch [3/3], Step [3420/6471], Loss: 1.9715, Perplexity: 7.1815\n",
      "Epoch [3/3], Step [3430/6471], Loss: 1.7247, Perplexity: 5.61080\n",
      "Epoch [3/3], Step [3440/6471], Loss: 2.1392, Perplexity: 8.49309\n",
      "Epoch [3/3], Step [3450/6471], Loss: 1.9862, Perplexity: 7.2881\n",
      "Epoch [3/3], Step [3460/6471], Loss: 1.9553, Perplexity: 7.0660\n",
      "Epoch [3/3], Step [3470/6471], Loss: 1.8007, Perplexity: 6.0536\n",
      "Epoch [3/3], Step [3480/6471], Loss: 2.0199, Perplexity: 7.53772\n",
      "Epoch [3/3], Step [3490/6471], Loss: 1.9009, Perplexity: 6.69177\n",
      "Epoch [3/3], Step [3500/6471], Loss: 2.0718, Perplexity: 7.93946\n",
      "Epoch [3/3], Step [3510/6471], Loss: 1.8672, Perplexity: 6.4704\n",
      "Epoch [3/3], Step [3520/6471], Loss: 1.9864, Perplexity: 7.2894\n",
      "Epoch [3/3], Step [3530/6471], Loss: 2.7109, Perplexity: 15.0430\n",
      "Epoch [3/3], Step [3540/6471], Loss: 1.9208, Perplexity: 6.82666\n",
      "Epoch [3/3], Step [3550/6471], Loss: 1.7926, Perplexity: 6.0053\n",
      "Epoch [3/3], Step [3560/6471], Loss: 1.9171, Perplexity: 6.8011\n",
      "Epoch [3/3], Step [3570/6471], Loss: 1.9199, Perplexity: 6.8199\n",
      "Epoch [3/3], Step [3580/6471], Loss: 2.1488, Perplexity: 8.57427\n",
      "Epoch [3/3], Step [3590/6471], Loss: 2.1400, Perplexity: 8.4992\n",
      "Epoch [3/3], Step [3600/6471], Loss: 1.8621, Perplexity: 6.4372\n",
      "Epoch [3/3], Step [3610/6471], Loss: 2.0698, Perplexity: 7.92369\n",
      "Epoch [3/3], Step [3620/6471], Loss: 2.5636, Perplexity: 12.9831\n",
      "Epoch [3/3], Step [3630/6471], Loss: 1.9332, Perplexity: 6.91194\n",
      "Epoch [3/3], Step [3640/6471], Loss: 2.0486, Perplexity: 7.7572\n",
      "Epoch [3/3], Step [3650/6471], Loss: 2.2311, Perplexity: 9.31049\n",
      "Epoch [3/3], Step [3660/6471], Loss: 1.9708, Perplexity: 7.1764\n",
      "Epoch [3/3], Step [3670/6471], Loss: 1.9593, Perplexity: 7.09428\n",
      "Epoch [3/3], Step [3680/6471], Loss: 2.2052, Perplexity: 9.0718\n",
      "Epoch [3/3], Step [3690/6471], Loss: 1.9034, Perplexity: 6.70882\n",
      "Epoch [3/3], Step [3700/6471], Loss: 2.2093, Perplexity: 9.10925\n",
      "Epoch [3/3], Step [3710/6471], Loss: 2.0669, Perplexity: 7.90052\n",
      "Epoch [3/3], Step [3720/6471], Loss: 1.8060, Perplexity: 6.08644\n",
      "Epoch [3/3], Step [3730/6471], Loss: 2.0622, Perplexity: 7.8632\n",
      "Epoch [3/3], Step [3740/6471], Loss: 1.9454, Perplexity: 6.99644\n",
      "Epoch [3/3], Step [3750/6471], Loss: 2.0362, Perplexity: 7.66130\n",
      "Epoch [3/3], Step [3760/6471], Loss: 1.9717, Perplexity: 7.1829\n",
      "Epoch [3/3], Step [3770/6471], Loss: 2.9382, Perplexity: 18.8821\n",
      "Epoch [3/3], Step [3780/6471], Loss: 2.0601, Perplexity: 7.8468\n",
      "Epoch [3/3], Step [3790/6471], Loss: 1.7568, Perplexity: 5.79408\n",
      "Epoch [3/3], Step [3800/6471], Loss: 2.0828, Perplexity: 8.0267\n",
      "Epoch [3/3], Step [3810/6471], Loss: 1.8675, Perplexity: 6.4721\n",
      "Epoch [3/3], Step [3820/6471], Loss: 2.0999, Perplexity: 8.1656\n",
      "Epoch [3/3], Step [3830/6471], Loss: 1.8824, Perplexity: 6.5695\n",
      "Epoch [3/3], Step [3840/6471], Loss: 1.9091, Perplexity: 6.7470\n",
      "Epoch [3/3], Step [3850/6471], Loss: 1.9131, Perplexity: 6.77402\n",
      "Epoch [3/3], Step [3860/6471], Loss: 1.8655, Perplexity: 6.45909\n",
      "Epoch [3/3], Step [3870/6471], Loss: 2.0520, Perplexity: 7.7832\n",
      "Epoch [3/3], Step [3880/6471], Loss: 1.8833, Perplexity: 6.57538\n",
      "Epoch [3/3], Step [3890/6471], Loss: 2.0729, Perplexity: 7.9480\n",
      "Epoch [3/3], Step [3900/6471], Loss: 1.9111, Perplexity: 6.76032\n",
      "Epoch [3/3], Step [3910/6471], Loss: 2.0078, Perplexity: 7.44674\n",
      "Epoch [3/3], Step [3920/6471], Loss: 1.9953, Perplexity: 7.35438\n",
      "Epoch [3/3], Step [3930/6471], Loss: 2.0093, Perplexity: 7.45773\n",
      "Epoch [3/3], Step [3940/6471], Loss: 1.6729, Perplexity: 5.3274\n",
      "Epoch [3/3], Step [3950/6471], Loss: 2.4632, Perplexity: 11.7429\n",
      "Epoch [3/3], Step [3960/6471], Loss: 2.1692, Perplexity: 8.75141\n",
      "Epoch [3/3], Step [3970/6471], Loss: 2.1322, Perplexity: 8.43337\n",
      "Epoch [3/3], Step [3980/6471], Loss: 2.1364, Perplexity: 8.46866\n",
      "Epoch [3/3], Step [3990/6471], Loss: 1.9362, Perplexity: 6.93227\n",
      "Epoch [3/3], Step [4000/6471], Loss: 1.8007, Perplexity: 6.0537\n",
      "Epoch [3/3], Step [4010/6471], Loss: 1.9798, Perplexity: 7.24147\n",
      "Epoch [3/3], Step [4020/6471], Loss: 1.9192, Perplexity: 6.8152\n",
      "Epoch [3/3], Step [4030/6471], Loss: 1.9392, Perplexity: 6.9529\n",
      "Epoch [3/3], Step [4040/6471], Loss: 1.7819, Perplexity: 5.9413\n",
      "Epoch [3/3], Step [4050/6471], Loss: 1.8258, Perplexity: 6.2077\n",
      "Epoch [3/3], Step [4060/6471], Loss: 2.0925, Perplexity: 8.10532\n",
      "Epoch [3/3], Step [4070/6471], Loss: 2.0737, Perplexity: 7.95387\n",
      "Epoch [3/3], Step [4080/6471], Loss: 2.3400, Perplexity: 10.3811\n",
      "Epoch [3/3], Step [4090/6471], Loss: 1.8807, Perplexity: 6.55829\n",
      "Epoch [3/3], Step [4100/6471], Loss: 1.8538, Perplexity: 6.38397\n",
      "Epoch [3/3], Step [4110/6471], Loss: 2.1398, Perplexity: 8.4980\n",
      "Epoch [3/3], Step [4120/6471], Loss: 1.9644, Perplexity: 7.13055\n",
      "Epoch [3/3], Step [4130/6471], Loss: 2.0462, Perplexity: 7.73867\n",
      "Epoch [3/3], Step [4140/6471], Loss: 2.0692, Perplexity: 7.9188\n",
      "Epoch [3/3], Step [4150/6471], Loss: 1.9709, Perplexity: 7.17700\n",
      "Epoch [3/3], Step [4160/6471], Loss: 1.9091, Perplexity: 6.74729\n",
      "Epoch [3/3], Step [4170/6471], Loss: 2.0248, Perplexity: 7.5743\n",
      "Epoch [3/3], Step [4180/6471], Loss: 1.8240, Perplexity: 6.19683\n",
      "Epoch [3/3], Step [4190/6471], Loss: 1.9421, Perplexity: 6.9732\n",
      "Epoch [3/3], Step [4200/6471], Loss: 2.0620, Perplexity: 7.86203\n",
      "Epoch [3/3], Step [4210/6471], Loss: 1.7503, Perplexity: 5.7565\n",
      "Epoch [3/3], Step [4220/6471], Loss: 2.0962, Perplexity: 8.1354\n",
      "Epoch [3/3], Step [4230/6471], Loss: 1.9953, Perplexity: 7.3543\n",
      "Epoch [3/3], Step [4240/6471], Loss: 1.9063, Perplexity: 6.7284\n",
      "Epoch [3/3], Step [4250/6471], Loss: 1.8881, Perplexity: 6.60708\n",
      "Epoch [3/3], Step [4260/6471], Loss: 1.9076, Perplexity: 6.73661\n",
      "Epoch [3/3], Step [4270/6471], Loss: 1.8575, Perplexity: 6.40754\n",
      "Epoch [3/3], Step [4280/6471], Loss: 2.0176, Perplexity: 7.5204\n",
      "Epoch [3/3], Step [4290/6471], Loss: 2.2146, Perplexity: 9.15755\n",
      "Epoch [3/3], Step [4300/6471], Loss: 2.0636, Perplexity: 7.8739\n",
      "Epoch [3/3], Step [4310/6471], Loss: 2.1706, Perplexity: 8.76320\n",
      "Epoch [3/3], Step [4320/6471], Loss: 2.1043, Perplexity: 8.20179\n",
      "Epoch [3/3], Step [4330/6471], Loss: 1.9890, Perplexity: 7.30809\n",
      "Epoch [3/3], Step [4340/6471], Loss: 1.8757, Perplexity: 6.52513\n",
      "Epoch [3/3], Step [4350/6471], Loss: 2.0701, Perplexity: 7.92568\n",
      "Epoch [3/3], Step [4360/6471], Loss: 1.8701, Perplexity: 6.48900\n",
      "Epoch [3/3], Step [4370/6471], Loss: 2.2180, Perplexity: 9.18873\n",
      "Epoch [3/3], Step [4380/6471], Loss: 1.8712, Perplexity: 6.49597\n",
      "Epoch [3/3], Step [4390/6471], Loss: 2.3703, Perplexity: 10.7005\n",
      "Epoch [3/3], Step [4400/6471], Loss: 2.1795, Perplexity: 8.84230\n",
      "Epoch [3/3], Step [4410/6471], Loss: 1.8425, Perplexity: 6.3121\n",
      "Epoch [3/3], Step [4420/6471], Loss: 1.8183, Perplexity: 6.1614\n",
      "Epoch [3/3], Step [4430/6471], Loss: 1.8873, Perplexity: 6.60175\n",
      "Epoch [3/3], Step [4440/6471], Loss: 1.9534, Perplexity: 7.05245\n",
      "Epoch [3/3], Step [4450/6471], Loss: 1.9547, Perplexity: 7.0621\n",
      "Epoch [3/3], Step [4460/6471], Loss: 2.0836, Perplexity: 8.03373\n",
      "Epoch [3/3], Step [4470/6471], Loss: 2.1736, Perplexity: 8.7895\n",
      "Epoch [3/3], Step [4480/6471], Loss: 2.0243, Perplexity: 7.5709\n",
      "Epoch [3/3], Step [4490/6471], Loss: 1.9121, Perplexity: 6.76717\n",
      "Epoch [3/3], Step [4500/6471], Loss: 1.8901, Perplexity: 6.62007\n",
      "Epoch [3/3], Step [4510/6471], Loss: 1.8696, Perplexity: 6.48575\n",
      "Epoch [3/3], Step [4520/6471], Loss: 2.0852, Perplexity: 8.0466\n",
      "Epoch [3/3], Step [4530/6471], Loss: 2.3405, Perplexity: 10.3866\n",
      "Epoch [3/3], Step [4540/6471], Loss: 1.8194, Perplexity: 6.16823\n",
      "Epoch [3/3], Step [4550/6471], Loss: 2.1940, Perplexity: 8.9710\n",
      "Epoch [3/3], Step [4560/6471], Loss: 2.1616, Perplexity: 8.6846\n",
      "Epoch [3/3], Step [4570/6471], Loss: 2.3691, Perplexity: 10.6878\n",
      "Epoch [3/3], Step [4580/6471], Loss: 1.9058, Perplexity: 6.72453\n",
      "Epoch [3/3], Step [4590/6471], Loss: 2.0139, Perplexity: 7.4925\n",
      "Epoch [3/3], Step [4600/6471], Loss: 1.8330, Perplexity: 6.2524\n",
      "Epoch [3/3], Step [4610/6471], Loss: 2.7750, Perplexity: 16.0394\n",
      "Epoch [3/3], Step [4620/6471], Loss: 1.8649, Perplexity: 6.45555\n",
      "Epoch [3/3], Step [4630/6471], Loss: 1.8900, Perplexity: 6.6193\n",
      "Epoch [3/3], Step [4640/6471], Loss: 2.0134, Perplexity: 7.4885\n",
      "Epoch [3/3], Step [4650/6471], Loss: 2.0643, Perplexity: 7.8797\n",
      "Epoch [3/3], Step [4660/6471], Loss: 1.9891, Perplexity: 7.3090\n",
      "Epoch [3/3], Step [4670/6471], Loss: 2.1916, Perplexity: 8.94923\n",
      "Epoch [3/3], Step [4680/6471], Loss: 2.0199, Perplexity: 7.53764\n",
      "Epoch [3/3], Step [4690/6471], Loss: 2.0410, Perplexity: 7.6987\n",
      "Epoch [3/3], Step [4700/6471], Loss: 1.8661, Perplexity: 6.4633\n",
      "Epoch [3/3], Step [4710/6471], Loss: 1.8248, Perplexity: 6.2015\n",
      "Epoch [3/3], Step [4720/6471], Loss: 2.0672, Perplexity: 7.90304\n",
      "Epoch [3/3], Step [4730/6471], Loss: 2.4943, Perplexity: 12.1137\n",
      "Epoch [3/3], Step [4740/6471], Loss: 2.0248, Perplexity: 7.57493\n",
      "Epoch [3/3], Step [4750/6471], Loss: 2.1648, Perplexity: 8.71279\n",
      "Epoch [3/3], Step [4760/6471], Loss: 2.2130, Perplexity: 9.1428\n",
      "Epoch [3/3], Step [4770/6471], Loss: 2.0453, Perplexity: 7.73157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [4780/6471], Loss: 2.4536, Perplexity: 11.6305\n",
      "Epoch [3/3], Step [4790/6471], Loss: 1.8349, Perplexity: 6.26435\n",
      "Epoch [3/3], Step [4800/6471], Loss: 2.1260, Perplexity: 8.3813\n",
      "Epoch [3/3], Step [4810/6471], Loss: 2.0190, Perplexity: 7.5310\n",
      "Epoch [3/3], Step [4820/6471], Loss: 1.8952, Perplexity: 6.65397\n",
      "Epoch [3/3], Step [4830/6471], Loss: 1.7893, Perplexity: 5.9853\n",
      "Epoch [3/3], Step [4840/6471], Loss: 2.1586, Perplexity: 8.6588\n",
      "Epoch [3/3], Step [4850/6471], Loss: 2.0098, Perplexity: 7.4621\n",
      "Epoch [3/3], Step [4860/6471], Loss: 2.0418, Perplexity: 7.7046\n",
      "Epoch [3/3], Step [4870/6471], Loss: 2.1330, Perplexity: 8.4404\n",
      "Epoch [3/3], Step [4880/6471], Loss: 1.9623, Perplexity: 7.1159\n",
      "Epoch [3/3], Step [4890/6471], Loss: 2.1451, Perplexity: 8.54310\n",
      "Epoch [3/3], Step [4900/6471], Loss: 2.0351, Perplexity: 7.65318\n",
      "Epoch [3/3], Step [4910/6471], Loss: 1.8307, Perplexity: 6.2382\n",
      "Epoch [3/3], Step [4920/6471], Loss: 2.0025, Perplexity: 7.40765\n",
      "Epoch [3/3], Step [4930/6471], Loss: 2.0171, Perplexity: 7.51658\n",
      "Epoch [3/3], Step [4940/6471], Loss: 2.0211, Perplexity: 7.5465\n",
      "Epoch [3/3], Step [4950/6471], Loss: 2.2059, Perplexity: 9.0780\n",
      "Epoch [3/3], Step [4960/6471], Loss: 2.0098, Perplexity: 7.4619\n",
      "Epoch [3/3], Step [4970/6471], Loss: 1.9635, Perplexity: 7.12400\n",
      "Epoch [3/3], Step [4980/6471], Loss: 1.9303, Perplexity: 6.89184\n",
      "Epoch [3/3], Step [4990/6471], Loss: 2.0048, Perplexity: 7.4245\n",
      "Epoch [3/3], Step [5000/6471], Loss: 2.2009, Perplexity: 9.0328\n",
      "Epoch [3/3], Step [5010/6471], Loss: 1.9644, Perplexity: 7.1304\n",
      "Epoch [3/3], Step [5020/6471], Loss: 2.3101, Perplexity: 10.0756\n",
      "Epoch [3/3], Step [5030/6471], Loss: 1.9210, Perplexity: 6.82773\n",
      "Epoch [3/3], Step [5040/6471], Loss: 1.8096, Perplexity: 6.1079\n",
      "Epoch [3/3], Step [5050/6471], Loss: 1.8996, Perplexity: 6.68292\n",
      "Epoch [3/3], Step [5060/6471], Loss: 1.9135, Perplexity: 6.77669\n",
      "Epoch [3/3], Step [5070/6471], Loss: 2.0042, Perplexity: 7.4200\n",
      "Epoch [3/3], Step [5080/6471], Loss: 2.3102, Perplexity: 10.0761\n",
      "Epoch [3/3], Step [5090/6471], Loss: 1.8219, Perplexity: 6.18362\n",
      "Epoch [3/3], Step [5100/6471], Loss: 2.2650, Perplexity: 9.63150\n",
      "Epoch [3/3], Step [5110/6471], Loss: 1.9563, Perplexity: 7.0731\n",
      "Epoch [3/3], Step [5120/6471], Loss: 1.9985, Perplexity: 7.37820\n",
      "Epoch [3/3], Step [5130/6471], Loss: 1.8945, Perplexity: 6.6492\n",
      "Epoch [3/3], Step [5140/6471], Loss: 2.0373, Perplexity: 7.66984\n",
      "Epoch [3/3], Step [5150/6471], Loss: 1.9170, Perplexity: 6.8004\n",
      "Epoch [3/3], Step [5160/6471], Loss: 2.0782, Perplexity: 7.99028\n",
      "Epoch [3/3], Step [5170/6471], Loss: 1.9657, Perplexity: 7.1400\n",
      "Epoch [3/3], Step [5180/6471], Loss: 1.9966, Perplexity: 7.36404\n",
      "Epoch [3/3], Step [5190/6471], Loss: 1.9107, Perplexity: 6.75776\n",
      "Epoch [3/3], Step [5200/6471], Loss: 1.8695, Perplexity: 6.48521\n",
      "Epoch [3/3], Step [5210/6471], Loss: 1.9944, Perplexity: 7.34787\n",
      "Epoch [3/3], Step [5220/6471], Loss: 1.9354, Perplexity: 6.92666\n",
      "Epoch [3/3], Step [5230/6471], Loss: 2.1203, Perplexity: 8.33385\n",
      "Epoch [3/3], Step [5240/6471], Loss: 2.0107, Perplexity: 7.46833\n",
      "Epoch [3/3], Step [5250/6471], Loss: 1.8803, Perplexity: 6.55588\n",
      "Epoch [3/3], Step [5260/6471], Loss: 1.9009, Perplexity: 6.6920\n",
      "Epoch [3/3], Step [5270/6471], Loss: 1.8257, Perplexity: 6.20706\n",
      "Epoch [3/3], Step [5280/6471], Loss: 2.0684, Perplexity: 7.9125\n",
      "Epoch [3/3], Step [5290/6471], Loss: 1.8227, Perplexity: 6.1886\n",
      "Epoch [3/3], Step [5300/6471], Loss: 1.9393, Perplexity: 6.95418\n",
      "Epoch [3/3], Step [5310/6471], Loss: 1.8507, Perplexity: 6.3645\n",
      "Epoch [3/3], Step [5320/6471], Loss: 1.9682, Perplexity: 7.15815\n",
      "Epoch [3/3], Step [5330/6471], Loss: 1.9366, Perplexity: 6.9352\n",
      "Epoch [3/3], Step [5340/6471], Loss: 2.1049, Perplexity: 8.20595\n",
      "Epoch [3/3], Step [5350/6471], Loss: 2.1281, Perplexity: 8.39900\n",
      "Epoch [3/3], Step [5360/6471], Loss: 2.1081, Perplexity: 8.2323\n",
      "Epoch [3/3], Step [5370/6471], Loss: 2.0024, Perplexity: 7.40694\n",
      "Epoch [3/3], Step [5380/6471], Loss: 1.9597, Perplexity: 7.0971\n",
      "Epoch [3/3], Step [5390/6471], Loss: 2.0175, Perplexity: 7.51952\n",
      "Epoch [3/3], Step [5400/6471], Loss: 2.3698, Perplexity: 10.6949\n",
      "Epoch [3/3], Step [5410/6471], Loss: 1.9607, Perplexity: 7.10474\n",
      "Epoch [3/3], Step [5420/6471], Loss: 2.0273, Perplexity: 7.5938\n",
      "Epoch [3/3], Step [5430/6471], Loss: 1.9061, Perplexity: 6.7269\n",
      "Epoch [3/3], Step [5440/6471], Loss: 1.8937, Perplexity: 6.64421\n",
      "Epoch [3/3], Step [5450/6471], Loss: 1.9094, Perplexity: 6.74886\n",
      "Epoch [3/3], Step [5460/6471], Loss: 2.0074, Perplexity: 7.4441\n",
      "Epoch [3/3], Step [5470/6471], Loss: 2.6798, Perplexity: 14.5816\n",
      "Epoch [3/3], Step [5480/6471], Loss: 1.9083, Perplexity: 6.7415\n",
      "Epoch [3/3], Step [5490/6471], Loss: 2.0725, Perplexity: 7.9447\n",
      "Epoch [3/3], Step [5500/6471], Loss: 1.9393, Perplexity: 6.95361\n",
      "Epoch [3/3], Step [5510/6471], Loss: 2.1965, Perplexity: 8.99318\n",
      "Epoch [3/3], Step [5520/6471], Loss: 2.0201, Perplexity: 7.5393\n",
      "Epoch [3/3], Step [5530/6471], Loss: 2.7357, Perplexity: 15.4207\n",
      "Epoch [3/3], Step [5540/6471], Loss: 2.0874, Perplexity: 8.0642\n",
      "Epoch [3/3], Step [5550/6471], Loss: 1.8499, Perplexity: 6.3589\n",
      "Epoch [3/3], Step [5560/6471], Loss: 1.9846, Perplexity: 7.27580\n",
      "Epoch [3/3], Step [5570/6471], Loss: 1.9093, Perplexity: 6.7483\n",
      "Epoch [3/3], Step [5580/6471], Loss: 2.0344, Perplexity: 7.6480\n",
      "Epoch [3/3], Step [5590/6471], Loss: 2.0008, Perplexity: 7.39531\n",
      "Epoch [3/3], Step [5600/6471], Loss: 1.9153, Perplexity: 6.7889\n",
      "Epoch [3/3], Step [5610/6471], Loss: 1.9948, Perplexity: 7.35051\n",
      "Epoch [3/3], Step [5620/6471], Loss: 2.0673, Perplexity: 7.9036\n",
      "Epoch [3/3], Step [5630/6471], Loss: 1.8052, Perplexity: 6.0812\n",
      "Epoch [3/3], Step [5640/6471], Loss: 2.0491, Perplexity: 7.7609\n",
      "Epoch [3/3], Step [5650/6471], Loss: 1.8078, Perplexity: 6.0970\n",
      "Epoch [3/3], Step [5660/6471], Loss: 2.2110, Perplexity: 9.12522\n",
      "Epoch [3/3], Step [5670/6471], Loss: 1.9551, Perplexity: 7.06435\n",
      "Epoch [3/3], Step [5680/6471], Loss: 1.9953, Perplexity: 7.35414\n",
      "Epoch [3/3], Step [5690/6471], Loss: 1.8428, Perplexity: 6.3140\n",
      "Epoch [3/3], Step [5700/6471], Loss: 2.1537, Perplexity: 8.6171\n",
      "Epoch [3/3], Step [5710/6471], Loss: 1.8709, Perplexity: 6.4940\n",
      "Epoch [3/3], Step [5720/6471], Loss: 1.8739, Perplexity: 6.51357\n",
      "Epoch [3/3], Step [5730/6471], Loss: 1.8571, Perplexity: 6.4048\n",
      "Epoch [3/3], Step [5740/6471], Loss: 1.8684, Perplexity: 6.4779\n",
      "Epoch [3/3], Step [5750/6471], Loss: 2.2966, Perplexity: 9.94005\n",
      "Epoch [3/3], Step [5760/6471], Loss: 2.0436, Perplexity: 7.7182\n",
      "Epoch [3/3], Step [5770/6471], Loss: 1.9761, Perplexity: 7.2143\n",
      "Epoch [3/3], Step [5780/6471], Loss: 2.0949, Perplexity: 8.12495\n",
      "Epoch [3/3], Step [5790/6471], Loss: 1.8559, Perplexity: 6.39764\n",
      "Epoch [3/3], Step [5800/6471], Loss: 1.8596, Perplexity: 6.42127\n",
      "Epoch [3/3], Step [5810/6471], Loss: 2.4730, Perplexity: 11.8574\n",
      "Epoch [3/3], Step [5820/6471], Loss: 1.8782, Perplexity: 6.54175\n",
      "Epoch [3/3], Step [5830/6471], Loss: 1.8180, Perplexity: 6.15976\n",
      "Epoch [3/3], Step [5840/6471], Loss: 2.1642, Perplexity: 8.7073\n",
      "Epoch [3/3], Step [5850/6471], Loss: 1.9231, Perplexity: 6.84206\n",
      "Epoch [3/3], Step [5860/6471], Loss: 1.9848, Perplexity: 7.27751\n",
      "Epoch [3/3], Step [5870/6471], Loss: 1.8030, Perplexity: 6.06804\n",
      "Epoch [3/3], Step [5880/6471], Loss: 1.9508, Perplexity: 7.03442\n",
      "Epoch [3/3], Step [5890/6471], Loss: 2.4453, Perplexity: 11.5335\n",
      "Epoch [3/3], Step [5900/6471], Loss: 2.1457, Perplexity: 8.54795\n",
      "Epoch [3/3], Step [5910/6471], Loss: 2.0115, Perplexity: 7.47474\n",
      "Epoch [3/3], Step [5920/6471], Loss: 1.9355, Perplexity: 6.9278\n",
      "Epoch [3/3], Step [5930/6471], Loss: 1.9347, Perplexity: 6.92227\n",
      "Epoch [3/3], Step [5940/6471], Loss: 1.9144, Perplexity: 6.7827\n",
      "Epoch [3/3], Step [5950/6471], Loss: 1.8964, Perplexity: 6.6619\n",
      "Epoch [3/3], Step [5960/6471], Loss: 2.0017, Perplexity: 7.40191\n",
      "Epoch [3/3], Step [5970/6471], Loss: 1.7779, Perplexity: 5.9177\n",
      "Epoch [3/3], Step [5980/6471], Loss: 1.8521, Perplexity: 6.37332\n",
      "Epoch [3/3], Step [5990/6471], Loss: 2.0172, Perplexity: 7.5175\n",
      "Epoch [3/3], Step [6000/6471], Loss: 1.8309, Perplexity: 6.2397\n",
      "Epoch [3/3], Step [6010/6471], Loss: 2.0755, Perplexity: 7.96833\n",
      "Epoch [3/3], Step [6020/6471], Loss: 1.9045, Perplexity: 6.7162\n",
      "Epoch [3/3], Step [6030/6471], Loss: 1.8881, Perplexity: 6.60705\n",
      "Epoch [3/3], Step [6040/6471], Loss: 1.9382, Perplexity: 6.94649\n",
      "Epoch [3/3], Step [6050/6471], Loss: 1.9479, Perplexity: 7.0143\n",
      "Epoch [3/3], Step [6060/6471], Loss: 1.8120, Perplexity: 6.1226\n",
      "Epoch [3/3], Step [6070/6471], Loss: 2.0474, Perplexity: 7.7476\n",
      "Epoch [3/3], Step [6080/6471], Loss: 2.0458, Perplexity: 7.73560\n",
      "Epoch [3/3], Step [6090/6471], Loss: 1.9898, Perplexity: 7.31377\n",
      "Epoch [3/3], Step [6100/6471], Loss: 1.7535, Perplexity: 5.77488\n",
      "Epoch [3/3], Step [6110/6471], Loss: 1.7257, Perplexity: 5.61625\n",
      "Epoch [3/3], Step [6120/6471], Loss: 2.0593, Perplexity: 7.8403\n",
      "Epoch [3/3], Step [6130/6471], Loss: 2.5464, Perplexity: 12.7614\n",
      "Epoch [3/3], Step [6140/6471], Loss: 2.1516, Perplexity: 8.59892\n",
      "Epoch [3/3], Step [6150/6471], Loss: 2.9512, Perplexity: 19.1294\n",
      "Epoch [3/3], Step [6160/6471], Loss: 1.9411, Perplexity: 6.96620\n",
      "Epoch [3/3], Step [6170/6471], Loss: 2.0079, Perplexity: 7.4476\n",
      "Epoch [3/3], Step [6180/6471], Loss: 1.9897, Perplexity: 7.3131\n",
      "Epoch [3/3], Step [6190/6471], Loss: 1.9084, Perplexity: 6.74217\n",
      "Epoch [3/3], Step [6200/6471], Loss: 1.7809, Perplexity: 5.9354\n",
      "Epoch [3/3], Step [6210/6471], Loss: 1.9694, Perplexity: 7.16642\n",
      "Epoch [3/3], Step [6220/6471], Loss: 1.9023, Perplexity: 6.70105\n",
      "Epoch [3/3], Step [6230/6471], Loss: 1.9362, Perplexity: 6.93244\n",
      "Epoch [3/3], Step [6240/6471], Loss: 2.0332, Perplexity: 7.6382\n",
      "Epoch [3/3], Step [6250/6471], Loss: 1.7996, Perplexity: 6.0470\n",
      "Epoch [3/3], Step [6260/6471], Loss: 1.8614, Perplexity: 6.43261\n",
      "Epoch [3/3], Step [6270/6471], Loss: 1.9583, Perplexity: 7.08731\n",
      "Epoch [3/3], Step [6280/6471], Loss: 2.3645, Perplexity: 10.6388\n",
      "Epoch [3/3], Step [6290/6471], Loss: 2.0134, Perplexity: 7.4885\n",
      "Epoch [3/3], Step [6300/6471], Loss: 2.1210, Perplexity: 8.3398\n",
      "Epoch [3/3], Step [6310/6471], Loss: 1.9039, Perplexity: 6.7121\n",
      "Epoch [3/3], Step [6320/6471], Loss: 2.0377, Perplexity: 7.6732\n",
      "Epoch [3/3], Step [6330/6471], Loss: 2.0699, Perplexity: 7.92380\n",
      "Epoch [3/3], Step [6340/6471], Loss: 1.8223, Perplexity: 6.1861\n",
      "Epoch [3/3], Step [6350/6471], Loss: 2.0576, Perplexity: 7.8275\n",
      "Epoch [3/3], Step [6360/6471], Loss: 1.8920, Perplexity: 6.63299\n",
      "Epoch [3/3], Step [6370/6471], Loss: 1.9124, Perplexity: 6.7691\n",
      "Epoch [3/3], Step [6380/6471], Loss: 1.8240, Perplexity: 6.1965\n",
      "Epoch [3/3], Step [6390/6471], Loss: 2.3215, Perplexity: 10.1912\n",
      "Epoch [3/3], Step [6400/6471], Loss: 1.8972, Perplexity: 6.6673\n",
      "Epoch [3/3], Step [6410/6471], Loss: 1.8867, Perplexity: 6.59792\n",
      "Epoch [3/3], Step [6420/6471], Loss: 2.1639, Perplexity: 8.7052\n",
      "Epoch [3/3], Step [6430/6471], Loss: 1.9938, Perplexity: 7.3432\n",
      "Epoch [3/3], Step [6440/6471], Loss: 1.8244, Perplexity: 6.1993\n",
      "Epoch [3/3], Step [6450/6471], Loss: 1.8360, Perplexity: 6.2711\n",
      "Epoch [3/3], Step [6460/6471], Loss: 1.7500, Perplexity: 5.75494\n",
      "Epoch [3/3], Step [6470/6471], Loss: 2.1524, Perplexity: 8.60529\n",
      "Epoch [3/3], Step [6471/6471], Loss: 1.9343, Perplexity: 6.9192"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
